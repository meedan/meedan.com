---
report:
- sys:
    id: 1vw9SWrQcyEyWtga37JUw6
    created_at: !ruby/object:DateTime 2020-06-15 20:05:48.745000000 Z
    updated_at: !ruby/object:DateTime 2020-11-11 22:19:47.551000000 Z
    content_type_id: report
    revision: 2
    space:
      sys:
        id: g118h5yoccvd
    type: Entry
    content_type:
      sys:
        id: report
    environment:
      sys:
        id: master
        type: Link
        linkType: Environment
    locale: en-US
  title: 'Trust, Religion, and Politics: Coronavirus Misinformation in Iran'
  slug: trust-religion-and-politics-coronavirus-misinformation-in-iran
  description: Lack of public trust in officials, religiously charged narratives by
    unofficial fringe figures and political manipulation of the discourse is shaping
    COVID-19 misinformation in Iran.
  lead_image:
    sys:
      id: 1dZgE7lhkFmGDiYmoEDtE0
      created_at: !ruby/object:DateTime 2020-06-19 03:22:05.276000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:10:16.075000000 Z
    title: 'Trust, Religion, and Politics: Coronavirus Misinformation in Iran'
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/1dZgE7lhkFmGDiYmoEDtE0/fc846020c620fa0f20cb73d2ecff1772/iran.png"
  social_card_image:
    sys:
      id: 1TXTPQACmPJLvpvGpSNDPO
      created_at: !ruby/object:DateTime 2020-06-22 23:46:33.116000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:10:19.697000000 Z
    title: 'Trust, Religion, and Politics: Coronavirus Misinformation in Iran'
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/1TXTPQACmPJLvpvGpSNDPO/bee0b505859726cfeace88c3c4a15486/meedan_report_covid-19-iran_01.png"
  introduction: In early 2020, the introduction of COVID-19 in the MENA region found
    its epicenter in Iran. The country’s outbreak preceded much of the rest of the
    world, and during the outset of the crisis the Iranian government’s slow reaction
    and delayed quarantines ignited ridicule internationally. Online and offline misinformation
    were part of the country’s battle with the virus. In this chapter, we highlight
    three main aspects that have shaped the spread of coronavirus misinformation in
    Iran; a) the lack of public trust in officials, b) religiously charged narratives
    by unofficial fringe figures, c) political manipulation of the discourse around
    the virus. We end this chapter by focusing on the counter measures implemented
    and the limitations that enabled the dissemination of the misleading content.
  body: "\n### Trust Crisis in Iran\n\n#### Political Turbulence and Trust\n\nIn the
    months leading up to the COVID-19 crisis in Iran, there was an increase in distrust
    in Iran’s official institutions. Following November 2019 protests that resulted
    in what is suspected to have been thousands of deaths, Iran’s Revolutionary Guards,
    the country’s wide-reaching military corps<sup><a id=\"fnref1\" href=\"#fn1\">[1]</a></sup>,
    and the (semi) elected Iranian government faced a crisis of legitimacy. In addition,
    the downing and the subsequent coverup of the Ukraine Airlines passenger jet in
    January 2020 by the Guards led to a new round of protests ([Behravesh, Jan. 14,
    2020](https://www.aljazeera.com/news/2020/01/iran-plane-shoot-protest-crisis-competency-legitimacy-200113151404256.html)).\n\nAs
    COVID-19 began to take hold in the country, the Supreme Leader Ayatollah Khamenei,
    President Rouhani, the Ministry of Health and other officials initially played
    down the pandemic, in fear that the virus would lower voter turnout for the country’s
    February election ([BBC Persian, Feb. 25, 2020](https://www.bbc.com/persian/iran-51630282)).
    Less reported in the media are the other issues and events that have eroded public
    trust against these authorities throughout the pandemic: Channels affiliated to
    the Revolutionary Guards and state affiliated news channels promoted conspiracy
    theories about the virus’s origins. Masaf Institute for example, a cultural and
    economic institution affiliated with Revolutionary Guard under the leadership
    of a well-known and cult-like conspiracy theorist, Ali Akbar Raefipour, stood
    out amongst these channels. They used their “health” Telegram channel called [@masaf_salam](https://t.me/masaf_salam)
    as well as it’s main channel [@masaf](https://t.me/masaf), to promote a wide range
    of conspiracies about the origins of the virus. Other accounts belonging or affiliated
    with the Revolutionary Guards such [@afsaran_ir](https://t.me/Afsaran_ir), [@sepah_cyberi](https://t.me/c/1153794295/4589),
    [@sepahcyberi](https://t.me/sepahcybery) were also actors promoting such theories.
    However, they further dramatized and promoted conspiracy theories that were being
    promoted in Iran’s official media regarding the virus as an attack by the US against
    Iran.\n\nWhile political information worked to divide and blame the origins of
    the virus, influential channels on the popular messaging service Telegram, even
    some associated with hardline and traditional institutions such as the Revolutionary
    Guards, were providing credible information by at least, [29 February](https://t.me/c/1153794295/2423),
    asking followers to follow social distancing measures and sharing videos of medical
    staff asking Iranians to stay home to save lives. \n\n#### Religious Figures Fill
    Trust Vacuum\n\nOur analysis of social media found examples of religiously-charged
    COVID-19 messages promoted in India, the US, and MENA countries such as Iran.
    It is the latter that we focus on in this report. There is no doubt that Islam
    prohibts the spreading of unverified information, gossipes, and rumours<sup><a
    id=\"fnref2\" href=\"#fn2\">[2]</a></sup>. Yet, with a deficit of public trust
    in authorities, opportunities for information sharing have been allocated to unqualified
    actors, most significantly, clerics and religious figures. \n\nIn Iran, where
    some Shiite Muslim religious figures often use their faith as a pillar of opposition
    against westernization, a fringe figure named Ayatollah Abbas Tabrizian has come
    to be known as “the father of Islamic medicine.” Prior to the outbreak of COVID-19,
    Tabrizian [posted a video](https://t.me/DaftareAyatollahTabrizian/6965) in January
    of himself burning a copy of _Harrison's Principles of Internal Medicine_, an
    American reference textbook of medicine used globally. Tabrizian accused the book
    of being “a tool of the West’s infiltration of other countries” in a video sermon
    posted to his Telegram channel. \n\nThose who follow Tabrizian’s thinking against
    modern medicine are known to refuse to vaccinate, or follow medical advice, for
    major illnesses including cancer diagnoses ([Faghihi, Mar. 10, 2020](https://www.al-monitor.com/pulse/originals/2020/03/bizarre-cures-for-coronavirus-in-iran.html)).
    On 25 February, as the coronavirus crisis unfolded in Iran, Tabrizian advised
    followers infected with the virus to swab their anus’ with a piece of cotton dipped
    in violet oil (See Figure 1). The [post](https://t.me/DaftareAyatollahTabrizian)
    has gone viral on his Telegram channel of over 200,000 followers and beyond onto
    other Persian language social media, with mixed reactions that included both ridicule
    and adherence to Tabrizian’s messaging. \n\n![meedan covid-19-report iran Ayatollah-Abbas-Tabrizian-official-Telegram-3](//images.ctfassets.net/g118h5yoccvd/7tCYRd8B523rGfF2DF4UTt/ae60d6c86fc30b83d24f38ce0e1ce0d4/meedan_covid-19-report_iran_Ayatollah-Abbas-Tabrizian-official-Telegram-3.png)\n<p
    class=\"caption\">Figure 1: A screenshot from Ayatollah Abbas Tabrizian’s official
    Telegram page. In a 25 February 2020 post, he lists 13 actions to take in an effort
    to battle an infection against coronavirus. Point 8 has been particularly shocking,
    where he advises infected followers to swab their anus with violet oil before
    bed. This screenshot was taken by the authors on 28 March 2020.</p>\n\nAnother
    controversial video that circulated across Persian language social media shows
    Mortazavi Kohsal, known to be a disciple of Ayatollah Tabrizian ([Saeedian, Mar.
    23, 2020](https://twitter.com/mahdiarsaeedian/status/1241289478301499392); Bahar
    News, Mar. 23, 2020). As he claims to practice Islamic medicine, Kohsal visits
    critically ill and quarantined coronavirus patients, while wearing no protective
    clothes or gear, to rub a liquid he claimed to be “the perfume of the Prophet”
    on the patients lips in the city of Anzali in Gilan. Two days later, one of the
    young patients Kohsal rubbed the perfume died from the virus ([Bahar News, Mar.
    23, 2020](http://www.baharnews.ir/news/206368/%D8%AF%D8%B1%DA%AF%D8%B0%D8%B4%D8%AA-%D8%AC%D9%88%D8%A7%D9%86-%D8%A7%D9%87%D9%84-%D8%A7%D9%86%D8%B2%D9%84%DB%8C-%D8%AF%DB%8C%D8%AF%D8%A7%D8%B1-%D9%83%D9%87%D9%86%D8%B3%D8%A7%D9%84)).
    \n\nMehdi Sabili, another follower of Tabrizian, similarly circulated a viral
    video recommending camel urine as a coronavirus remedy. Authorities arrested himfor
    creating “confusion and anxiety” for the public, and undermining the countries
    “Islamic and medicinal values” in addition to running a Islamic Medicine clinic
    in Tehran without a license  ([YJC, Apr. 22, 2020](https://www.yjc.ir/fa/news/7327494/%D8%AA%D8%A8%D9%84%DB%8C%D8%BA-%DA%A9%D9%86%D9%86%D8%AF%D9%87-%D9%85%D8%B2%D8%A7%DB%8C%D8%A7%DB%8C-%D9%86%D9%88%D8%B4%DB%8C%D8%AF%D9%86-%D8%A7%D8%AF%D8%B1%D8%A7%D8%B1-%D8%B4%D8%AA%D8%B1-%D8%A8%D8%A7%D8%B2%D8%AF%D8%A7%D8%B4%D8%AA-%D8%B4%D8%AF)).
    \n\n#### Protection for Religious Misinformers\n\nCriticisms against these actors
    and their contribution to fatal dangers within the Iranian establishment are aplenty
    ([Radio Farda, Dec. 29, 2020](https://en.radiofarda.com/a/successor-to-khamenei-died-because-he-trusted-islamic-medicine-son-reveals/30349899.html)).
    However the phenomenon remains protected from official reproach through Tabrizian’s
    entrenched networks amongst Iran’s clerical elite. While two of Tabrizian’s disciples
    have been arrested after causing outrage in both internal Iranian and international
    media, Tabrizian has been left untouched. His honorific title of “Ayatollah” has
    never been questioned for removal by Iran’s Shiite seminaries ([YJC, Apr. 23,
    2020](https://www.yjc.ir/fa/news/7328801/%D8%A7%D9%88%D9%84%DB%8C%D9%86-%D8%B5%D8%AD%D8%A8%D8%AA%E2%80%8C%D9%87%D8%A7%DB%8C-%D8%B4%DB%8C%D8%A7%D8%AF-%D9%86%D9%88%D8%B4%D9%86%D8%AF%D9%87-%D8%A7%D8%AF%D8%B1%D8%A7%D8%B1-%D8%B4%D8%AA%D8%B1-%D9%BE%D8%B3-%D8%A7%D8%B2-%D8%AF%D8%B3%D8%AA%DA%AF%DB%8C%D8%B1%DB%8C-%D9%81%DB%8C%D9%84%D9%85))<sup><a
    id=\"fnref3\" href=\"#fn3\">[3]</a></sup>.  \n\nIn February 2020, three Iranian
    doctors who condemned Tabrizian’s medical advice were sentenced to flogging in
    the city of Mashhad ([Asr Iran, Feb. 9, 2020](https://www.asriran.com/fa/news/712989/%D8%A7%D8%B7%D9%84%D8%A7%D8%B9%DB%8C%D9%87-%D9%86%D8%B8%D8%A7%D9%85-%D9%BE%D8%B2%D8%B4%DA%A9%DB%8C-%D9%85%D8%B4%D9%87%D8%AF-%D8%AF%D8%B1%D8%A8%D8%A7%D8%B1%D9%87-%D8%A7%D9%86%D8%AA%D8%B4%D8%A7%D8%B1-%D8%B1%D8%A7%DB%8C-%D9%85%D8%AD%DA%A9%D9%88%D9%85%DB%8C%D8%AA-3-%D9%BE%D8%B2%D8%B4%DA%A9-%D8%A8%D8%A7-%D8%B4%DA%A9%D8%A7%DB%8C%D8%AA-%D8%AA%D8%A8%D8%B1%DB%8C%D8%B2%DB%8C%D8%A7%D9%86))<sup><a
    id=\"fnref4\" href=\"#fn4\">[4]</a></sup>. While the doctors were later [acquitted](https://www.irna.ir/news/83761292/%D8%B3%D9%87-%D9%BE%D8%B2%D8%B4%DA%A9-%D9%85%D9%88%D8%B1%D8%AF-%D8%B4%DA%A9%D8%A7%DB%8C%D8%AA-%D8%B9%D8%A8%D8%A7%D8%B3-%D8%AA%D8%A8%D8%B1%DB%8C%D8%B2%DB%8C%D8%A7%D9%86-%D8%AA%D8%A8%D8%B1%D8%A6%D9%87-%D8%B4%D8%AF%D9%86%D8%AF),
    these arrests highlight Tabrizian’s influence with authorities. Furthermore, a
    journalist running the Telegram channel for the semi-official Iranian Labour News
    Agency (ILNA) was arrested on 23 April for posting a satirical cartoon of Tabrizian
    and his suggested coronavirus remedies on the basis of insulting the Supreme Leader
    Khamenei. The cartoon depicted Khamenei as a nurse asking for silence as he allows
    for remedies to be applied to a coronavirus patient ([RFE/RL, Apr. 28, 2020](https://www.rferl.org/a/iran-arrests-editor-journalist-over-cartoon-mocking-khamenei/30581070.html)).
    Popular platforms such as the Iranian alternative to YouTube, Aparat, that strictly
    adhere to Iran’s censorship regulations ([Center for Human Rights in Iran, Nov.
    29, 2017](https://www.iranhumanrights.org/2017/11/after-fiery-speech-iran-censors-ahmadinejad-related-online-searches/))
    leaves the official pages for Tabrizian prescribing and promoting remedies unchecked
    (see the page for Tabrizian’s archive of speeches [here](https://www.aparat.com/tebeslamiclip),
    and his office’s official channel [here](https://www.aparat.com/daftareayatollahtabrizian)).\n\nReligious
    authorities also falsely believed sacred protection would prevent the virus from
    spreading in religious shrines, and resistance to closing down the religious shrines
    in the city of Qom, where the virus had its first outbreak in Iran, was a contributing
    factor to the spread of the disease ([Batrawy, Feb. 25, 2020](https://apnews.com/7b8f420db1a339116d66bc55c9085d7c)).
    However, as the pandemic has carried on for a few months, some clerical figures,
    especially in Qom, have come to be seen as community leaders in guiding public
    health on good advice ([Bozorgmehr, Jun. 17, 2020](https://www.ft.com/content/8e9b50bb-ebf7-4702-9894-1f2081ae869a)).\n\nDespite
    many authorities condemning “religious misinformation”, it was still a phenomenon
    that contributed to Iran’s _infodemic_. The combination of communications platforms
    allowing for wider audiences for fanatical solutions have contributed to the reach
    and impact of dangerous ideas and remedies. Politically, the spread of the virus
    from a holy Shiite centre such as Qom has fueled the existing sectarian tensions
    within the Gulf region and Lebanon. Scapegoating problems of coronavirus to Shiite
    Islam has contributed to eroding civil rights against already persecuted minority
    Shiite communities in Saudi Arabia and Bahrain ([Matthiesen, Mar. 23, 2020](https://www.foreignaffairs.com/articles/middle-east/2020-03-23/coronavirus-exacerbating-sectarian-tensions-middle-east)).\n\n####
    The Alcohol Epidemic Within the Pandemic: Not Part of the Online Infodemic\n\nTo
    date, over 300 Iranians have died and another 1,000 were poisoned by alcohol during
    the pandemic ([Karimi and Gambrell, Mar. 27, 2020](https://apnews.com/6e04783f95139b5f87a5febe28d72015?utm_campaign=SocialFlow&utm_source=Twitter&utm_medium=AP)).
    Contrary to some sources promoted by foriegn media, this was not a problem found
    in the “online infodemic.<sup><a id=\"fnref5\" href=\"#fn5\">[5]</a></sup>” Alcohol
    related deaths have long been a problem in Iran. Conditions of the lockdown have
    just exacerbated it. \n\nAs an Islamic theocracy, alcohol is banned in the country
    but still widely consumed. Iranians have different ways of acquiring alcohol.
    There are alcohol dealers that sell various imported alcoholic drinks, typically
    at very high prices. A cheaper alternative, however, is making the alcohol at
    home or purchasing alcohol distilled in illegal local workshops. The latter options
    are often the source of poorly made distillations that lead to methanol poisoning.
    In fact, alcohol related deaths due to these poisonous distillations have long
    been a public health crisis in Iran, well before the introduction of the coronavirus
    ([Hekmat et al 2012](http://ircmj.com/articles/15765.html)). Alcohol poisoning
    has increased during the pandemic as more people have been looking to occupy their
    time at home by drinking; falsely interpreting alcohol consumption as a coronavirus
    disinfectant or remedy; and an increase in dealers selling poisonous alcohol ([Hamshari](http://newspaper.hamshahrionline.ir/id/96869/36%D8%AC%D8%A7%D9%86-%D9%81%D8%AF%D8%A7%DB%8C-%D8%A7%D8%B4%D8%AA%D8%A8%D8%A7%D9%87.html)[
    Online, Mar. 14 2020](http://newspaper.hamshahrionline.ir/id/96869/36%D8%AC%D8%A7%D9%86-%D9%81%D8%AF%D8%A7%DB%8C-%D8%A7%D8%B4%D8%AA%D8%A8%D8%A7%D9%87.html)).
    This, combined with Iran’s existing issues of trust and access to information
    factors surrounding the virus in Iran have intensified into a record amount of
    deaths.\n\n### The Role of Geopolitics in Misinformation\n\n#### Conspiracy Theories\n\nPolitical
    leaders in Iran have spread conspiracy theories that accuse other countries of
    spreading the virus. High ranking officials of the Iranian Revolutionary Guards
    Corps (IRGC), as well as the Supreme Leader Ayatollah Khamenei have spread the
    conspiracy theory that coronavirus is part of an American led bio attack against
    Iran within the country’s officialdom ([Haghdoost and Motevalli, Mar. 12, ](https://www.bloomberg.com/news/articles/2020-03-12/iran-s-khamenei-says-virus-outbreak-may-be-biological-attack)[2020](https://www.bloomberg.com/news/articles/2020-03-12/iran-s-khamenei-says-virus-outbreak-may-be-biological-attack)).
    \n\nWhile some unofficial religious leaders promoted remedies, Iranian officials
    promoted fake virus testing technologies, making a mockery of the states efforts
    to combat the deadly virus. In what seemed like a show, the IRGC unveiled what
    was advertised as a “coronavirus remote detector” that could locate people infected
    within a 100 meters range. News about the unveiling ceremony on the 15th of April
    was published throughout Iran’s press and spread throughout international media.
    Links about this poorly-designed public diplomacy/propaganda attempt have circulated
    on Twitter for its “scientific capacity” (see Figure2). \n\n![meedan covid-19-report
    iran Press-TV-screenshot-3](//images.ctfassets.net/g118h5yoccvd/2JcRouH88cRjNCjkQ11dlQ/191c9729314ed1c17fa59c2f1485a0cf/meedan_covid-19-report_iran_Press-TV-screenshot-3.png)\n<p
    class=\"caption\">Figure 2: A screenshot of a Tweet by Press TV supporting the
    IRGC’s coronavirus detector. The screenshot was taken by the authors on the 25th
    of April 2020.</p>\n\n#### Information Controls\n\nOne argument about international
    coverage of the coronavirus in Iran however, is that Iran’s place as a “bad actor”
    or “pariah” state within the world, especially as a force countering U.S hegemony
    in the Middle East, has led to unbalanced reporting on the situation of the virus
    in Iran. As one of the first countries hit by the virus, analysis of Iran’s inadequate
    response was perceived as a shocking and singular demonstration of Iran’s incompetence.
    However, as the virus became a pandemic overtaking almost every country in the
    world, it became evident incompetence and inadequacy were not singular to Iran.
    Many advanced democracies struggled just as Iran did. This imbalance in foreign
    media coverage further exasperated confusions in what trusted sources could be.
    Authorities made an extra effort to tarnish the work of foreign media coverage
    of the virus in Iran as malicious.  In May. President Rouhani announced foreign
    media purposely manipulated news of the virus to “close down our country” ([IRNA
    News, May. 16, 2020](https://www.irna.ir/news/83789038/%D8%B1%D8%B3%D8%A7%D9%86%D9%87-%D9%87%D8%A7%DB%8C-%D8%A8%DB%8C%DA%AF%D8%A7%D9%86%D9%87-%D8%A8%D9%87-%D8%A8%D9%87%D8%A7%D9%86%D9%87-%DA%A9%D8%B1%D9%88%D9%86%D8%A7-%D8%A8%D9%87-%D8%AF%D9%86%D8%A8%D8%A7%D9%84-%D8%AA%D8%B9%D8%B7%DB%8C%D9%84-%DA%A9%D8%B1%D8%AF%D9%86-%DA%A9%D8%B4%D9%88%D8%B1-%D8%A8%D9%88%D8%AF%D9%86%D8%AF)).
    \n\nNevertheless, manipulation campaigns have been tied to Iran. The “International
    Union of Virtual Media” (IUVM) has been uncovered as an influence campaign tied
    to Iran by researchers at Graphika, FireEye, and Facebook ([Graphika, Apr. 14,
    2020](https://graphika.com/reports/irans-iuvm-turns-to-coronavirus/)). On 28 February
    BBC Persian published a whistleblower report that proved the number of deaths
    in Iran far exceeded the government reports ([BBC, Feb. 28, 2020](https://www.bbc.com/news/world-middle-east-51673053)).
    Shortly afterwards IUVM TV ran a smear campaign calling BBC Persian reporting
    a British conspiracy to undermine Iranian authorities (see figure 3). \n\n![A
    screenshot of a video posted on the IUVM website](//images.ctfassets.net/g118h5yoccvd/1YmSSDLn4bT5xi0dHqRBP0/3273db2399cd30487084692c9e141449/meedan_covid-19-report_iran_IUVM-website-screenshot_3.png)\n<p
    class=\"caption\">Figure 3: A screenshot of a video posted on the IUVM website,
    indicating BBC Persian’s reporting on Iran’s coronavirus cases and responses are
    part of a British conspiracy against Iran. The screenshot was taken by the authors
    on the 25th of April 2020.</p> \n\nGeopolitical warfare over controlling the coronavirus
    narrative continues alongside the censorship of major social media platforms,
    including  Twitter, Facebook, YouTube, Telegram and major international media
    outlets such as BBC Persian. This has helped place hurdles on access to trusted
    information, especially pertinent during the start of the spread of the virus.
    \n\nPrior to the country’s parliamentary elections information sources about the
    virus were scarce if not only on censored platforms. However, the government denied
    the arrival of the virus as “disinformation” being spread through social media
    ([BBC Persian, Feb. 16 2020](https://www.bbc.com/persian/iran-51521216)). \n\nFear
    of contradicting the government has also stopped many Iranians from sharing information
    online. There have been a number of arrests of medical workers exposing the true
    extent of the medical catastrophe ([ARTICLE19](https://www.article19.org/resources/iran-coronavirus-crisis-highlights-need-for-the-free-flow-of-information/),
    Mar. 27,[ 2020](https://www.article19.org/resources/iran-coronavirus-crisis-highlights-need-for-the-free-flow-of-information/)).
    Some criticism of the government’s public health responses led to high profile
    arrests, such as Shiraz City Councillor Mehdi Hajati, who on his [Twitter account](https://twitter.com/mehdihajati/status/1237649682953715712)
    criticized authorities that did not quarantine Qom as the enemy of the people.\n\nIn
    many addresses to the nation President Rouhani has warned of a coronavirus _infodemic_
    ([Tasnim, Mar. 14, 2020](https://www.tasnimnews.com/fa/news/1398/12/24/2222979/%D8%B1%D9%88%D8%AD%D8%A7%D9%86%DB%8C-%D8%B1%D8%B3%D8%A7%D9%86%D9%87-%D9%87%D8%A7-%D8%A7%D8%AC%D8%A7%D8%B2%D9%87-%D9%86%D8%AF%D9%87%D9%86%D8%AF-%D8%AF%D8%B1-%D9%85%D9%88%D8%B6%D9%88%D8%B9-%DA%A9%D8%B1%D9%88%D9%86%D8%A7-%D8%A8%D9%87-%D8%B3%D9%85%D8%AA-%D8%AF%D9%88%D9%82%D8%B7%D8%A8%DB%8C-%D8%A8%D8%B1%D9%88%DB%8C%D9%85-%D8%A8%D9%87-%D8%AC%D8%A7%D9%85%D8%B9%D9%87-%D8%A2%D8%B1%D8%A7%D9%85%D8%B4-%D8%AF%D9%87%DB%8C%D9%85)).
    While figures such as Tabrizian have been exempt, authorities have been known
    to crack down on those criticizing the states as opposed to actors producing dangerous
    misinformation.\n\n### Counter-Measures\n\nWhile misinformation is circulated,
    counter-measures were barely implemented in the MENA region. Old coercive methods
    have been imposed, like arrests. MENA governments have launched more “threats”
    to those who spread rumours – or anything that contradicts the official news –
    on social media in relation to coronavirus, accusing them of affecting the public
    order.Coronavirus “misinformation” has become another reason to put more people
    behind bars in this region. There have been a number of arrests in Iran due to
    spreading “misinformation” on social media. To enforce that, the Iranian government
    created a task force to combat “misinformation” and “the spread of fear” leading
    to at least 24 arrests ([ARTICLE19, Mar. 27. 2020](https://www.article19.org/resources/iran-coronavirus-crisis-highlights-need-for-the-free-flow-of-information/)).
    While official measures did not help counter the spread of misinformation, measures
    implemented by social media platforms did not seem to help either. \n\n#### Social
    Media Platforms and Misinformation\n\nSocial media use in Western contexts are
    mainly dominated by platforms owned by Google, Facebook and Twitter. Although
    they operate imperfectly, there have been channels for criticism and accountability
    for the policies and decisions these companies take for fighting misinformation
    by civil society and governments. Within the Iranian context, the platforms used
    are more complex. According to a 2019 poll done by the Islamic Student Polling
    Agency, the top three most used social media applications (not including streaming
    platforms) are WhatsApp, Telegram and Instagram (in that order of popularity)
    ([ISPA, Apr. 30, 2019](http://ispa.ir/Default/Details/fa/2095)). [Iran’s Alexa
    ranking](https://www.alexa.com/topsites/countries/IR) for websites also lists
    the local video streaming platform Aparat as the second most visited website in
    Iran . \n\nWhat is emblematic of Iran’s information spaces are the lack of oversight
    that can be afforded to flag or remove false information. Viral news stories or
    videos forwarded in private chats or WhatsApp groups have become an emblem of
    COVID19’s Iranian infodemic. As researchers, this phenomenon is hard to track
    and trace across WhatsApp beyond interviews done with Iranians. In terms of oversight
    of the virality of false information on the platform, WhatsApp has struggled to
    find solutions. These struggles are naturally inherent in the nature of the platform
    as a private and encrypted messaging<sup><a id=\"fnref6\" href=\"#fn6\">[6]</a></sup>.
    \ \n\nAs the second most popular social media platform in Iran, Telegram’s major
    role lies in its public channels, which have been a platform for false information
    during the coronavirus pandemic. Content moderation policies on Telegram are notoriously
    more lax and subject to fewer accountability oversight mechanisms. This might
    be a product of how niche its social media function is within only certain communities
    or countries. However, as the “infodemic” within the coronavirus pandemic has
    become a global issue of universal relevance in almost every community and country,
    Telegram introduced a system to verify its channels in April in order to fight
    against dangerous coronavirus mis/disinformation<sup><a id=\"fnref7\" href=\"#fn7\">[7]</a></sup>.\n\n####
    Misinformation Mitigation in Iran\n\nThroughout the coronavirus pandemic, problematic
    channels such as that of [Ayatollah ](https://t.me/DaftareAyatollahTabrizian)[Tabrizian’](https://t.me/DaftareAyatollahTabrizian)s,
    propagating false remedies according to the dictates of his own school of “Islamic
    Medicine” have circulated unchecked. However, many channels affiliated with authorities
    did publish legitimate information that might have been ignored due to the crisis
    of legitimacy that exists between citizens and authorities.\n\nIn an official
    attempt to raise awareness, the Iranian government raised concerns about coronavirus
    misinformation as an issue mainly from “enemies.” Before official orders were
    given to lockdown public spaces, businesses, and restaurants in Iran, President
    Rouhani indicated “it is the conspiracy of our enemies to create fear to shut
    down our country” ([BBC Persian, Feb. 25, 2020](https://www.bbc.com/persian/iran-51630282)).
    Instead of promoting media literacy in the country, the government flagged these
    warnings to further cement the theory of “western influence” in creating or worsening
    the situation or to deflect from government criticism. While official efforts
    were lacking to counter the spread of misleading information in Iran, independent
    fact-checking efforts were also lacking.\n\n#### Independent Fact-Checking\n\nThe
    MENA region, in general, lacks fact-checking efforts. Only a few local fact checkers
    work on countering Arabic and Persian misinformation in relation to coronavirus
    at the time of writing. AFP Factual and Fatabyyano are among those few entities
    that run coronavirus fact checking efforts, and act as the official Facebook third-party
    fact checkers in the region (See more on [Google Fact Check Tools](https://toolbox.google.com/factcheck/explorer/search/coronavirus;hl=)
    and [IFCN](https://ifcncodeofprinciples.poynter.org/signatories)). Others, in
    a less formal way, run fact-checking efforts like Verify-Syria, Factnameh, Matsda2sh,
    and Misbar – all are currently focusing on coronavirus fact-checking. [BBC Monitoring](https://www.bbc.co.uk/news/world-middle-east-51677530)
    has done some work to dispel misinformation on Persian social media, however their
    analyses are only in the English language ([Sardarizadeh, Feb. 29, 2020](https://www.bbc.com/news/world-middle-east-51677530)).
    \n\nHowever, this region lacks fact-checking efforts, social media platforms have
    also never prioritized collaborating with third-party fact checkers in the region.To
    this date, Facebook, for example, has only two partners that cover the entire
    MENA region ([Elswah & Howard, Mar. 29, 2020](https://comprop.oii.ox.ac.uk/research/tunisia-election-memo/)).
    While social media platforms have spent resources in directing their users to
    accurate coronavirus-information in the MENA region, a lot still needs to be done,
    including developing their algorithms, content moderation, and fact-checking.\n\n###
    Conclusions\n\nThe world is currently witnessing an unprecedented challenge that
    threatens people’s lives. The MENA region has long been suffering from other problems
    like illiteracy, corruption, and authoritarianism. Coronavirus has added one more
    challenge to this region’s unprepared governments. In this short report, we presented
    how coronavirus has been used for religious and political polarization. While
    we focused our attention on Iran -the epicenter of the virus in the region- there
    are systemic issues that plague the information environment in Iran that are reflected
    into the rest of the region. From issues of unofficial religious actors, lack
    of trust in authorities, and political polarisations. \n\nWhile Islam encourages
    Muslims to verify information and prohibit its followers from misleading others,
    some unofficial religious figures used their authority and connections to the
    political elite to mislead their communities and have remained unpenalized. Religious
    misinformation has had the ability to further politicize government responses
    to this pandemic. From shifting government accountability about appropriate measures
    to quarantine epicentres, to using excuses to persecute religious minorities in
    Saudi Arabia and Bahrain. Politicization of the pandemic across geopolitical tensions
    has been a particular highlight in the spread of misinformation from officialdom
    across the MENA region. \n\nConspiracy theories about American bio-attacks against
    Iran have led to pressure to go as far as to stop Doctors Without Borders from
    setting up and manning a hospital in the city of Isfahan as a continuation of
    long-standing projects in Iran and coordinated with the country’s Ministry of
    Health ([MSF, Mar. 24, 2020](https://www.doctorswithoutborders.org/what-we-do/news-stories/story/iranian-authorities-revoke-approval-msf-coronavirus-treatment-center);
    [Motevalli, Mar. 24, 2020](https://www.bloomberg.com/news/articles/2020-03-24/french-charity-msf-says-iran-coronavirus-aid-is-on-hold)).
    Information control practices such as internet censorship and arrests of critics
    of the government has further added to the deficit in public trust against governments
    in this region. This has exacerbated the spread of misinformation, leading to
    hundreds of fatalities. \n\nWhile the MENA region has not monopolized examples
    of coronavirus misinformation, the examples we outlined above highlight the deeper
    problems of authoritarianism, literacy (or digital literacy), sectarian tensions,
    and geopolitical conflicts that are worsening what should be a region united—in
    terms of nations, regions, and religions—against a global pandemic that knows
    no borders. "
  footnotes:
  - sys:
      id: 7DM0hR46JaInBD5GPlI09s
      created_at: !ruby/object:DateTime 2020-06-16 20:14:55.387000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:05:18.791000000 Z
      content_type_id: footnote
      revision: 1
    body: The Revolutionary Guards are a separate military force from Iran’s national
      military, tasked with protecting the Islamic Revolution.
  - sys:
      id: 5tGdS2Q5xC833V0yJL1ERD
      created_at: !ruby/object:DateTime 2020-06-16 20:15:05.313000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:05:15.486000000 Z
      content_type_id: footnote
      revision: 1
    body: 'In the Quran, spreading unverified information is prohibited: “O ye who
      believe! If a wicked person comes to you with any news, ascertain the truth,
      lest ye harm people unwittingly, and afterwards become full of repentance for
      what ye have done” (Quran: 49: 6)'
  - sys:
      id: 2PCziqAyq1Ak8fZqmq1stC
      created_at: !ruby/object:DateTime 2020-06-16 20:15:17.563000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:05:12.351000000 Z
      content_type_id: footnote
      revision: 1
    body: Ayatollah is an honorific title for high ranking Shiite clerics. Iran’s
      clerical government and Shiite seminaries have previously stripped clerics with
      the title of “Ayatollah” for political reasons that criticized the Islamic Republic’s
      system (Sherril 2018).
  - sys:
      id: 70uKOnW0zp81xTFuxYMhNX
      created_at: !ruby/object:DateTime 2020-06-16 20:15:21.329000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:05:09.342000000 Z
      content_type_id: footnote
      revision: 1
    body: A judge later acquitted all three of the charges on 22 April.
  - sys:
      id: 4GshZlIOUWlXwOwB9PYblK
      created_at: !ruby/object:DateTime 2020-06-23 18:21:10.055000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:05:06.528000000 Z
      content_type_id: footnote
      revision: 1
    body: '[UK tabloids](https://www.dailypost.co.uk/news/north-wales-news/tragic-link-between-north-wales-18019185)
      falsely claim the story of the British man’s use of Whisky while he had coronavirus
      became viral misinformation in Iran leading to alcohol poisoning. After our
      own search of Persian social media, there is no little evidence this was a widely
      shared or impactful piece of news in Persian social media. Other news sources
      such as the [Independent](https://www.independent.co.uk/news/world/middle-east/iran-coronavirus-methanol-drink-cure-deaths-fake-a9429956.html)
      and the [Associated Press](https://www.theglobeandmail.com/world/article-hundreds-killed-in-iran-over-false-belief-that-drinking-methanol-can/)
      have also promoted the false theory that the increase in alcohol deaths is because
      of “fake remedies spread across social media in Iran".'
  - sys:
      id: 3NNZuWTtlUAEEPtkH16KMk
      created_at: !ruby/object:DateTime 2020-06-16 20:15:27.038000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:05:02.402000000 Z
      content_type_id: footnote
      revision: 1
    body: On April 7 2020 WhatsApp announced it was [restricting the number of people
      users could forwarded messages](https://slate.com/technology/2020/04/whatsapp-message-forwarding-disinformation-coronavirus.html)
      to in order to slow the spread of coronavirus misinformation.
  - sys:
      id: 5e6lOwlcHvBAAE2MLuIeSL
      created_at: !ruby/object:DateTime 2020-06-16 20:15:33.062000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:04:59.087000000 Z
      content_type_id: footnote
      revision: 1
    body: 'On April 3, 2020 Telegram introduced its [channel verification procedures](https://telegram.org/blog/coronavirus). '
  references: |-
    - ARTICLE19. 2020. “[Iran: Coronavirus Crisis Highlights Need for the Free Flow of Information](https://www.article19.org/resources/iran-coronavirus-crisis-highlights-need-for-the-free-flow-of-information/).” ARTICLE 19. March 27, 2020.
    - Asr Iran. 2020. '[The Announcement of Mashad's Medical Society on the Conviction of 3 Doctors for Insult to Tabrizian](https://www.asriran.com/fa/news/712989)'. عصر ايران. World. 9 February 2020.
    - Bahar News. 2020. “[The Death of a Young Citizen of Anzali After a Visit from Kohsal!](http://www.baharnews.ir/news/206368/%D8%AF%D8%B1%DA%AF%D8%B0%D8%B4%D8%AA-%D8%AC%D9%88%D8%A7%D9%86-%D8%A7%D9%87%D9%84-%D8%A7%D9%86%D8%B2%D9%84%DB%8C-%D8%AF%DB%8C%D8%AF%D8%A7%D8%B1-%D9%83%D9%87%D9%86%D8%B3%D8%A7%D9%84)” پایگاه خبری بهار نیوز. March 23, 2020.
    - BBC News. 2013. “[Sunnis and Shia in the Middle East.](https://www.bbc.co.uk/news/world-middle-east-25434060)” Accessed March 29, 2020.
    - BBC. 2020. '[Coronavirus Kills 210 in Iran - Hospital Sources](https://www.bbc.com/news/world-middle-east-51673053)'. BBC News, 28 February 2020, sec. Middle East.
    - BBC Persian. 2020. ‘[Rouhani: The Conspiracy of Enemies to Create Coronavirus Fear in Order to Shut the Country Down](https://www.bbc.com/persian/iran-51630282)’. 24 February 2020.
    - ––. n.d. ‘[Iran’s Ministry of Health Denied Sending Letters of Warning About Coronavirus](https://www.bbc.com/persian/iran-51521216)’. 16/02/2020. Accessed 3 May 2020
    - Behravesh, Maysam. 2020. ‘[“Funeral for Public Trust”: New Crisis in Iran after Plane Crash](https://www.aljazeera.com/news/2020/01/iran-plane-shoot-protest-crisis-competency-legitimacy-200113151404256.html)’. 14 January 2020.
    - Borger, Julian. 2020. ‘[US Ignores Calls to Suspend Venezuela and Iran Sanctions amid Coronavirus Pandemic](https://www.theguardian.com/world/2020/mar/31/us-ignores-global-appeals-suspend-sanctions-coronavirus-pandemic-iran-venezuela)’. The Guardian, 31 March 2020, sec. World news.
    - Bozorgmehr, N. (2020, June 17). '[How Iran’s clergy fought back against coronavirus](https://www.ft.com/content/8e9b50bb-ebf7-4702-9894-1f2081ae869a)'.
    - Centre for Human Rights in Iran. 2017. ‘[After Fiery Speech, Iran Censors Ahmadinejad-Related Online Searches](http://www.iranhumanrights.org/2017/11/after-fiery-speech-iran-censors-ahmadinejad-related-online-searches/)’. 29 November 2017.
    - Elswah, Mona, and Philip N. Howard. 2020. “[The Challenges of Monitoring Social Media in the Arab World: The Case of the 2019 Tunisian Elections.](https://comprop.oii.ox.ac.uk/research/tunisia-election-memo)” _The Computational Propaganda Project_. Accessed March 29, 2020.
    - Faghihi, Rohollah. 2020. “[A Cleric’s Cure for Coronavirus Becomes Butt of Jokes in Iran.](https://www.al-monitor.com/pulse/originals/2020/03/bizarre-cures-for-coronavirus-in-iran.html)” Al-Monitor. March 10, 2020.
    - Faramarzi, Scheherazade. 2020. “[Iranian officials troll the US handling of the coronavirus to hide their own failings.](https://atlanticcouncil.org/blogs/iransource/iranian-officials-troll-the-us-handling-of-the-coronavirus-to-hide-their-own-failings/)” Accessed April 30, 2020.
    - Festinger, Leon. 1957. _“A theory of cognitive dissonance.”_ California: Stanford University Press.
    - Freedom House. 2019. “[Freedom on the Net.](https://freedomhouse.org/report/freedom-net)” Freedom House. Accessed March 29, 2020.
    - Graphika. 2020. ‘[Iran’s IUVM Turns To Coronavirus](https://graphika.com/reports/irans-iuvm-turns-to-coronavirus/)’. 14 April 2020.
    - Haghdoost, Yasna, and Golnar Motevalli. 2020. “[Iran’s Khamenei Says Virus Outbreak May Be ‘Biological Attack.’](https://www.bloomberg.com/news/articles/2020-03-12/iran-s-khamenei-says-virus-outbreak-may-be-biological-attack)” _Bloomberg News_, March 12, 2020.
    - Hamshahri Online. 2020. ‘[Victims of Alcohol Poisoning Outnumber Coronavirus in Khuzestan](http://newspaper.hamshahrionline.ir/)’. 14 March 2020.
    - Hekmat, Reza, Fariboorz Samini, Bita Dadpour, Faezeh Maghsudloo, and Mohammad Javad Mojahedi. 2012. ‘[Should Guidelines for Conventional Hemodialysis Initiation in Acute Methanol Poisoning, Be Revised, When No Fomepizole Is Used?](https://doi.org/10.5812/ircmj.3467)’ Iranian Red Crescent Medical Journal. Kowsar. November 15, 2012.
    - Human Rights Watch. 2019. ‘[Iran: Sanctions Threatening Health](https://www.hrw.org/news/2019/10/29/iran-sanctions-threatening-health)’. 29 October 2019.
    - Islamic Azad University News Agency. 2020. “[Which institution didn’t give permission to quarantine Qom?](https://doi.org//fa/news/15/477237/%DA%86%D9%87-%D8%AF%D8%B3%D8%AA%DA%AF%D8%A7%D9%87%DB%8C-%D8%A7%D8%AC%D8%A7%D8%B2%D9%87-%D9%82%D8%B1%D9%86%D8%B7%DB%8C%D9%86%D9%87-%D8%B4%D9%87%D8%B1-%D9%82%D9%85-%D8%B1%D8%A7-%D9%86%D8%AF%D8%A7%D8%AF)” March 11, 2020.
    - IRNA. 2020. “[Foreign media tried to use COVID19 to shut Iran down.](https://www.irna.ir/news/83789038)” May 16, 2020.
    - ISNA. 2020. “[An Announcement of Disgust Against the Burning of Harrison’s Medical Science Book.](https://bit.ly/3asZByZ)” January 25, 2020.
    - ISPA. 2018. “[Migration from Telegram to WhatsApp](http://ispa.ir/Default/Details/fa/2095)”. Accessed April 30, 2020.
    - Karimi, Nasser, and Jon Gambrell. 2020. “[In Iran, False Belief a Poison Fights Virus Kills Hundreds.](https://apnews.com/6e04783f95139b5f87a5febe28d72015)” Associated Press. March 27, 2020.
    - Matthiesen, Toby. 2020. “[The Coronavirus Is Exacerbating Sectarian Tensions in the Middle East](https://www.foreignaffairs.com/articles/middle-east/2020-03-23/coronavirus-exacerbating-sectarian-tensions-middle-east),” March 23, 2020.
    - Motevalli, Golnar. 2020. ‘[Medical Charity MSF Says Iran Coronavirus Aid Is “On Hold” - Bloomberg](https://www.bloomberg.com/news/articles/2020-03-24/french-charity-msf-says-iran-coronavirus-aid-is-on-hold)’. 24 March 2020.
    - MSF. 2020. ‘[Iranian Authorities Revoke Approval for MSF Coronavirus Treatment Center](https://www.doctorswithoutborders.org/what-we-do/news-stories/story/iranian-authorities-revoke-approval-msf-coronavirus-treatment-center)’. 24 March 2020.
    - OHCHR. 2020. ‘[Bachelet Calls for Easing of Sanctions to Enable Medical Systems to Fight COVID-19 and Limit Global Contagion](https://www.ohchr.org/EN/NewsEvents/Pages/DisplayNews.aspx?NewsID=25744&LangID=E)’. 24 March 2020.
    - RFE/RL. 2020. “[Man Seen Licking Shrine Grids Despite Coronavirus Arrested In Iran.](https://en.radiofarda.com/a/man-seen-licking-shrine-grids-despite-coronavirus-arrested-in-iran/30462926.html)” RFE/RL. Accessed March 29, 2020.
    - RFE/RL. 2019. ‘[Successor To Khamenei Died Because He Trusted Islamic Medicine, Son Reveals](https://en.radiofarda.com/a/successor-to-khamenei-died-because-he-trusted-islamic-medicine-son-reveals/30349899.html)’. RFE/RL. 29 December 2019.
    - ––. 2020. ‘[Iran Arrests Editor, Journalist Over Cartoon Mocking Khamenei](https://www.rferl.org/a/iran-arrests-editor-journalist-over-cartoon-mocking-khamenei/30581070.html)’. 29 April 2020.
    - Saeedian, Mahdiar. 2020. “[Ayatollah Kohsal Administering Islam Medicine on Critical Coronavirus Patients.](https://twitter.com/mahdiarsaeedian/status/1241289478301499392)” Twitter. March 21, 2020.
    - Sardarizadeh, Shayan. 2020. “[Coronavirus Misinformation Clouds over Iran.](https://www.bbc.com/news/world-middle-east-51677530)” _BBC News_, February 29, 2020, sec. Middle East.
    - Statista. 2010. “[Population in Middle East/North Africa by Religion 2010.](https://www.statista.com/statistics/374759/population-in-middle-east-north-africa-by-religion/)” Statista. Accessed March 29, 2020.
    - Tasnim News. 2020. ‘Rouhani: the media should not become polarized/ we should calm the society about coronavirus’. 14 March 2020.
    - Young Journalists Club. 2020a. ‘[The Promoter of the proponent of drinking camel urine is arrested](http://www.yjc.ir/fa/news/7327494)’. خبرگزاری باشگاه خبرنگاران آخرین اخبار ایران و جهان  YJC. World. 22 April 2020.
    - ––. 2020b. ‘[The first interview of the swindler of camel urine](http://www.yjc.ir/fa/news/7328801)’. Text,Image. fa. خبرگزاری باشگاه خبرنگاران آخرین اخبار ایران و جهان  YJC. World. 23 April 2020.
  alt_date: !ruby/object:DateTime 2020-06-23 00:00:00.000000000 Z
  author:
  - sys:
      id: pBXwSnwaN2YJp4KJc9uRg
      created_at: !ruby/object:DateTime 2020-06-16 00:03:57.410000000 Z
      updated_at: !ruby/object:DateTime 2020-08-10 20:16:34.986000000 Z
      content_type_id: contributor
      revision: 3
    name: Mahsa Alimardani
    bio: Mahsa Alimardnai is a doctoral candidate at the Oxford Internet Institute
      (OII), where she examines political communication online in Iran. She's a researcher
      on the MENA region for the freedom of expression organisation ARTICLE19 and
      a Senior Information Control Fellow for the Open Technology Fund (OTF).
  - sys:
      id: 3dw2wNKNhNmWlCRxSpvQ0y
      created_at: !ruby/object:DateTime 2020-06-16 00:04:03.128000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:05:26.777000000 Z
      content_type_id: contributor
      revision: 1
    name: Mona Elswah
    bio: Mona Elswah is a doctoral candidate at the Oxford Internet Institute (OII),
      where she examines social movements in the Arab World. She is also a researcher
      and a core member of the Computational Propaganda project (COMPROP) with a focus
      on the MENA region and authoritarian regimes. Before joining the OII, Mona was
      awarded a Ford Foundation fellowship to study at the American University in
      Cairo. She has a master’s degree in journalism and mass communication and a
      Graduate Diploma in public policy.
  publication:
  - sys:
      id: 2lQKVWqTIPTns0oj5UA78S
      created_at: !ruby/object:DateTime 2020-06-15 19:58:33.906000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:12:32.900000000 Z
      content_type_id: publication
      revision: 6
    title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
    short_title: 2020 Misinfodemic Report
    kicker: A Check Global Report
    description: 'Misinfodemic: when online misinformation contributes to real-world
      disease. In the coming months, Meedan will publish eight pieces of reporting
      and analysis in an effort to map the anatomy of the global COVID-19 information
      ecosystem.'
    slug: 2020-misinfodemic-report-covid-19-in-emerging-economies
    sections:
    - sys:
        id: 6K6TuAvZBDXdJATv8SynCi
    - sys:
        id: 1jkEcTAbvK0Y7O1NmysvKT
    - sys:
        id: 3ixzizcKVwnSXEFRRyVuxL
    cover_image:
      sys:
        id: Tu2ovEuxh8gztQD9kuF0G
        created_at: !ruby/object:DateTime 2020-06-23 05:08:56.807000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:33.955000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/Tu2ovEuxh8gztQD9kuF0G/0f5da326dfe06869775caf1002c330e6/meedan_report_2020-infodemic-report_01.png"
    publication_promo_graphic:
      sys:
        id: 5wkwfNNTjvV1Cgf2FAQFD4
        created_at: !ruby/object:DateTime 2020-04-16 20:11:43.061000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:37.263000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5wkwfNNTjvV1Cgf2FAQFD4/fbdc448905c4b9240944587574577654/Meedan_global-misinfodemic-report.png"
    social_card_image:
      sys:
        id: 5L7KfUhYU7lqikodPkJoQQ
        created_at: !ruby/object:DateTime 2020-06-22 23:00:20.044000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:39.651000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5L7KfUhYU7lqikodPkJoQQ/7e82797cad59e546a8fc7e5f4cb23a87/meedan_report_2020-misinfodemic-report_01.png"
    publication_promo: This report is part of a Check Global Report about [COVID-19
      in Emerging Economies](https://meedan.com/reports/2020-misinfodemic-report-covid-19-in-emerging-economies/).
    color:
      sys:
        id: 43kw5bcVj1xtfe7ua88F6V
  color:
    sys:
      id: 1TCGlH6d0n4BiXTMpDB7Y7
      created_at: !ruby/object:DateTime 2020-06-15 23:44:13.060000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:10:30.292000000 Z
      content_type_id: color
      revision: 1
    name: Orange
    value: "#FF864D"
    thumbnail:
      sys:
        id: 6ioZnbN9f9SjY2XZfp46QG
        created_at: !ruby/object:DateTime 2020-06-15 23:50:31.815000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:10:27.575000000 Z
      title: ffb34d
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/6ioZnbN9f9SjY2XZfp46QG/28488ebd1d9e4b4cd2dbc9ba34074735/ffb34d.png"
    text: "#262626"
  image: https://images.ctfassets.net/g118h5yoccvd/1TXTPQACmPJLvpvGpSNDPO/bee0b505859726cfeace88c3c4a15486/meedan_report_covid-19-iran_01.png
- sys:
    id: 3FjVXrJAIvjk2zHMcvRxtq
    created_at: !ruby/object:DateTime 2020-06-17 23:32:00.982000000 Z
    updated_at: !ruby/object:DateTime 2020-11-12 01:14:07.229000000 Z
    content_type_id: report
    revision: 4
    space:
      sys:
        id: g118h5yoccvd
    type: Entry
    content_type:
      sys:
        id: report
    environment:
      sys:
        id: master
        type: Link
        linkType: Environment
    locale: en-US
  title: False health information in Kenya
  slug: false-health-information-in-kenya
  description: 'A surge in false claims linked to COVID-19 in Kenya follows existing
    misinformation trends in the country.

    '
  lead_image:
    sys:
      id: 2FaNwN7eNAIKR9NSFae2xD
      created_at: !ruby/object:DateTime 2020-06-18 00:04:24.098000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:06:03.021000000 Z
    title: kenya
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/2FaNwN7eNAIKR9NSFae2xD/fb01cfa24e810a121848317a6fe9a830/kenya.png"
  social_card_image:
    sys:
      id: 3qrqeRUJmXadMqSKn51oaF
      created_at: !ruby/object:DateTime 2020-06-22 23:22:05.621000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:06:06.178000000 Z
    title: False health information in Kenya
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/3qrqeRUJmXadMqSKn51oaF/bfb84d389358017676bccf23d25d039f/meedan_report_covid-19-kenya_africa-check_01.png"
  introduction: In this paper, we focus on our experience of harmful false information
    about health in Kenya. We identify major trends and discuss the contextual factors
    influencing mis- and disinformation in the country. We show that while there has
    been a surge in false information linked to Covid-19, this is not new to the country,
    with health myths that lead to vaccine hesitancy having a long tail, for example.
    Finally, we suggest areas that fact-checkers can look into to counter harmful
    health mis- and disinformation.
  body: "### Introduction\n\nFalse information about health causes real harm to people’s
    lives. Africa Check has been fact-checking public statements about health in Kenya
    since late 2016. \n\nIn our first two years of operation in the country we focused
    on claims made by public figures, institutions or the media that we judged would
    cause the most harm if left unchallenged. In that period our experience was mainly
    of **misinformation**, with many journalists, people and entities publicly sharing
    content that they likely did not know was false.\n\nThis ranged from government
    officials making inaccurate claims about the prevalence of HIV/AIDS in adolescents
    at major international meetings<sup><a id=\"fnref1\" href=\"#fn1\">[1]</a></sup>
    to poor fact-checking by both local and international media, with cited statistics
    often being either deficient<sup><a id=\"fnref2\" href=\"#fn2\">[2]</a></sup>
    or non-existent<sup><a id=\"fnref3\" href=\"#fn3\">[3]</a></sup>.\n\nOur experience
    of **disinformation** (deliberately falsified information) in that period included
    dangerous claims about vaccines, with significant opposition fanned by conservative
    groups to tetanus vaccines<sup><a id=\"fnref4\" href=\"#fn4\">[4]</a></sup> for
    example. Churches play a major role in Kenyan society and were quite instrumental
    in seeding this. We have seen a resurgence of disinformation around vaccines due
    to the Covid-19 pandemic, including claims<sup><a id=\"fnref5\" href=\"#fn5\">[5]</a></sup>
    that the continent would be used for trials.\n\nThere has also been broader public
    awareness about disinformation in recent years, resulting in more people flagging
    questionable health content to us. This has given us greater insight into the
    types of false information shared in the country, from the occasional hoax to
    outright fabricated content and miracle cures for cancer<sup><a id=\"fnref6\"
    href=\"#fn6\">[6]</a></sup> and other diseases of concern to Kenyans. \n\nBut
    we have noted **two key trends** in the country that are pre-eminent:\n\n- The
    use of **false context** (including false connection and clickbait headlines)\n-
    \ **Manipulation** of genuine content\n\nDecontextualisation often involves the
    use of genuine imagery re-shared as new, whether during everyday discussions of
    public health or during a major news event such as the Covid-19 crisis. Certain
    media, such as pictures and videos, have an intrinsic ability to make text appear
    more believable.\n\nManipulation is mainly doctored or tampered-with content.
    It is also notable that the two types of disinformation provide a low “barrier
    to entry” compared to say, deepfakes — or even shallowfakes.\n\nWhat the two have
    in common is the use of genuine content as a starting point. But there is also
    an added element of profit-seeking involved where believability is used to scam
    people or drive traffic to websites. This is rather surprising as we expected
    that, for reasons of structural deficiency around healthcare, including affordability,
    most false information would be about cures. \n\nHowever, it isn’t completely
    dissociated. The use of genuine content as a jumping-off point provides a sort
    of beachhead strategy into the other forms of disinformation mentioned, whether
    being screenshots appearing to be from credible sources that push false information
    to those that rely on heuristics to stoke emotion and drive virality.   \n\nThis
    is important from the perspective of both amplification and fact-checking, because
    studies show that content with a kernel of truth tends to be most shared<sup><a
    id=\"fnref7\" href=\"#fn7\">[7]</a></sup>, and is also best countered by acknowledging
    this<sup><a id=\"fnref8\" href=\"#fn8\">[8]</a></sup>.\n\n### Contextual factors
    influencing mis- and disinformation in Kenya \n\nA number of factors are at play.
    All broadly coalesce around financial, political, and social and cultural motivations.
    \n\n- Of the 9.9 million people who use the internet in the country, 6.3 million
    — nearly two-thirds (64%) — are young people aged between 15 and 34<sup><a id=\"fnref9\"
    href=\"#fn9\">[9]</a></sup>. **But digital literacy remains low, with a notable
    digital divide where many are offline.** Our research<sup><a id=\"fnref10\" href=\"#fn10\">[10]</a></sup>
    also shows that while all demographics are vulnerable to false information, older
    people and people with lower levels of education in general are at increased fallibility.
    \n\n- **Youth unemployment** is also high<sup><a id=\"fnref11\" href=\"#fn11\">[11]</a></sup>,
    with many young people seeking to leverage the country’s increased connectivity
    to gain from a growing digital economy. This informs why false health information
    often involves scams seeking to make profit by duping potential job applicants,
    sites looking to make money off advertising and those people looking to sell hope
    to the desperate through false and untested cures. \n\n- There is also **low investment
    in quality public healthcare**, while private hospitals are largely still out
    of reach for most. In many families in Kenya, a cancer diagnosis is a sure path
    to financial ruin. This makes many Kenyans particularly vulnerable to unproven
    cures being peddled on social networks.\n\n- **Culture** also plays a role. Worldviews
    in Kenya tend to lean towards the communal rather than the ideological, and are
    very experiential. Community leaders for example have a lot of influence on society,
    and beliefs play an elevated role in how information is processed. False cures
    and anti-vaccination positions pushed by figures of authority are some of the
    ways this is manifested.Related to this is an inherent elevation of social consensus
    information. There is an innate belief that because a certain piece of information
    is being shared in groups Kenyans are members of (such as Whatsapp groups), it
    is being done in good faith and that it also is “pre-verified” and accurate. This
    informs why authority figures successfully share false information — such as a
    prominent senator who claimed<sup><a id=\"fnref12\" href=\"#fn12\">[12]</a></sup>
    soya causes breast cancer, or Nairobi’s incumbent governor claiming<sup><a id=\"fnref13\"
    href=\"#fn13\">[13]</a></sup> that alcohol is a ‘throat disinfectant’ against
    the new coronavirus.\n\n- There is also **a distrust of authority**, though not
    of experts, giving alternative voices such as bloggers and influencers a growing
    audience. These invest their time capital into the system for a return in social
    capital (likes and follows), and in algorithmic capital. Facebook groups and trending
    topics are for example quite influential in Kenya, and the players use this mainly
    for economic capital, in a sort of permissive feedback loop well outlined by Danil
    Mikhailov<sup><a id=\"fnref14\" href=\"#fn14\">[14]</a></sup>.\n\n### Examples
    of mis- and disinformation in Kenya\n\n#### Incorrect public pronouncements not
    based on, and/or misrepresenting verifiable data\n\nNairobi governor Mike Sonko
    [includes alcohol in food packs](https://africacheck.org/spot-check/nairobi-governor-sonko-puts-families-at-risk-with-coronavirus-busting-alcohol-in-food-packs/)
    to poor families, claiming that “alcohol plays a very major role in killing the
    coronavirus or any sort of virus”.\n\n#### False cures/prevention regimens\n\nFalse
    cancer cures from [a mysterious ‘Dr Gupta’](https://africacheck.org/spot-check/beware-false-cancer-cures-from-dr-gupta-on-facebook/)
    — that cancer will “disappear” if patients stop eating sugar, drink lemon juice
    before meals for three months, and take spoonfuls of coconut oil morning and night
    — go viral on Facebook.\n\n#### Hoaxes and scams\n\n[Job openings at WHO for a
    high salary](https://africacheck.org/fbcheck/coronavirus-job-scam-no-world-health-organization-kenya-not-hiring-on-facebook/):
    applicants were asked to submit their personal information, with enough information
    being asked for to initiate identity theft. \n\n#### Manipulated and/or out-of-context
    videos and images\n\n [‘Every Kenyan to receive 100K due to the lockdown on corona’](https://africacheck.org/fbcheck/no-false-news-of-ksh100000-payout-for-every-kenyan-during-covid-19-lockdown-not-broadcast-on-citizen-tv/)\n\n####
    Conspiracy theories and predictions\n\n“[Barack Obama is asking Africans not to
    accept vaccines that will come from America and Europe](https://africacheck.org/fbcheck/no-former-us-president-obama-didnt-warn-africans-against-coronavirus-vaccines/)”.\n\n###
    Covid-19 disinformation in Kenya\n\nAfrica Check has looked at false information
    about Covid-19 mainly through the lens of disinformation, with content where the
    intent could be safely assessed as not meant to mislead or cause harm left out.
    Since March 2020 we have maintained an updated “[live guide](https://africacheck.org/reports/live-guide-all-our-coronavirus-fact-checks-in-one-place/)”,
    with decontextualized content — imagery and articles — forming the bulk of the
    entries for Kenya, for reasons discussed elsewhere in this report, key being the
    low barriers to entry. \n\n**Hoaxes**, fabricated content about non-existent events
    and entities, **half-truths,** where truths swim in the same waters as misleading
    claims, and **scams** with a financial motive are also key players in the Kenyan
    false information ecosystem. The prevalence of half-truths is especially effective
    — infusing false claims with plausible ones adds to the overall believability.
    \n\n**Cures and vaccines** are the next main types of pandemic disinformation.
    Many claims about cures come with a profit element in the background, such as
    buying a “natural” product or visiting a practitioner. Claims about vaccines are
    often caught up in the country’s strong conservative beliefs or protests about
    Africans being guinea pigs. \n\n### How false information about health in Kenya
    compares internationally\n\nLike in many other countries, the false health information
    that is shared in Kenya tends to be that which evokes emotional responses. But
    there are some differences.\n\nA lot of health mis- and disinformation in the
    Global North tends to be grounded in **ideology**, but there is little evidence
    of this in Kenya. This is a reason why harmful false information that is Kenya-specific
    originates on social and professional media, as opposed to, say, the anonymous
    web or conspiracy communities as is often the case elsewhere.\n\n**Relationships**
    also come before purpose in Kenya. Mis- and disinformation that taps into these
    networks — such as families and communities — is quite widely shared. \n\nFurther,
    due to how information leaches into the community (community meetings are crucibles
    for the individual to act on (false) information), disinformation often has more
    impact — for example a lot of stigma around Covid-19 patients has been experienced
    in Kenyan communities, sometimes with fatal outcomes. \n\n### Conclusion\n\nThe
    entire information ecosystem — government, the media, big tech, fact-checkers
    and audiences — has a role to play in countering health mis- and disinformation.
    \n\nBut there are some areas fact-checkers can look into:\n\n- Disinformation
    in particular in Kenya very often thrives by leveraging on **relationships**,
    because a lot of people have an emotional rather than rational or logical relationship
    to information. \n- **Indigenisation of fact-checking is important**: Our fact-checking
    work in 11 other African languages generated a lot of interest because it helps
    break accurate information out of the urban ivory towers where it is still very
    much concentrated. Others like the BBC have gone into localisation of content
    in much more detail<sup><a id=\"fnref15\" href=\"#fn15\">[15]</a></sup>.\n- More
    research into **how individual/societal differences such as culture affect responses
    to false** **health** **information** is needed. We need to better understand
    the circumstances that make fact-checking have any impact on politicians and how
    to make it a durable effect. How can we persuade the alcohol-peddling Nairobi
    governor to check himself?\n- Should we cloak up like mis/disinformation peddlers?
    For example, we know that adding an image to a tweet can make the difference between
    a fact-check that gets seen, and one that is outranked by more attention-grabbing
    posts. We’ve in the past debunked false health information with video, and we
    are already sending audio to our audiences. Should it be a standard? This is very
    much a point of debate<sup><a id=\"fnref16\" href=\"#fn16\">[16]</a></sup>."
  footnotes:
  - sys:
      id: 2DkAXIG4Ntyrbm3HDpNXq4
      created_at: !ruby/object:DateTime 2020-06-17 23:45:18.750000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:06:55.948000000 Z
      content_type_id: footnote
      revision: 1
    body: "[No, adolescents don’t account for half of new HIV infections in Kenya](https://africacheck.org/reports/no-adolescents-dont-account-for-half-of-new-hiv-infections-in-kenya/)."
  - sys:
      id: 15omfxfgnFxiqZMm2iJbwM
      created_at: !ruby/object:DateTime 2020-06-17 23:53:25.855000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:06:59.383000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Little evidence that 10,000 children abused by sex tourism in Kenyan town](https://africacheck.org/reports/little-evidence-that-10000-children-abused-by-sex-tourism-in-kenyan-town/)."
  - sys:
      id: 2btJHRj6ClEbAdolu2nyDE
      created_at: !ruby/object:DateTime 2020-06-17 23:53:49.634000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:07:02.498000000 Z
      content_type_id: footnote
      revision: 1
    body: "[‘65% of females’ in Kenyan slum traded sex for sanitary pads? Unicef &
      UK newspaper pull stat from thin air.](https://africacheck.org/reports/65-of-females-in-kenyan-slum-traded-sex-for-sanitary-pads-unicef-uk-newspaper-pull-stat-from-thin-air/)"
  - sys:
      id: 41a1CgPv7IKlAsuEDl2q0G
      created_at: !ruby/object:DateTime 2020-06-17 23:54:26.278000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:07:05.420000000 Z
      content_type_id: footnote
      revision: 1
    body: "[ANALYSIS: What could Kenya’s Odinga gain by dredging up an unfounded tetanus
      vaccine claim?](https://africacheck.org/2017/09/13/analysis-what-could-kenyas-odinga-gain-by-dredging-up-an-unfounded-tetanus-vaccine-claim/)"
  - sys:
      id: 5pz4bl2tUvvzVRKwRV6oek
      created_at: !ruby/object:DateTime 2020-06-17 23:55:05.584000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:07:09.314000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Bill Gates not testing Covid-19 vaccine in Africa](https://africacheck.org/fbcheck/bill-gates-not-testing-covid-19-vaccine-in-africa/)"
  - sys:
      id: 7ts93qPQn2DJMucLcUwUuX
      created_at: !ruby/object:DateTime 2020-06-17 23:55:25.799000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:07:28.129000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Beware false cancer cures from ‘Dr Gupta’ on Facebook](https://africacheck.org/spot-check/beware-false-cancer-cures-from-dr-gupta-on-facebook/)"
  - sys:
      id: 6HxAJhcWSzs4OQMmrcWSsL
      created_at: !ruby/object:DateTime 2020-06-17 23:55:45.169000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:07:28.092000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Communicating fact checks online](https://africacheck.org/wp-content/uploads/2020/02/EN-How-to-communicate-a-fact-check-online.pdf)."
  - sys:
      id: bbVGW71L6ZcXPBcxtegG7
      created_at: !ruby/object:DateTime 2020-06-17 23:56:47.971000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:07:28.124000000 Z
      content_type_id: footnote
      revision: 1
    body: "[COMMENT: Fighting falsehood, reducing harm? Lessons from Africa’s complex
      countries](https://africacheck.org/2019/05/31/comment-fighting-falsehood-reducing-harm-lessons-from-africas-complex-countries/)"
  - sys:
      id: 1vT206XBr6DA2lPQGULslF
      created_at: !ruby/object:DateTime 2020-06-17 23:57:12.944000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:07:28.097000000 Z
      content_type_id: footnote
      revision: 1
    body: 'Kenya National Bureau of Statistics (2020). 2019 Kenya Population and Housing
      Census. Volume IV: Distribution of Population by socioeconomic characteristics.
      Nairobi: KNBS'
  - sys:
      id: 6Mmmaj34l25JoPl9F6DPkY
      created_at: !ruby/object:DateTime 2020-06-17 23:57:23.574000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:07:27.843000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Who is most likely to believe and to share misinformation?](https://africacheck.org/wp-content/uploads/2020/02/EN-Who-believes-and-shares-misinformation.pdf)"
  - sys:
      id: 67vmIwn0dwu45qxHig8YRG
      created_at: !ruby/object:DateTime 2020-06-17 23:58:38.416000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:07:27.969000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Kenya’s youth unemployment at 39%? Why this headline-grabbing number is
      wrong](https://africacheck.org/reports/kenyas-youth-unemployment-at-39-why-this-headline-grabbing-number-is-wrong/)."
  - sys:
      id: 1rg9qDm999vKSTrsosM5HL
      created_at: !ruby/object:DateTime 2020-07-22 22:57:06.057000000 Z
      updated_at: !ruby/object:DateTime 2020-07-22 22:57:51.890000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Kenyan senator wrong about breast cancer risk from soya](https://africacheck.org/reports/kenyan-senator-wrong-about-breast-cancer-risk-from-soya/)"
  - sys:
      id: b9py99eBZgKx07AZPVsVp
      created_at: !ruby/object:DateTime 2020-06-17 23:58:58.217000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:07:27.862000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Nairobi governor Sonko puts families at risk with ‘coronavirus-busting’
      alcohol in food packs](https://africacheck.org/spot-check/nairobi-governor-sonko-puts-families-at-risk-with-coronavirus-busting-alcohol-in-food-packs/)."
  - sys:
      id: 67qc59CNM18PcqWxyqXE8F
      created_at: !ruby/object:DateTime 2020-06-17 23:59:21.737000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:07:27.849000000 Z
      content_type_id: footnote
      revision: 1
    body: "[How spreaders of misinformation acquire influence online](https://medium.com/@danil.s.mikhailov/how-spreaders-of-misinformation-acquire-influence-online-456757723e50)"
  - sys:
      id: 4UQVO4wOslN6fXtG6JbjvM
      created_at: !ruby/object:DateTime 2020-06-17 23:59:43.239000000 Z
      updated_at: !ruby/object:DateTime 2020-06-26 21:11:06.460000000 Z
      content_type_id: footnote
      revision: 2
    body: "[The COVID-19 WhatsApp service by WHO - is it effective?](https://www.bbc.co.uk/blogs/mediaactioninsight/entries/a6e20e60-64af-4308-9c41-be109a6265db)"
  - sys:
      id: 3pAq6hbMMlpdwrAcYns7QA
      created_at: !ruby/object:DateTime 2020-06-17 23:59:53.830000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:07:27.788000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Communicating fact checks online](https://africacheck.org/wp-content/uploads/2020/02/EN-How-to-communicate-a-fact-check-online.pdf)."
  alt_date: !ruby/object:DateTime 2020-06-23 00:00:00.000000000 Z
  author:
  - sys:
      id: 7A5mBS9Hmf7Dc9GNo6NOO9
      created_at: !ruby/object:DateTime 2020-06-18 00:03:21.562000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:08:52.862000000 Z
      content_type_id: contributor
      revision: 1
    name: Africa Check
    bio: "[Africa Check](https://africacheck.org/) is the continent’s leading independent,
      nonpartisan, fact-checking organisation. Established in 2012, we have four offices
      across the continent: in Johannesburg (South Africa), Nairobi (Kenya), Lagos
      (Nigeria) and Dakar (Senegal). Since 2012, we have fact-checked more than 1,500
      claims in English and French, focusing on areas from education and health to
      elections and the economy. Our goal is to raise the quality of information available
      to society across the continent. Ultimately, we aim to strengthen democracy
      and improve life outcomes."
  publication:
  - sys:
      id: 2lQKVWqTIPTns0oj5UA78S
      created_at: !ruby/object:DateTime 2020-06-15 19:58:33.906000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:12:32.900000000 Z
      content_type_id: publication
      revision: 6
    title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
    short_title: 2020 Misinfodemic Report
    kicker: A Check Global Report
    description: 'Misinfodemic: when online misinformation contributes to real-world
      disease. In the coming months, Meedan will publish eight pieces of reporting
      and analysis in an effort to map the anatomy of the global COVID-19 information
      ecosystem.'
    slug: 2020-misinfodemic-report-covid-19-in-emerging-economies
    sections:
    - sys:
        id: 6K6TuAvZBDXdJATv8SynCi
    - sys:
        id: 1jkEcTAbvK0Y7O1NmysvKT
    - sys:
        id: 3ixzizcKVwnSXEFRRyVuxL
    cover_image:
      sys:
        id: Tu2ovEuxh8gztQD9kuF0G
        created_at: !ruby/object:DateTime 2020-06-23 05:08:56.807000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:33.955000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/Tu2ovEuxh8gztQD9kuF0G/0f5da326dfe06869775caf1002c330e6/meedan_report_2020-infodemic-report_01.png"
    publication_promo_graphic:
      sys:
        id: 5wkwfNNTjvV1Cgf2FAQFD4
        created_at: !ruby/object:DateTime 2020-04-16 20:11:43.061000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:37.263000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5wkwfNNTjvV1Cgf2FAQFD4/fbdc448905c4b9240944587574577654/Meedan_global-misinfodemic-report.png"
    social_card_image:
      sys:
        id: 5L7KfUhYU7lqikodPkJoQQ
        created_at: !ruby/object:DateTime 2020-06-22 23:00:20.044000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:39.651000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5L7KfUhYU7lqikodPkJoQQ/7e82797cad59e546a8fc7e5f4cb23a87/meedan_report_2020-misinfodemic-report_01.png"
    publication_promo: This report is part of a Check Global Report about [COVID-19
      in Emerging Economies](https://meedan.com/reports/2020-misinfodemic-report-covid-19-in-emerging-economies/).
    color:
      sys:
        id: 43kw5bcVj1xtfe7ua88F6V
  color:
    sys:
      id: 3VU0jnxwp2VUwfdpUQb35X
      created_at: !ruby/object:DateTime 2020-06-19 03:25:05.797000000 Z
      updated_at: !ruby/object:DateTime 2020-06-24 01:09:11.259000000 Z
      content_type_id: color
      revision: 1
    name: Bright Red
    value: "#FF4D7E"
    thumbnail:
      sys:
        id: 6DFxyXxmFWXQ8Eq5QdgVl3
        created_at: !ruby/object:DateTime 2020-06-19 03:25:20.750000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:09:08.974000000 Z
      title: FF4D7E
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/6DFxyXxmFWXQ8Eq5QdgVl3/25dc83caf2981ab55b5b35e3c59f2467/FF4D7E"
    text: "#262626"
  image: https://images.ctfassets.net/g118h5yoccvd/3qrqeRUJmXadMqSKn51oaF/bfb84d389358017676bccf23d25d039f/meedan_report_covid-19-kenya_africa-check_01.png
- sys:
    id: 4hNlM6eXCZG6w43fIoHUPY
    created_at: !ruby/object:DateTime 2020-07-07 21:18:34.780000000 Z
    updated_at: !ruby/object:DateTime 2020-11-11 22:24:02.329000000 Z
    content_type_id: report
    revision: 1
    space:
      sys:
        id: g118h5yoccvd
    type: Entry
    content_type:
      sys:
        id: report
    environment:
      sys:
        id: master
        type: Link
        linkType: Environment
    locale: en-US
  title: COVID-19 and China’s Information Control Policy
  slug: covid-19-and-chinas-information-control-policy
  description: Does censorship within the country now have international consequences?
  lead_image:
    sys:
      id: 4rVZ1kd05N92klEjWtpM54
      created_at: !ruby/object:DateTime 2020-07-28 00:59:12.259000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:09:57.224000000 Z
    title: COVID-19 and China’s Information Control Policy
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/4rVZ1kd05N92klEjWtpM54/471497d0c482ec4bd0ff9bc0ef6a7b68/china-3.png"
  social_card_image:
    sys:
      id: 7wPWaFfbDhTZVmGH9LXmPr
      created_at: !ruby/object:DateTime 2020-07-28 01:00:12.866000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:09:37.718000000 Z
    title: COVID-19 and China’s Information Control Policy
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/7wPWaFfbDhTZVmGH9LXmPr/ef5d6c56d9173c83fefd70eeabe6464c/meedan_report_covid-19-china_02.png"
  introduction: On March 12, 2020 China’s deputy director of its Ministry of Foreign
    Affairs, Zhao Lijian published an unusual tweet. It claimed that the United States
    army brought COVID-19 to China when the country participated in the October 2019
    Military Olympic Games.<sup><a id="fnref1" href="#fn1">[1]</a></sup> The claim,
    if not a conspiracy, was not supported by any evidence. It spread like wildfire,
    becoming a global piece of misinformation about the origin of the pandemic.
  body: "A few days later, disinformation messages arrived on the cell phones and
    social media feeds of individual American citizens. One post read, “The Trump
    administration is about to lock down the entire country,\" which was later confirmed
    to be false. United States officials considered the tactic an unprecedented move
    by Chinese agents<sup><a id=\"fnref2\" href=\"#fn2\">[2]</a></sup>.\n\nThe University
    of Hong Kong’s censorship monitoring project, Weiboscope, monitors the Chinese
    social platform Weibo for disinformation, misinformation and censorship. Weibo
    is China's equivalent to Twitter, and Weiboscope offers key insights into how
    information control manifests on one of China's most prominent social networks.
    As the Chinese government exerts information control over COVID-19 narratives
    inside China, the international impact of state censorship becomes a significant
    global issue. \n\nOn the afternoon of December 31, 2019 a Weibo post read, “Wuhan
    pneumonia cannot be judged to be SARS. Wuhan has the only virus laboratory in
    the country and is also a world-class virology laboratory. There are ways to deal
    with the virus. If Wuhan can't figure it out, no one can handle it.” It was swiftly
    censored within an hour. \n\nThe post mentioned both the SARS-like nature of the
    virus, suggesting its high infection rate, and the Wuhan laboratory, signalling
    the origin of the virus. Based on our knowledge of the government’s censorship
    system, both of these characteristics seem to be attributable to the decision
    to remove the post from the social platform. \n\nAt the time, social media was
    already circulating speculations that an unknown pneumonia was linked to a SARS-like
    disease, according to Weiboscope data. Ophthalmologist Dr. Li Wenliang, who learned
    about the then-mysterious illness in a hospital, had sent a message in the popular
    Chinese private messaging app WeChat, to warn other medical doctors about the
    unnamed illness. Wenliang was then accused, with another six people, of disturbance
    of public order by the local authority (Green, 2020).\n\nOn December 31, 2019,
    the same day the post was removed from Weibo and shortly after Wenliang's warning,
    the Chinese government claimed the “viral pneumonia cases” had “no apparent pattern
    of human-to-human transmission” and were “preventable and controllable” (Wuhan
    Municipal Health Commission, 2019). The Chinese government then also informed
    the World Health Organization and the United States of its position.\n\nLess than
    a month later on January 20th, the Chinese central government announced the COVID-19
    epidemic to the public (Phillips et al., 2020). A group of leading Chinese virologists
    appeared on a state television program and confirmed that the virus is transmissible
    between humans (National Health Commission, 2020). A group of Chinese Center for
    Disease Control and Prevention scientists  published a paper in the New England
    Journal of Medicine on January 29, 2020 concluding that “human-to-human transmission
    among close contacts has occurred since the middle of December and spread out
    gradually within a month after that” (Li et. al., 2020).\n\nCensoring virus-related
    messages in late December and early January may have put countless Chinese people
    unknowingly at risk of  infection, depriving them of receiving early warning from
    media and  social media about the highly transmissible disease (Fu and Zhu, 2020).
    The censorship and information control may have also delayed the general public’s
    response to make necessary and timely protective measures. \n\n### How does China
    control information in general?\n\nInternet and digital technology have been profoundly
    shaping China’s social, economic, and even political domains. Even though the
    Internet is known to serve as an enabler for citizen activism in China, the state’s
    repressive power is remarkably strengthened by means of digital technologies<sup><a
    id=\"fnref3\" href=\"#fn3\">[3]</a></sup>. To illustrate this, for instance, three
    hundred million cameras will be reportedly installed in China for the purposes
    of surveillance and the devices are backed with facial and video recognition technologies.
    Chinese authorities have strongly emphasized the concept of cyber sovereignty,
    which broadly extends the notion of cyber security as a justification to control
    certain online activities which could be viewed as a threat to the state’s stability
    (Lee, 2018). Against this backdrop, China has introduced a sophisticated system
    to regulate traditional and online media and the system has been integrated into
    all Internet platforms. \n\nThe Chinese government has built the “Great Firewall”
    – a collective term to describe a large and sophisticated Internet filtering system
    and digital governance framework. These stringent measures make the Chinese Internet
    virtually an intranet, in which sensitive private and public communication are
    filtered, and access to foreign sites such as Facebook, Twitter, and YouTube are
    blocked. Having said that, these practices are only part of the country’s censorship
    system. A major part of the content censorship is implemented by domestic Internet
    content providers. To comply with their license conditions, Chinese Internet service
    providers must act as censors to screen customer’s messages or disable accounts,
    which are executed by the company’s internal department or outsourced to external
    operators. \n\nChinese citizens also overly rely on local media, which are mostly
    state-owned, in order to receive information about COVID-19. International media
    or foreign social media sites are not accessible and posts circulating foreign
    media are also censored. For example, our system records a censored post on January
    18, which is attached with a screen capture of a coronavirus story titled “Four
    new confirmed cases in Wuhan; Estimated over 1,700 cases, according to expert”
    from the Hong Kong based media outlet, Radio and Television Hong Kong. Another
    Weibo message titled “Express: Suspected cases with unknown pneumonia found in
    South Korea,” which was hyperlinked to a story of the Yonhap News Agency on January
    8, 2020, was detected to be censored. \n\nChina’s real name registration policy
    requires Internet  users to disclose one’s real identity. The policy was officially
    launched on March 16, 2012. Registered users must release their identity numbers
    or indirect identifiers — such as mobile phone numbers — to service providers
    for government verification. Users can then choose to post either by nickname
    or real name; unregistered users can view posts, but can’t create content. Much
    concern has been raised that the true identity disclosure policy has imposed a
    chilling effect on online comments, especially on political criticism and other
    sensitive topics (Fu et al., 2013). Recently, the Chinese government reportedly
    requested new mobile phone users to receive facial scans. Online anonymity is
    virtually impossible in China. Surveillance cameras, facial recognition technology,
    and identity tracking have been extensively used to assist the authority’s public
    monitoring of the implementation of self-quarantine policy during the coronavirus
    outbreak<sup><a id=\"fnref5\" href=\"#fn5\">[5]</a></sup>.\n\nChina has introduced
    the Cybersecurity Law, which came into effect on June 1, 2017, and has created
    substantial market uncertainty<sup><a id=\"fnref6\" href=\"#fn6\">[6]</a></sup>.
    Public and private communication are closely monitored by service providers or/and
    the authorities. If a content crosses a “red line” (an often invisible, unknown
    yardstick) it may be censored, or the account owner’s content administrator may
    be invited to “drink tea”<sup><a id=\"fnref7\" href=\"#fn7\">[7]</a></sup>, requested
    to remove it. If an account is repeatedly found to be non-compliant with content
    regulations, it can be suspended or even permanently terminated<sup><a id=\"fnref8\"
    href=\"#fn8\">[8]</a></sup>. For certain serious offences, the content author
    may be sentenced to jail<sup><a id=\"fnref9\" href=\"#fn9\">[9]</a></sup>. More
    than that, the “red line” is opaque and its threshold and boundaries are largely
    unknown: life-style “clickbait” was blocked<sup><a id=\"fnref10\" href=\"#fn10\">[10]</a></sup>,
    rap pop music was banned<sup><a id=\"fnref11\" href=\"#fn11\">[11]</a></sup>,
    television drama series<sup><a id=\"fnref12\" href=\"#fn12\">[12]</a></sup> were
    disapproved, and video game services were stopped<sup><a id=\"fnref13\" href=\"#fn13\">[13]</a></sup>.
    Three Chinese volunteers were detained by police because of uploading censored
    COVID-19 material to Github<sup><a id=\"fnref14\" href=\"#fn14\">[14]</a></sup>.
    \n\n### Impact beyond China’s borders\n\nWith China’s economic rise and growing
    global influence, the country has modernized its information systems and cyber
    warfare capabilities. Being the second largest economy in the world, information
    technology is not only key to both economic growth and the vital functions of
    society in China, but also national security. China now operates the most sophisticated
    online censorship system in the world, with over 854 million online citizens under
    its sphere (CNNIC, 2019). Apart from traditional warfare methods such as land,
    sea, air, and outer space, the online cyberspace has become a highly recognized
    domain of warfare to the Chinese government (Fritz, 2017). As China increases
    its engagement with the world, this struggle for controlling the narrative about
    the Chinese Communist Party and the country has expanded to global public opinion,
    an ambitious external propaganda project (Tsai, 2017), and the Internet is one
    of the latest battlespaces via the means of information warfare.\n\nApart from
    traditional forms of propaganda published on Chinese state media, there are now
    cases of organized and coordinated efforts of online influence campaigns on issues
    deemed of core interest and importance to the Chinese government, such as the
    detentions of Uyghurs in Xinjiang, the Taiwanese presidential elections, and the
    more recent Hong Kong protests (Mazarr et al., 2019).\n\nIn 2016, in wake of Taiwan
    President Tsai Ing-wen’s presidential election, over 27,000 comments flooded her
    Facebook page, mostly stating that Taiwan is a part of China. Since access to
    Facebook in Mainland China is blocked by the Great Firewall maintained by the
    state, the flood of comments were known as part of an organized campaign from
    an Internet forum called “Diba,” where forum users ironically scaled the Great
    Firewall to post nationalistic messages adhering to the official line of the Chinese
    Communist Party on Tsai’s Facebook posts (Yang & Chen, 2017).\n\nDiba users have
    since attempted but failed to influence the narrative on the Hong Kong protests
    in 2019 as well by trying to infiltrate LIHKG, a local reddit-like online forum
    frequently used by protesters in Hong Kong. Although no direct connection between
    the Diba users and the Chinese state were identified, concern has been raised
    towards a growing trend of either a grassroots level or organized “cyber-nationalism”
    by younger Chinese Internet users to influence the narrative on topics deemed
    of strategic value to the Chinese government (Fang & Repnikova, 2018). \n\nSocial
    media sites from the Western world have also acted on what they believe is a coordinated
    effort by the Chinese government to influence global narrative on China related
    topics, such as the Hong Kong protests. In August 2019, China’s information operations
    were brought to the light after several social media platforms, including Twitter
    and Facebook, discovered the existence of coordinated network activities by China
    that were spreading disinformation about Hong Kong protests. Twitter said that
    the aim of these operations was to undermine the protest movement’s “legitimacy
    and political positions.<sup><a id=\"fnref15\" href=\"#fn15\">[15]</a></sup>”\n\nAt
    the time of writing, the COVID-19 pandemic is still developing, and the crisis
    changes daily, but it seems apparent that the world after COVID-19 will look different.
    Relationships between China and the world are anticipated to become bitter and
    more contentious. China’s strategy to control information and its consequences
    has become a global concern. A lawsuit was launched to sue China for economic
    losses<sup><a id=\"fnref16\" href=\"#fn16\">[16]</a></sup> and an independent
    investigation into China’s information transparency and the virus origin was suggested.
    Further development remains an open question."
  footnotes:
  - sys:
      id: TdHDHZsZNJbX5dQIsL8wp
      created_at: !ruby/object:DateTime 2020-07-22 20:15:00.009000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:10:08.276000000 Z
      content_type_id: footnote
      revision: 1
    body: Twitter is blocked by the Great Firewall in China.
  - sys:
      id: 2CEnXPOhXeisIqvcqoBklU
      created_at: !ruby/object:DateTime 2020-07-22 20:15:08.793000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:10:11.507000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Chinese Agents Helped Spread Messages That Sowed Virus Panic in U.S.,
      Officials Say](https://www.nytimes.com/2020/04/22/us/politics/coronavirus-china-disinformation.html)"
  - sys:
      id: 2yBHGcixdrFsSdgzFyBYSg
      created_at: !ruby/object:DateTime 2020-07-22 20:15:34.521000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:10:14.391000000 Z
      content_type_id: footnote
      revision: 1
    body: "[China’s terrifying surveillance state looks a lot like America’s future](https://www.vanityfair.com/news/2018/07/china-surveillance-state-artificial-intelligence)
      (Vanity Fair)"
  - sys:
      id: 66I34JqsBNZkEv1YkaIlpW
      created_at: !ruby/object:DateTime 2020-07-22 20:15:54.655000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:10:17.021000000 Z
      content_type_id: footnote
      revision: 1
    body: "[China is rolling out facial recognition for all new mobile phone numbers](https://edition.cnn.com/2019/12/02/tech/china-facial-recognition-mobile-intl-hnk-scli/index.html)"
  - sys:
      id: 7eH312YRJqVUeVA4er8ja1
      created_at: !ruby/object:DateTime 2020-07-22 20:16:05.727000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:10:36.390000000 Z
      content_type_id: footnote
      revision: 1
    body: "['The new normal': China's excessive coronavirus public monitoring could
      be here to stay](https://www.theguardian.com/world/2020/mar/09/the-new-normal-chinas-excessive-coronavirus-public-monitoring-could-be-here-to-stay)"
  - sys:
      id: 7wwy3zLlUppPEV5SfWYZmp
      created_at: !ruby/object:DateTime 2020-07-22 20:16:25.457000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:10:39.984000000 Z
      content_type_id: footnote
      revision: 1
    body: "[China's new cyber law just kicked in and nobody's sure how it works](https://money.cnn.com/2017/06/01/technology/business/china-cybersecurity-law/index.html)
      (CNN Money)"
  - sys:
      id: 5bWQAPDLJF2KLPvwl4yTQl
      created_at: !ruby/object:DateTime 2020-07-22 20:16:44.374000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:10:43.097000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Twitter allegedly subject to Chinese censorship reach through hacked accounts
      and deleted tweets](https://www.abc.net.au/news/2018-12-21/chinese-censorship-allegedly-extends-to-twitter/10633850)
      (ABC News)"
  - sys:
      id: 4kEq0SkYxLLeWmMLGQ6mEX
      created_at: !ruby/object:DateTime 2020-07-22 20:17:01.376000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:10:46.070000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Chinese celebrity's Weibo account blocked after criticizing Huawei](http://www.globaltimes.cn/content/1153854.shtml)
      (Global Times)"
  - sys:
      id: 1E3thIBL1Yr2ASu1IVaRyN
      created_at: !ruby/object:DateTime 2020-07-22 20:17:26.592000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:10:50.596000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Chinese blogger jailed under crackdown on 'internet rumours'](https://www.theguardian.com/world/2014/apr/17/chinese-blogger-jailed-crackdown-internet-rumours-qin-zhihui)
      (The Guardian)"
  - sys:
      id: 59RRuVm7kB2qKDRHITMRA4
      created_at: !ruby/object:DateTime 2020-07-22 20:17:42.500000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:10:54.214000000 Z
      content_type_id: footnote
      revision: 1
    body: "[The unmaking of Mimeng: the rise and fall of China’s polarising social
      media empress](https://www.scmp.com/news/china/society/article/2188326/unmaking-mimeng-rise-and-fall-chinas-polarising-social-media)
      (SCMP)"
  - sys:
      id: 1T1bep5xxChG9g8Ku3hRYB
      created_at: !ruby/object:DateTime 2020-07-22 20:17:55.290000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:10:57.541000000 Z
      content_type_id: footnote
      revision: 1
    body: "[China’s hip-hop culture ban: authorities send mixed messages](https://www.scmp.com/culture/music/article/2142444/chinas-hip-hop-culture-ban-authorities-send-mixed-messages)
      (SCMP)"
  - sys:
      id: 59OiSYvfmkjemujVc0i217
      created_at: !ruby/object:DateTime 2020-07-22 20:18:17.281000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:11:00.461000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Too lavish, too nasty: Chinese state media goes to war against Yanxi Palace
      and other period dramas](https://www.scmp.com/news/china/society/article/2183885/yanxi-palace-drama-chinese-state-media-declares-war-period-shows)
      (SCMP)"
  - sys:
      id: 3patZIhCH72jDUuFdHdum4
      created_at: !ruby/object:DateTime 2020-07-22 20:18:36.271000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:11:04.829000000 Z
      content_type_id: footnote
      revision: 1
    body: "[China’s new gaming rules to ban poker, blood and imperial schemes](https://techcrunch.com/2019/04/21/chinas-new-gaming-rules-to-ban-poker-blood-and-imperial-schemes/)
      (TechCrunch)"
  - sys:
      id: 3jkoEVgkyTYjkZlKRCdbNI
      created_at: !ruby/object:DateTime 2020-07-22 20:18:54.996000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:11:08.072000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Chinese activists detained after sharing censored coronavirus material
      on crowdsourcing site Github](https://www.scmp.com/news/china/politics/article/3081569/chinese-activists-detained-after-sharing-censored-coronavirus)"
  - sys:
      id: 63sCCYaQri1Gfwh6NGaFKj
      created_at: !ruby/object:DateTime 2020-07-22 20:19:22.133000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:11:11.889000000 Z
      content_type_id: footnote
      revision: 1
    body: "[Information operations directed at Hong Kong](https://blog.twitter.com/en_us/topics/company/2019/information_operations_directed_at_Hong_Kong.html)"
  - sys:
      id: 6ZdeOJw88rZFaxNmiIXCgO
      created_at: !ruby/object:DateTime 2020-07-22 20:19:34.241000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:11:14.862000000 Z
      content_type_id: footnote
      revision: 1
    body: "[In a first, Missouri sues China over coronavirus economic losses](https://www.reuters.com/article/us-health-coronavirus-china-lawsuit/in-a-first-missouri-sues-china-over-coronavirus-economic-losses-idUSKCN2232US)"
  references: "- CNNIC. (2019). The 44th Statistical report on internet development
    in China. *China Internet Network Information Center (CNNIC), China*.\n- Fang,
    K., & Repnikova, M. (2018). Demystifying “Little Pink”: The creation and evolution
    of a gendered label for nationalistic activists in China. *New Media & Society*,
    20(6), 2162–2185.\n- Freedom House. (2018). Freedom on the Net 2018: [The Rise
    of Digital Authoritarianism](https://freedomhouse.org/report/freedom-net/freedom-net-2018/rise-digital-authoritarianism).
    \n- Fritz, J. R. (2017). *China’s Cyber Warfare: The Evolution of Strategic Doctrine*.
    Lexington Books.\n- Fu, K., Chan, C., & Chau, M. (2013). Assessing censorship
    on microblogs in China: Discriminatory keyword analysis and the real-name registration
    policy. *IEEE Internet Computing*, *17(3), 42–50.\n- Fu, K., & Zhu, Y. (2020).
    [Did the world overlook the media’s early warning of COVID-19?](https://doi.org/10.1080/13669877.2020.1756380)
    Journal of Risk Research, 1–5. \n- Green, A. (2020). Li Wenliang. *[The Lancet](https://doi.org/10.1016/S0140-6736(20)30382-2)*,
    395(10225), 682.\n- Lee, J.-A. (2018). Hacking into China’s Cybersecurity Law.
    *Wake Forest L. Rev*., 53, 57.\n- Li, Q., Guan, X., Wu, P., Wang, X., Zhou, L.,
    Tong, Y., Ren, R., Leung, K. S. M., Lau, E. H. Y., Wong, J. Y., Xing, X., Xiang,
    N., Wu, Y., Li, C., Chen, Q., Li, D., Liu, T., Zhao, J., Liu, M., … Feng, Z. (2020).
    [Early Transmission Dynamics in Wuhan, China, of Novel Coronavirus–Infected Pneumonia](https://doi.org/10.1056/NEJMoa2001316).
    *New England Journal of Medicine*, 382(13), 1199–1207.\n- Mazarr, M. J., Casey,
    A., Demus, A., Harold, S. W., Matthews, L. J., Beauchamp-Mustafaga, N., & Sladden,
    J. (2019). *Hostile Social Manipulation Present Realities and Emerging Trends*.
    RAND Corporation Santa Monica United States.\n- National Health Commission. (2020).
    *[Press Conference of the National Health Commission High-Level Expert Group Regarding
    the Pneumonia Caused by the Novel Coronavirus Infection](http://www.nhc.gov.cn/xcs/s7847/202001/8d735f0bb50b45af928d9944d16950c8.shtml)*.\n-
    Phillips, N., Mallapaty, S., & Cyranoski, D. (2020). [How quickly does the Wuhan
    virus spread?](https://doi.org/10.1038/d41586-020-00146-w) *Nature*.\n- Tsai,
    W.-H. (2017). [Enabling China’s Voice to Be Heard by the World: Ideas and Operations
    of the Chinese Communist Party’s External Propaganda System](https://doi.org/10.1080/10758216.2016.1236667).
    *Problems of Post-Communism*, 64(3–4), 203–213.\n- Wuhan Municipal Health Commission.
    (2019). *[Wuhan Municipal Health Commission’s Situation Report of the Epidemic
    of Pneumonia in Our City](http://wjw.wuhan.gov.cn/xwzx_28/gsgg/202004/t20200430_1199576.shtml)*
    (in Chinese).\n- Yang, S., & Chen, P.-Y. (2017). “Diba Expedition to Facebook”:
    A preliminary study of massive online collective action in China. *IConference
    2017 Proceedings*."
  author:
  - sys:
      id: 2eP3SnlKPgADI7bCE5bbjr
      created_at: !ruby/object:DateTime 2020-07-22 20:14:47.896000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:11:20.719000000 Z
      content_type_id: contributor
      revision: 1
    name: King-wa Fu
    bio: Dr. King-wa FU is an Associate Professor at the JMSC. His research interests
      cover political participation and media use, computational media studies, health
      and the media, and younger generation’s Internet use. He is a visiting Associate
      Professor at the MIT Media Lab and was a Fulbright-RGC Hong Kong Senior Research
      Scholar in 2016-2017. He has a PhD from the JMSC, a master’s degree in Social
      Sciences and a MPhil in Engineering. He was a journalist at the Hong Kong Economic
      Journal before turning to academia.
  organization: Journalism and Media Studies Centre, <br />The University of Hong
    Kong
  publication:
  - sys:
      id: 2lQKVWqTIPTns0oj5UA78S
      created_at: !ruby/object:DateTime 2020-06-15 19:58:33.906000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:12:32.900000000 Z
      content_type_id: publication
      revision: 6
    title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
    short_title: 2020 Misinfodemic Report
    kicker: A Check Global Report
    description: 'Misinfodemic: when online misinformation contributes to real-world
      disease. In the coming months, Meedan will publish eight pieces of reporting
      and analysis in an effort to map the anatomy of the global COVID-19 information
      ecosystem.'
    slug: 2020-misinfodemic-report-covid-19-in-emerging-economies
    sections:
    - sys:
        id: 6K6TuAvZBDXdJATv8SynCi
    - sys:
        id: 1jkEcTAbvK0Y7O1NmysvKT
    - sys:
        id: 3ixzizcKVwnSXEFRRyVuxL
    cover_image:
      sys:
        id: Tu2ovEuxh8gztQD9kuF0G
        created_at: !ruby/object:DateTime 2020-06-23 05:08:56.807000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:33.955000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/Tu2ovEuxh8gztQD9kuF0G/0f5da326dfe06869775caf1002c330e6/meedan_report_2020-infodemic-report_01.png"
    publication_promo_graphic:
      sys:
        id: 5wkwfNNTjvV1Cgf2FAQFD4
        created_at: !ruby/object:DateTime 2020-04-16 20:11:43.061000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:37.263000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5wkwfNNTjvV1Cgf2FAQFD4/fbdc448905c4b9240944587574577654/Meedan_global-misinfodemic-report.png"
    social_card_image:
      sys:
        id: 5L7KfUhYU7lqikodPkJoQQ
        created_at: !ruby/object:DateTime 2020-06-22 23:00:20.044000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:39.651000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5L7KfUhYU7lqikodPkJoQQ/7e82797cad59e546a8fc7e5f4cb23a87/meedan_report_2020-misinfodemic-report_01.png"
    publication_promo: This report is part of a Check Global Report about [COVID-19
      in Emerging Economies](https://meedan.com/reports/2020-misinfodemic-report-covid-19-in-emerging-economies/).
    color:
      sys:
        id: 43kw5bcVj1xtfe7ua88F6V
  color:
    sys:
      id: 2TZKWffUJGg1qZc3rFIxTk
      created_at: !ruby/object:DateTime 2020-07-22 22:49:35.988000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:12:16.059000000 Z
      content_type_id: color
      revision: 1
    name: Bright Orange
    value: "#FFB34D"
    thumbnail:
      sys:
        id: 5zPJST9f1Y3hLceo562134
        created_at: !ruby/object:DateTime 2020-07-22 22:50:13.469000000 Z
        updated_at: !ruby/object:DateTime 2020-07-28 23:12:13.304000000 Z
      title: FFB34D
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5zPJST9f1Y3hLceo562134/757cd636c9b462e5bd1f4a9bdb0416bf/FFB34D"
    text: "#262626"
  image: https://images.ctfassets.net/g118h5yoccvd/7wPWaFfbDhTZVmGH9LXmPr/ef5d6c56d9173c83fefd70eeabe6464c/meedan_report_covid-19-china_02.png
- sys:
    id: 2sKzBmA7npYvPuvjnkYitv
    created_at: !ruby/object:DateTime 2020-07-22 23:21:35.597000000 Z
    updated_at: !ruby/object:DateTime 2020-11-11 22:20:02.572000000 Z
    content_type_id: report
    revision: 1
    space:
      sys:
        id: g118h5yoccvd
    type: Entry
    content_type:
      sys:
        id: report
    environment:
      sys:
        id: master
        type: Link
        linkType: Environment
    locale: en-US
  title: Coronavirus Misinformation in India Is Not Limited to Health Misinformation
  slug: coronavirus-misinformation-in-india-is-not-limited-to-health-misinformation
  description: An analysis of more than 150 fact-checks shows trends in COVID-19 misinformation.
  lead_image:
    sys:
      id: 4QY8HOpHY9b8RVKCe2mN0Y
      created_at: !ruby/object:DateTime 2020-07-22 23:45:55.835000000 Z
      updated_at: !ruby/object:DateTime 2020-07-27 13:38:20.292000000 Z
    title: Coronavirus Misinformation in India Is Not Limited to Health Misinformation
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/4QY8HOpHY9b8RVKCe2mN0Y/18a2027337822abb1f78602809088847/india.png"
  social_card_image:
    sys:
      id: 3hrpvLcTanPaFxcvRbFt9Z
      created_at: !ruby/object:DateTime 2020-07-22 23:48:04.845000000 Z
      updated_at: !ruby/object:DateTime 2020-07-27 13:38:25.519000000 Z
    title: Coronavirus Misinformation in India Is Not Limited to Health Misinformation
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/3hrpvLcTanPaFxcvRbFt9Z/d9d8be34291410f327211d270292ace5/meedan_report_covid-19-india_01.png"
  introduction: BOOM is an India-based fact checking organization that began actively
    debunking fake news about the COVID-19 pandemic since January 25, 2020, one week
    after China declared the spread of the virus. In an [analysis](https://www.boomlive.in/fact-file/fake-news-in-the-time-of-coronavirus-a-boom-study-8008)
    of around 178 fact checks that were purely related to COVID-19 information between
    January to May 2020, we found evidence of a phased ‘misinfodemic’ of misinformation
    spreading across online platforms in India. We found that COVID-19 misinformation
    increased in the month of March as the number of COVID-19 cases increased in the
    country. In this period, COVID-19 dominated the news cycle which also contributed
    to the increase in misinformation about the disease. While in February, BOOM fact-checked
    13 misinformation pieces about COVID-19, in March, this number had increased to
    69 articles.
  body: "Most of the false or misleading claims we found were circulated with videos
    (35%). There was also a significant number of text messages (29.4%) being shared
    with fake cures, treatments or quotes from celebrities, along with images (29.4%)
    that were either misrepresented or doctored. We also noticed a small number of
    audio clips (2.2%) going viral with false context.  \n\nOur investigation classifies
    two waves of COVID-19 misinformation on social platforms such as WhatsApp, Facebook,
    Twitter – medical and communal. Medical misinformation that we found focused on
    theemergence, origin, spread, and cures of the virus. Communal misinformation
    emerged as the number of patients testing positive for COVID-19 increased, and
    the disease spread across the country. This type of misinformation leaned on and
    exacerbated existing tensions in the country, particularly long-standing rifts
    between the country’s Muslim minority and Hindu populations. Misinformation during
    this wave took on narratives related to the economy, politics, and local communities.\n\nIn
    both forms, COVID-19 misinformation on social media paints a picture of a changing
    pattern of news consumption, suggesting that the area may be an important topic
    for future research [(](https://aisel.aisnet.org/pacis2019/194/)[Li et al](https://aisel.aisnet.org/pacis2019/194/),
    2019). According to a 2018 Weber Shandwick [study](https://www.webershandwick.com/wp-content/uploads/2018/11/Healthcare-Info-Search-Report.pdf)
    about The Great American Search for Healthcare Information, a study of 1700 individuals,
    around 73% of Americans gathered their healthcare information from the internet.
    There is no Indian study highlighting the sources of health information for Indians.
    \ \n\nIn India, in some cities like Mumbai, the police force enforced [strict
    action for peddlers](https://www.livemint.com/news/india/mumbai-police-warns-whatsapp-group-admins-over-covid-related-fake-news-11590374904713.html)
    of fake news and misinformation. Recent media reports suggest that Facebook and
    WhatsApp have been [taking active steps](https://www.facebook.com/facebookmedia/blog/working-to-stop-misinformation-and-false-news)
    to curb the spread of misinformation on their websites. Research should be carried
    out to further understand how social media itself could be used to contain the
    spread of misinformation.This report outlines two distinct types of misinformation
    which we observed throughout the COVID-19 crisis in India between January to April
    2020.\n\n### Wave One: Medical Misinformation\n\n*India reported its first three
    cases of COVID-19 between the last week of January 2020 and the first week of
    February 2020. According to our analysis, this is also when the first wave of
    misinformation emerged in the country. Misinformation during this period was largely
    related to the science, spread, cause and conspiracy theories about the virus.
    Then, throughout February, India did not report any new cases. Our analysis found
    that during this time, there was a rise in misinformation about videos from China,
    being shared with misleading narratives. During this time, [a video of three events](https://www.boomlive.in/fake-news/video-shows-chinese-policemen-killing-coronavirus-patients-factcheck-6885)
    not related to COVID-19 was passed off as police brutality on COVID-19 patients
    in China. On March 2, 2020, India reported new cases as Indians returned home
    from countries in Europe and Asia.  With the emergence of new cases, misinformation
    on the prevention, control and cure of the virus, such as the use of home remedies
    not backed by science, like [drinking garlic water](https://www.boomlive.in/health/boiled-garlic-water-for-treating-coronavirus-not-really-6737)
    to prevent COVID-19, began to circulate again.*\n\nIndia’s first wave of misinformation
    revolved around the **origin, causes, spread, and treatment of the virus** and
    also **China’s response to controlling the outbreak**. In the early months of
    the pandemic the origin of the virus, and its modes of transmission, were unclear
    to scientists and the public alike. Conspiracy theories emerged on Indian social
    media, even though the virus was not yet present in the country. False claims
    about the virus originating in various animals, without any scientific evidence,
    circulated on platforms. Scientific progress on human to human transmission led
    to further speculations about the virus, such as false claims about the virus
    being a bioweapon, [manufactured in a lab](https://www.boomlive.in/fact-file/is-coronavirus-a-bioweapon-the-internet-loves-to-think-so-7623).
    \ \n\nThe origin of the virus was, and still is, mired in misinformation about
    it having emerged from a lab in Wuhan, of the virus already being [patented](https://www.boomlive.in/health/coronavirus-patented-why-social-media-posts-are-misleading-6661),
    of the virus being a bioweapon, all without any proof or credible evidence. In
    one misleading account, a false message which announced that COVID-19 is a Chinese
    conspiracy was attributed to somebody pretending to be a Chinese[intelligence
    officer](https://www.boomlive.in/fake-news/false-chinese-intelligence-officer-reveals-coronavirus-is-a-bioweapon-7224).
    However, this turned out to be a fictional story on social news platform  Reddit.
    The thread asked users to write a fictional story related to a real life event.\n\nFalse
    conspiracy theories circulated around China killing [20,000 patients](https://www.boomlive.in/fake-news/false-china-seeks-court-approval-to-kill-over-20000-coronavirus-patients-6812)
    to underreport the cases of COVID-19 in the country—misinformation that emerged
    from a web article on AB-TC, also known as City News, a regular fake news peddler.
    Other theories, such that [Dettol,](https://www.boomlive.in/health/did-dettol-know-about-coronavirus-outbreak-beforehand-a-factcheck-6767)
    many [books](https://www.boomlive.in/fake-news/did-these-books-predict-the-coronavirus-outbreak),
    the [Simpsons and Asterix](https://www.boomlive.in/health/did-the-simpsons-and-comic-series-asterix-predict-the-2019-coronavirus-outbreak-6867)
    predicted or already knew about the virus, have been rampantly circulating on
    social media. F[alse information](https://www.boomlive.in/fake-news/bat-infestation-video-from-florida-falsely-shared-as-coronavirus-source-found-6747)
    also circulated about [bats](https://www.boomlive.in/fake-news/fake-news-site-invents-story-about-covid-19-patient-zero-having-sex-with-bats-7407),
    [chicken](https://www.boomlive.in/health/false-coronavirus-found-in-broiler-chickens-6757),
    and [goats](https://www.boomlive.in/fake-news/old-video-shared-as-goats-infected-with-covid-19-in-ajmer-7265),
    as misleading origins of the virus have been circulating on social media. Other
    misinformation speculated that [5G Technology](https://www.boomlive.in/fact-file/did-the-race-to-5g-cause-the-coronavirus-outbreak-7404)could
    have led to emergence of the virus. All of these claims have been debunked by
    BOOM and other fact-checking organizations.\n\n**Fake notifications** attributed
    to various organizations also emerged as a norm during the first wave. Claims
    about the causes, spread, cure, and treatment of COVID-19 that were not supported
    by research were attributed to national and international organizations.Fake notifications
    attributed to [UNICEF](https://www.boomlive.in/health/viral-coronavirus-advisory-is-not-from-unicef-7171)
    and the Ministry of Health about ways [to prevent the virus](https://www.boomlive.in/health/hoax-alert-viral-emergency-notification-on-coronavirus-is-fake-6682)
    and not purchase colours from China during [Holi](about:blank), the festival of
    colours in India, emerged on social media. \n\n**Videos and narratives from China**
    with unreliable claims were scattered across social media. These claims focused
    on China’s surveillance tactics, atrocities that residents faced, strict authoritative
    action, and even false narratives related to the heads of the country. [Mock drills](https://www.boomlive.in/fake-news/coronavirus-mock-drill-by-chinese-police-viral-as-real-6997)
    across China were shared as police atrocities, a group of unrelated videos were
    combined to [showcase](https://www.boomlive.in/fake-news/video-shows-chinese-policemen-killing-coronavirus-patients-factcheck-6885)
    Chinese officials killing coronavirus patients. Along with this, an [Indonesian](https://www.boomlive.in/fake-news/coronavirus-indonesian-market-video-viral-as-wuhan-china-6697)
    wet animal market was shared as a Chinese market suggesting that China is not
    mending its ways. \n\nThe Chinese Prime Minister and President were not spared
    either. The [Chinese President](https://www.boomlive.in/fake-news/old-photos-of-xi-jinpings-mosque-visit-falsely-linked-to-coronavirus-6803)
    and [PM’](https://www.boomlive.in/fake-news/no-the-chinese-pm-did-not-visit-a-mosque-to-protect-himself-from-coronavirus-6763)s
    earlier visits to mosques were shared with narratives that the officials were
    visiting the mosques to improve China’s prevailing conditions.\n\nMoreover, as
    the symptoms of the virus were similar in nature to colds, social media was rife
    with different ways that the virus could have be cured or treated; false and unproven
    information about avoiding [cold food](https://www.boomlive.in/health/can-avoiding-ice-creams-and-cold-drinks-for-90-days-prevent-coronavirus-6689),
    drinking hot water, homeopathic tablets for prophylaxis to the consumption of
    anti-malarial hydroxychloroquine were shared online as preventive mechanisms.
    Claims about [homeopathic medicine](https://www.boomlive.in/health/does-homeopathic-medicine-arsenicum-album-30-prevent-coronavirus-a-fact-check-7156),
    home remedies such as various types of [tea](https://www.boomlive.in/coronavirus-outbreak/false-different-types-of-tea-can-act-as-a-cure-for-covid-19-7506),
    [garlic](https://www.boomlive.in/health/boiled-garlic-water-for-treating-coronavirus-not-really-6737)
    and hot water as well as [cow urine](https://www.boomlive.in/factcheck/can-cow-urine-or-gaumutra-cure-coronavirus-what-we-know-7308)
    curing COVID-19 circulated in the country. [Alcohol](https://www.boomlive.in/health/does-drinking-alcohol-prevent-coronavirus-6935)
    and [marijuana](https://www.boomlive.in/fake-news/marijuana-kills-coronavirus-meme-viral-with-false-claims-6831)
    were also shared as plausible cures for coronavirus. Along with this, there were
    also many [speculations](about:blank) about [vaccines](https://www.boomlive.in/health/viral-posts-claiming-vaccines-for-coronavirus-have-been-developed-are-false-7201)
    for the virus before the actual trial of vaccines had begun. \n\nAfter reporting
    three cases in February 2020 India did not report any new cases until March 2,
    2020. During this period, we observed misinformation decreasing in volume, picking
    up again in early March when new international hotspots began emerging.\n\n###
    Wave Two: Communal Misinformation\n\n*In March, an Islamic congregation held in
    New Delhi changed the trajectory of the misinformation universe in India. The
    Tablighi Jamaat, a Muslim missionary movement, held an annual gathering with international
    attendees. The convening later emerged as a hotspot of COVID-19 cases. As cases
    increased steadily, misinformation about religion and politics started circulating
    more prominently in the country. The Indian Prime Minister,  Narendra Modi, along
    with enforcing a lockdown in the country for 21 days and further extending it
    twice, also assigned activities for expressing gratitude to all the frontline
    workers in the country. Misinformation emerged around the [banging of plates](https://www.boomlive.in/fake-news/how-celebs-amplified-misinformation-around-janta-curfew-7317)
    and [lighting of diyas](https://www.boomlive.in/fake-news/false-photo-shows-india-illuminated-during-pm-modis-9-pm-9-minutes-appeal-7562)
    or earthen lamps, tasks requested by the Prime Minister.*\n\n*India is currently
    amidst the second wave of misinformation that is an amalgamation of the pre-existing
    first wave and themes of economic, administrative, communal, and lockdown related
    misinformation. The older themes of origin, prevention, cure, and treatment continued
    to be in circulation. The second wave appears to be circulated more widely with
    a higher consumption rate on social media as it even takes into consideration
    the effect that the virus has upon sectors other than health.* \n\nThe second
    wave of misinformation in India is thematically about politics, religion, culture,
    and economics—which we define as communal misinformation. Misinformation with
    communal narratives, along with narratives around the lockdown, emerged to be
    the biggest contributor to misinformation during the second wave of misinformation.
    BOOM fact-checked 34 claims with communal narratives in this period. \n\nThe second
    wave of misinformation picked up pace when the Indian Prime Minister addressed
    the country for the first time on March 18, 2020. As an expression of gratitude
    for the frontline workers and to understand the importance of social distancing,
    he announced March 22 as Janata curfew, a self-imposed curfew, wherein he requested
    citizens to stay at home for that day and at 5 PM clap or play an instrument from
    the confines of their homes for frontline workers. Following this event, another
    task of lighting diyas, the 21-day lockdown and the subsequent 18-day lockdown
    followed. Fake news related to the lockdown, and the two gratitude events spread
    across the country along with hailing the PM. This included celebrities who shared
    the Prime Minister’s request for expressing gratitude with false narratives. One
    of the most viral pieces of misinformation was about the banging of plates killing
    the virus as misinformation on the virus’ shelf life on [surfaces](https://www.boomlive.in/fact-file/message-claiming-coronavirus-lives-on-a-surface-for-12-hours-is-misleading-7310)
    and [air](https://www.boomlive.in/fake-news/message-claiming-coronavirus-can-last-up-to-8-hours-in-air-is-misleading-7328)
    circulated with many different claims. \n\nClaims about the virus lasting on surfaces
    for various hours and it spreading through [currency](https://www.boomlive.in/health/can-contaminated-currencies-spread-coronavirus-all-you-need-to-know-7297)
    were rife on social media. Images depicting [money](https://www.boomlive.in/fake-news/did-italians-throw-their-money-on-the-streets-due-to-covid-19-deaths-7468)
    lying strewn over streets due to it being a transmission method were circulating
    on social media. Another piece suggesting [Narendra Modi](https://www.boomlive.in/fake-news/false-us-uk-asked-pm-modi-to-lead-18-nation-coronavirus-task-force-7475)
    was appointed as the leader of a special task force to tackle COVID-19 has also
    been doing the rounds. \n\nThe Indian administrative authorities were also targeted
    with misinformation as Indian social media was flooded with messages suggesting
    that the [Tourism Ministry](https://www.boomlive.in/fake-news/false-tourism-ministry-announces-closure-of-hotels-till-october-15-7645)
    was keeping all hotels and restaurants shut in lieu of the coronavirus. Other
    messages of the [Supreme Court](about:blank) banning Indian media houses from
    publishing Coronavirus news reports were also doing the rounds. The [Mumbai police](https://www.boomlive.in/fake-news/mumbai-police-call-out-whatsapp-audio-clips-going-viral-in-its-name-7451)
    also debunked a couple of videos which were repeatedly shared in its name. \n\nAnother
    piece of misinformation which was a notification that falsely attributed a Cambridge
    study to the [WHO](https://www.boomlive.in/fake-news/viral-messages-stating-who-protocols-for-lockdown-extension-are-false-7552)
    stating it had proposed a lockdown schedule to fight the virus was shared widely
    in the country.\n\nMany esteemed industrialists and doctors of the country have
    been dragged into the fake news surrounding coronavirus. Images and audio files
    bearing fake quotes related to India’s battle with COVID-19 have been misattributed
    to doctors like [Dr. Devi Shetty](https://www.boomlive.in/coronavirus-outbreak/dr-devi-shetty-did-not-record-viral-audio-clip-on-coronavirus-7275)_,_
    \ the founder of Narayana Hrudyalaya, a chain of hospitals in India and other
    influential celebrities such as [Ratan Tata](https://www.boomlive.in/fake-news/ratan-tata-quote-on-impact-of-coronavirus-on-indian-economy-is-fake-7634),
    the chairman of Tata Group of Companies and [Raghuram Rajan](https://www.boomlive.in/fake-news/raghuram-rajan-did-not-address-imf-webinar-on-covid-19-7767),
    the ex-governor of the Reserve Bank of India. \n\nThe communal and religious misinformation
    wave began when an Islamic congregation in Delhi came under the line of fire after
    many people who attended the event tested positive which led to a spurt of cases
    in India. Older videos of communal violence, incidences from other countries being
    shared as communal differences in India, videos of spitting in food, coughing
    in food shared falsely as Muslims deliberately trying to spread the virus in India
    gained momentum as soon as news about this congregation were shared across the
    country. \n\nAudio stating that Muslim [vendors](https://www.boomlive.in/fake-news/viral-audio-clip-claiming-muslim-vendors-in-surat-are-spreading-coronavirus-is-false-7644)
    were spreading coronavirus in Surat, [Jamaat attendees](https://www.boomlive.in/fake-news/false-markaz-attendee-thrashed-by-cops-for-misbehaving-with-hospital-staff-7647)
    were claimed to be harassed and beaten by the police, while a video of a [pharmacist](https://www.boomlive.in/fake-news/false-communal-spin-given-to-madhya-pradesh-pharmacists-death-7611)’s
    death was shared with a communal spin. Along with this, a gathering of migrant
    workers in [Mumbai](https://www.boomlive.in/fake-news/migrants-defying-lockdown-at-mumbais-bandra-station-given-communal-spin-7683)
    was given a communal spin suggesting Muslims were trying to defy the lockdown
    and spread coronavirus. Claims from [Haldwani](https://www.boomlive.in/fake-news/rumours-led-to-residents-protest-against-imams-quarantine-uttarakhand-police-7680)
    suggested that Muslims were purportedly defying the lockdown to spread coronavirus.
    \n\nIt is pertinent to understand the nature of India’s religious and communal
    narratives to gauge why the Muslim community was particularly targeted as a vector
    to spread misinformation. In the last three decades, the country has witnessed
    many incidences of Hindu- Muslim rivalry. From the demolition of the Babri Masjid
    in 1992 and the subsequent riots and bomb blasts across the country to the recent
    attack on students at Jamia Milia University in Delhi for the much touted National
    Register of Citizens (NRC) and the Citizen Amendment Act which was felt to be
    discriminatory against the community, the recent coronavirus misinformation augments
    the communal bias in the country. \n\nThe pre-existing themes of the first wave
    of misinformation have passed on to the second wave too. The claim that nigella
    seeds possess [hydroxychloroquine](https://www.boomlive.in/coronavirus-outbreak/hydroxychloroquine-found-in-kalonji-seeds-not-quite-7631)
    – a drug that is undertrial as a potential cure for COVID-19- was not scientifically
    backed. Claims that [thrombosis](https://www.boomlive.in/fake-news/is-thrombosis-the-leading-cause-of-covid-19-deaths-a-factcheck-7977)-
    blood clots are the leading reason for Coronavirus deaths instead of respiratory
    failure were also circulating during this period. \n\nVideos with false narratives
    are continuously in circulation. Old videos of people [committing suicide](https://www.boomlive.in/fake-news/2015-video-from-philadelphia-peddled-as-coronavirus-suicide-in-italy-7620)
    in two separate instances were passed off as people [committing suicide](https://www.boomlive.in/fake-news/dated-photos-of-a-suicide-in-spain-falsely-linked-to-coronavirus-in-italy-7370)
    due to coronavirus. A [lion](https://www.boomlive.in/fake-news/putin-let-loose-lions-to-keep-russians-indoors-bizarre-claim-goes-viral-7320)
    on the streets of South Africa was shared as Russia’s efforts to maintain a lockdown.
    A video of a woman in the hospital was claimed to be [Sophie Trudeau](https://www.boomlive.in/fake-news/no-this-is-not-sophie-trudeau-talking-about-coronavirus-7299)\n\nFurthermore,
    casualties in an earthquake in [Croatia](https://www.boomlive.in/fake-news/croatia-earthquake-pics-peddled-as-italy-overwhelmed-with-covid-19-patients-7363)
    were peddled as Italy’s deteriorating condition. Body bags from Ecuador were shared
    as [New York](https://www.boomlive.in/fake-news/no-this-video-does-not-show-body-bags-in-a-new-york-hospital-7588),
    Pakistani videos of [apple](https://www.boomlive.in/fast-check/video-of-boy-washing-apples-in-a-sewer-in-pakistan-revived-7605)
    washing and [quarantine centres](https://www.boomlive.in/fake-news/video-of-an-agitated-woman-at-a-quarantine-centre-in-pakistan-shared-as-india-7661)
    were circulating as India.\n\nIndia is currently not only battling COVID-19 and
    health misinformation but also is tackling many different narratives of misinformation.
    \ \n"
  author:
  - sys:
      id: 4kgyss6jtjX7WbUbXMCVE3
      created_at: !ruby/object:DateTime 2020-07-22 23:22:14.908000000 Z
      updated_at: !ruby/object:DateTime 2020-07-27 13:38:38.097000000 Z
      content_type_id: contributor
      revision: 1
    name: Shachi Sutaria
    bio: Shachi Sutaria is a fact-checker at BOOM. She has previously worked as a
      health research analyst at AMS Consulting, Lucknow for various national and
      international clients. She is a post- graduate in Public Health- Health administration
      from Tata Institute of Social Sciences, Mumbai.
  organization: " BOOM FactCheck, <br >India"
  publication:
  - sys:
      id: 2lQKVWqTIPTns0oj5UA78S
      created_at: !ruby/object:DateTime 2020-06-15 19:58:33.906000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:12:32.900000000 Z
      content_type_id: publication
      revision: 6
    title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
    short_title: 2020 Misinfodemic Report
    kicker: A Check Global Report
    description: 'Misinfodemic: when online misinformation contributes to real-world
      disease. In the coming months, Meedan will publish eight pieces of reporting
      and analysis in an effort to map the anatomy of the global COVID-19 information
      ecosystem.'
    slug: 2020-misinfodemic-report-covid-19-in-emerging-economies
    sections:
    - sys:
        id: 6K6TuAvZBDXdJATv8SynCi
    - sys:
        id: 1jkEcTAbvK0Y7O1NmysvKT
    - sys:
        id: 3ixzizcKVwnSXEFRRyVuxL
    cover_image:
      sys:
        id: Tu2ovEuxh8gztQD9kuF0G
        created_at: !ruby/object:DateTime 2020-06-23 05:08:56.807000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:33.955000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/Tu2ovEuxh8gztQD9kuF0G/0f5da326dfe06869775caf1002c330e6/meedan_report_2020-infodemic-report_01.png"
    publication_promo_graphic:
      sys:
        id: 5wkwfNNTjvV1Cgf2FAQFD4
        created_at: !ruby/object:DateTime 2020-04-16 20:11:43.061000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:37.263000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5wkwfNNTjvV1Cgf2FAQFD4/fbdc448905c4b9240944587574577654/Meedan_global-misinfodemic-report.png"
    social_card_image:
      sys:
        id: 5L7KfUhYU7lqikodPkJoQQ
        created_at: !ruby/object:DateTime 2020-06-22 23:00:20.044000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:39.651000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5L7KfUhYU7lqikodPkJoQQ/7e82797cad59e546a8fc7e5f4cb23a87/meedan_report_2020-misinfodemic-report_01.png"
    publication_promo: This report is part of a Check Global Report about [COVID-19
      in Emerging Economies](https://meedan.com/reports/2020-misinfodemic-report-covid-19-in-emerging-economies/).
    color:
      sys:
        id: 43kw5bcVj1xtfe7ua88F6V
  color:
    sys:
      id: 5JDZYN12fQYOkyNmPTQBff
      created_at: !ruby/object:DateTime 2020-06-18 00:00:55.264000000 Z
      updated_at: !ruby/object:DateTime 2020-07-27 13:39:33.113000000 Z
      content_type_id: color
      revision: 1
    name: Strong Cyan
    value: "#008D5E"
    thumbnail:
      sys:
        id: 4t2hYWT1aLyVQkXkWQ9lkB
        created_at: !ruby/object:DateTime 2020-06-18 00:01:45.060000000 Z
        updated_at: !ruby/object:DateTime 2020-07-27 13:38:58.479000000 Z
      title: '008D5E'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/4t2hYWT1aLyVQkXkWQ9lkB/5200e3453163a18e91698f955c3fa075/008D5E"
    text: "#262626"
  image: https://images.ctfassets.net/g118h5yoccvd/3hrpvLcTanPaFxcvRbFt9Z/d9d8be34291410f327211d270292ace5/meedan_report_covid-19-india_01.png
- sys:
    id: 4exEvSWgwUB4VuB5ECE5zV
    created_at: !ruby/object:DateTime 2020-08-11 20:01:15.624000000 Z
    updated_at: !ruby/object:DateTime 2020-11-11 22:24:14.693000000 Z
    content_type_id: report
    revision: 1
    space:
      sys:
        id: g118h5yoccvd
    type: Entry
    content_type:
      sys:
        id: report
    environment:
      sys:
        id: master
        type: Link
        linkType: Environment
    locale: en-US
  title: How a volcanic eruption frames the pandemic in the Philippines
  slug: how-a-volcanic-eruption-frames-the-pandemic-in-the-philippines
  description: 'Free speech and public trust: casualties in a national emergency '
  lead_image:
    sys:
      id: 2tmLP18VbEBJzrDZx1bVpS
      created_at: !ruby/object:DateTime 2020-08-11 20:28:05.647000000 Z
      updated_at: !ruby/object:DateTime 2020-08-13 20:24:01.059000000 Z
    title: How a volcanic eruption frames the pandemic in the Philippines
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/2tmLP18VbEBJzrDZx1bVpS/45233b867ced271eae930fe3f258b822/philippines.png"
  social_card_image:
    sys:
      id: 4RvRYJ3u3RXqT5J2FzGfez
      created_at: !ruby/object:DateTime 2020-08-11 20:30:04.849000000 Z
      updated_at: !ruby/object:DateTime 2020-08-13 20:24:05.315000000 Z
    title: How a volcanic eruption frames the pandemic in the Philippines
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/4RvRYJ3u3RXqT5J2FzGfez/398d73327d9e2f776a071e6be32e3bd4/meedan_report_covid-19-Philippines.png"
  introduction: Shortly before the COVID-19 pandemic took hold in the Philippines,
    a volcano about 31 miles south of Manila erupted on a quiet Sunday afternoon in
    January 2020, after 47 years of fitful rest. The Philippine Institute of Volcanology
    and Seismology (Phivolcs) immediately raised risk levels and informed the public
    of a moderate level of volcanic activity. When the volcano’s ash explosions became
    stronger, the organization warned “of imminent hazardous eruption.” The government
    immediately undertook forced evacuation of thousands of families from the volcano
    island and villages surrounding Taal lake, as it declared those ashfall-covered
    areas surrounding the volcano as no man’s land.
  body: "During the emergency, scientists at Phivolcs were near precise with their
    news bulletins and firm with warnings, even when some residents and local government
    officials complained of the restrictions imposed. Philvolcs [did not lack defenders](https://news.abs-cbn.com/news/01/21/20/sila-ang-dalubhasa-govt-execs-defend-phivolcs-after-vice-mayors-tirade)
    when a [popular broadcaster](https://verafiles.org/articles/vera-files-fact-check-mon-tulfo-falsely-claims-phivolcs-fail)
    and some lawmakers[ accused](about:blank) the agency of failing to warn the public
    about Taal volcano’s eruption. In general, the emergency science communication
    following the eruption of the volcano was hailed as a swift success. \n\nIt was
    on the heels of the volcanic crisis that COVID-19 made its way to the Philippines—a
    disaster for which communication was not nearly as smooth.\n\nThe month-long eruption
    of the Taal volcano affected more than [500,000](https://www.pna.gov.ph/articles/1093705)
    people in four provinces, and killed 39 people, but had “[minimal](https://www.pna.gov.ph/articles/1091423),”
    effect on the national economy. It is not generally comparable to the magnitude
    of the calamity that the COVID-19 pandemic has so far caused, which [economists
    estimate](https://www.bworldonline.com/impact-of-covid-19-on-key-philippine-economic-sectors/)
    to reach P276.3 billion (US$5.5 billion) to P2.5 trillion (US$50.2 billion). However,
    crisis communications amongst leadership may be one element through which the
    two deadly events can be compared. \n\nPhilippine health officials went through
    anger, frustration and distrust from the public before their pandemic efforts
    were appreciated. So far, the virus has afflicted more than 70,000 people and
    claimed the lives of at least 1,800 others as of July 22, 2020 in the country.\n\nThe
    tide of public sentiment turned for the national Department of Health (DOH) when
    it decided to put competent officials untainted by politics on the front line,
    like Assistant Secretary Maria Rosario Vergeire, the spokesperson for COVID-19
    related matters.\n\nRegardless, newly-gained public trust enjoyed by DOH still
    does not extend to the head of the office health chief, Secretary Francisco Duque
    III, whose [resignation](https://news.abs-cbn.com/news/04/16/20/senators-demand-duques-resignation-as-health-chief-over-coronavirus-crisis)
    was demanded by a majority of senators. Nor does it include President Rodrigo
    Duterte, whose [midnight briefings](https://opinion.inquirer.net/128693/duterte-is-the-weakest-link)
    on COVID-19 have become a source of annoyance, confusion and information disorder.\n\nDuque,
    an immunologist and pathologist and a graduate of Harvard School of Public Health,
    presided over the 2009 H1N1 outbreak during his first stint as health secretary.
    Despite his experience, the country’s top public health authority [dilly-dallied](https://malaya.com.ph/index/index.php/news_opinion/crowning-glory/)
    on crucial issues related to COVID-19 at the beginning of the pandemic. Appearing
    in a Jan. 29 hearing on COVID-19, called by the House of Representatives, Duque
    towed the line of President Duterte, that there was no need to impose a travel
    ban to and from China, not even to and from the province of Hubei where Wuhan,
    the first epicenter of the novel coronavirus disease outbreak, is located.\n\n“If
    we do this, then the concerned country, China in this case, might question why
    we’re not doing this for all the other countries that have reported confirmed
    cases of the [COVID-19],” Duque said. Critics said a health secretary should only
    be concerned about health matters, and should leave the business of diplomatic
    relations to the foreign secretary. Duque got an unlikely defender in the person
    of a Duterte critic, Sen. Leila de Lima, who is currently detained on allegations
    of drug trafficking. De Lima said the health secretary is merely [implementing
    a policy](https://newsinfo.inquirer.net/1260791/blame-duterte-not-duque-for-gross-negligence-in-covid-19-response#ixzz6SneCeS6O)
    set by the President.\n\nRelations between the Philippines and China [elicit intense
    sentiments](https://www.sws.org.ph/swsmain/artcldisppage/?artcsyscode=ART-20190405180119)
    among Filipinos, because the two countries are embroiled in a territorial dispute
    over islands in the South China Sea. The Philippines has won the case in the Permanent
    Court of Arbitration in The Hague in 2016, but the Duterte government decided
    to set aside the legal victory in favor of closer relations with the economic
    superpower, which has built artificial islands turned military garrisons in the
    disputed maritime area. Filipinos also resent the influx of Chinese working in
    online gaming in the country that, although it has boosted the real estate industry,
    [ has spawned ](https://www.aljazeera.com/news/2020/03/philippines-gambling-chinese-syndicates-linked-crime-surge-200305180334678.html)criminal
    activities including corruption, prostitution and murders.\n\nEventually, the
    government imposed a COVID-19 travel ban which many saw as superfluous, because
    by that time, Hubei was already on lockdown. Duterte extended the travel ban to
    cover any person traveling directly from China, Hong Kong, and Macau, with the
    exception of Filipinos and holders of permanent resident visas. Later, he included
    Taiwan in the travel ban following the country’s One- China policy that considers
    the island state a province of China. It was lifted after a few days following
    a protest from Taiwan and uproar from the 160,000 Filipino workers there.\n\n###
    Face Masks\n\nConflicting statements and actions by government officials about
    face masks confused the public even further. On January 12, at around the same
    time that the world was being alerted about the outbreak of a “mystery virus”
    in Wuhan, the Taal Volcano erupted, spewing ashes that covered surrounding communities.
    The eruption resulted in a high demand for protective masks, which continued to
    be a valued commodity after the volcano’s alert level was lowered, and as the
    threat of the COVID-19 grew. \n\nOn Feb. 5 the third COVID-19 case in the country
    was [confirmed](https://www.doh.gov.ph/doh-press-release/doh-confirms-3rd-2019-nCoV-ARD-case-in-PH)
    to be a 60-year-old woman who came from Wuhan, China. The Land Transportation
    Franchising and Regulatory Board (LTFRB), the government agency which manages
    operations of public utility vehicles in the country, issued a [memorandum](http://ltfrb.gov.ph/wp-content/uploads/2020/02/MC-2020-005.pdf)
    ordering all public utility drivers and conductors “to properly wear face masks
    at all times while on duty.” Noncompliance would [cause](https://www.doh.gov.ph/node/19359)
    them a penalty fee of P5,000 for breach of franchise regulations. Three days later
    the Health Department issued an [advisory](https://news.mb.com.ph/2020/02/11/doh-nods-rule-requiring-puv-drivers-to-wear-masks-says-its-prudent/?)
    clarifying that “people in good health” need not use face masks. Individuals who
    are sick, or those who usually come in close contact with the sick, are the only
    ones recommended to use masks. The DOH later [acknowledged](https://news.mb.com.ph/2020/02/11/doh-nods-rule-requiring-puv-drivers-to-wear-masks-says-its-prudent/?)
    “it might be prudent for drivers to use a mask” since they are exposed to a lot
    of people daily.” \n\nA Manila city official was seen incorrectly [demonstrating](https://verafiles.org/articles/vera-files-fact-check-manila-vice-mayor-lacuna-demos-wrong-w),
    during a televised press conference, the right way to wear a face mask. Manila
    Vice Mayor Sheila \"Honey\" Lacuna-Pangan, a former physician at the Manila health
    office, told the public that when one is sick, one should wear the white part
    on the inside because the white side prevents or blocks the bacteria, and the
    blue side should be outside. The WHO, however, said there is only one proper way
    to wear surgical masks: the colored side is out. The video of that press conference
    has been taken down online.\n\nThis confusion and diminished credibility in authorities
    fueled rumors and speculations about public health precautions. A Jan. 22 post
    by a Facebook user, for example, claimed that the dreaded virus was already in
    the popular island resort of Boracay, in the southern part of the Philippines.
    The [post, which VERA Files](https://verafiles.org/articles/vera-files-fact-check-duterte-did-not-open-pgh-wuhan-covid-1)
    tagged as “False” was shared 5,600 times in just a few days. It was easy for the
    Boracay story to be believed because alarm was raised about the large number of
    tourist arrivals from Wuhan in time for the Jan. 25 Chinese New Year, before a
    lock down on the COVID-19 epicenter was imposed.\n\nRiding on Duterte’s pro-China
    policy, [a false report](https://verafiles.org/articles/vera-files-fact-check-online-posts-claiming-novel-coronaviru)
    by the website City News (ab-tc.com) claimed that the President ordered the Philippine
    General Hospital (PGH) to accept COVID-19 patients from Wuhan. The post, showing
    an April 2019 Reuters photo of Duterte with Chinese President Xi Jinping, was
    shared by pro-Duterte groups and, using an app that measures potential reach of
    a certain post,  could have reached 94,000 Facebook users.The panic that the misinformation
    created among the population resulted in cancellation of classes, conferences,
    cultural performances and pop concerts long before a Luzon-wide lockdown was imposed.\n\n###
    Press freedom: A casualty in a national emergency situation\n\nOn March 16 the
    Philippines had registered two deaths from COVID-19 infection and the confirmed
    infections increased to more than 40. President Duterte declared a nationwide
    state of calamity, to facilitate response and the release of funds to combat the
    fast-spreading virus.\n\nEnhanced Community Quarantine was imposed throughout
    Luzon, the largest and most populous island in the country, including Metro Manila.
    Duterte called on the Armed Forces of the Philippines (AFP) and the Philippine
    National Police (PNP) to lead in the implementation of the quarantine guidelines.\n\nOn
    March 23, Philippine Congress passed a law ([Republic Act No. 11469)](https://www.senate.gov.ph/Bayanihan-to-Heal-as-One-Act-RA-11469.pdf)
    giving the President “powers that are necessary and proper to carry out the declared
    national policy.” Punishable acts enumerated in the law, which is known as “Bayanihan
    to Heal as One Act,” include:\n\n> Individual or groups creating, perpetrating,
    or spreading false information regarding the Covid-19 crisis on social media and
    other platforms, such as information having no valid or beneficial effect on the
    population, and are clearly geared to promote chaos, panic, anarchy, fear, or
    confusion; and those participating in cyber incidents that make use or take advantage
    of the current crisis situation to prey on the public through scams, phishing,
    fraudulent emails, or other similar act.\n\nPress freedom advocates and groups
    [protested](https://www.gmanetwork.com/news/news/nation/731258/groups-question-anti-fake-news-provision-in-covid-19-response-bill/story/)
    the inclusion of that provision in the law. The National Union of Journalists
    in the Philippines (NUJP) said: “While we acknowledge the need to fight disinformation
    in this time of crisis, we fear the Bayanihan to Heal as One Act will only end
    up criminalizing free speech.”\n\n“In times of crisis, when the swift delivery
    of accurate information to our people is vital, we need more, not less, independent
    reporting,” NUJP added.\n\nNUJP’s concern is not without basis. [Human Rights
    Watch slammed](https://hronlineph.com/2020/04/23/statement-hrw-calls-for-release-of-cebu-writer-arrested-for-fb-post/)
    authorities for the arrest of Maria Victoria Beltran, an artist from Cebu City
    in southern Philippines for “misuse of the law” over a Facebook post which she
    said was actually a satire.\n\nOn May 3, Press Freedom Day, PNP-Anti-Crime Group,
    [in a press release](http://pnp.gov.ph/index.php/news-and-information/3682-32-persons-nabbed-for-fake-news-on-social-media-cops-with-covid-19-now-at-19),
    said 32 persons were arrested for “spreading fake news on social media portals
    that have caused panic among people.” It did not say what were the posts nor mention
    the names of those arrested.\n\nThe Commission on Human Rights (CHR) has reminded
    the government that public service requires a higher tolerance for opinions and
    criticisms, especially that a democracy works best when there are healthy discourses
    on governance; thereby, allowing greater accountability from our public officials.\n"
  author:
  - sys:
      id: 1jp9N5eXr3trjtTIHAMq1k
      created_at: !ruby/object:DateTime 2020-08-12 19:45:33.837000000 Z
      updated_at: !ruby/object:DateTime 2020-08-13 20:24:11.871000000 Z
      content_type_id: contributor
      revision: 2
    name: Ellen Tordesillas, Merinette Retona, and Celine Isabelle Samson
    bio: VERA Files is a media nonprofit that probes Philippine issues and fact-checks
      false and misleading claims.
  organization: VERA Files
  publication:
  - sys:
      id: 2lQKVWqTIPTns0oj5UA78S
      created_at: !ruby/object:DateTime 2020-06-15 19:58:33.906000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:12:32.900000000 Z
      content_type_id: publication
      revision: 6
    title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
    short_title: 2020 Misinfodemic Report
    kicker: A Check Global Report
    description: 'Misinfodemic: when online misinformation contributes to real-world
      disease. In the coming months, Meedan will publish eight pieces of reporting
      and analysis in an effort to map the anatomy of the global COVID-19 information
      ecosystem.'
    slug: 2020-misinfodemic-report-covid-19-in-emerging-economies
    sections:
    - sys:
        id: 6K6TuAvZBDXdJATv8SynCi
    - sys:
        id: 1jkEcTAbvK0Y7O1NmysvKT
    - sys:
        id: 3ixzizcKVwnSXEFRRyVuxL
    cover_image:
      sys:
        id: Tu2ovEuxh8gztQD9kuF0G
        created_at: !ruby/object:DateTime 2020-06-23 05:08:56.807000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:33.955000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/Tu2ovEuxh8gztQD9kuF0G/0f5da326dfe06869775caf1002c330e6/meedan_report_2020-infodemic-report_01.png"
    publication_promo_graphic:
      sys:
        id: 5wkwfNNTjvV1Cgf2FAQFD4
        created_at: !ruby/object:DateTime 2020-04-16 20:11:43.061000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:37.263000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5wkwfNNTjvV1Cgf2FAQFD4/fbdc448905c4b9240944587574577654/Meedan_global-misinfodemic-report.png"
    social_card_image:
      sys:
        id: 5L7KfUhYU7lqikodPkJoQQ
        created_at: !ruby/object:DateTime 2020-06-22 23:00:20.044000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:39.651000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5L7KfUhYU7lqikodPkJoQQ/7e82797cad59e546a8fc7e5f4cb23a87/meedan_report_2020-misinfodemic-report_01.png"
    publication_promo: This report is part of a Check Global Report about [COVID-19
      in Emerging Economies](https://meedan.com/reports/2020-misinfodemic-report-covid-19-in-emerging-economies/).
    color:
      sys:
        id: 43kw5bcVj1xtfe7ua88F6V
  color:
    sys:
      id: 3GR3UkNztTEVBUahEh1b7q
      created_at: !ruby/object:DateTime 2020-08-11 20:11:35.807000000 Z
      updated_at: !ruby/object:DateTime 2020-08-13 20:24:18.585000000 Z
      content_type_id: color
      revision: 1
    name: light grayish purple
    value: "#9F82C4"
    thumbnail:
      sys:
        id: 5PxhPIyIf2tkF04WlosZJl
        created_at: !ruby/object:DateTime 2020-08-11 20:11:39.302000000 Z
        updated_at: !ruby/object:DateTime 2020-08-11 20:11:52.621000000 Z
      title: 9F82C4
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5PxhPIyIf2tkF04WlosZJl/b9b7b671f6d28a99a9bbb5d4db8671b7/9F82C4"
    text: "#262626"
  image: https://images.ctfassets.net/g118h5yoccvd/4RvRYJ3u3RXqT5J2FzGfez/398d73327d9e2f776a071e6be32e3bd4/meedan_report_covid-19-Philippines.png
- sys:
    id: 7MQ5rF8CI1JCFyhvvfezeK
    created_at: !ruby/object:DateTime 2020-08-13 02:16:29.415000000 Z
    updated_at: !ruby/object:DateTime 2020-11-12 01:04:11.043000000 Z
    content_type_id: report
    revision: 1
    space:
      sys:
        id: g118h5yoccvd
    type: Entry
    content_type:
      sys:
        id: report
    environment:
      sys:
        id: master
        type: Link
        linkType: Environment
    locale: en-US
  title: Churches and the COVID-19 misinfodemic in South Korea
  slug: churches-and-the-covid-19-misinfodemic-in-south-korea
  description: Some group infections at religious institutions are linked to health
    misinformation
  lead_image:
    sys:
      id: 7xrAaHwf6cPc4M7OHn8gb4
      created_at: !ruby/object:DateTime 2020-08-13 03:38:18.725000000 Z
      updated_at: !ruby/object:DateTime 2020-08-20 15:22:26.697000000 Z
    title: Churches and the COVID-19 misinfodemic in South Korea
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/7xrAaHwf6cPc4M7OHn8gb4/84577e9813cb8df207e06da700227b75/south-korea.png"
  social_card_image:
    sys:
      id: 79Qb4XpGv1rvoaxHnKfGC7
      created_at: !ruby/object:DateTime 2020-08-13 03:38:44.499000000 Z
      updated_at: !ruby/object:DateTime 2020-08-20 15:22:31.993000000 Z
    title: Churches and the COVID-19 misinfodemic in South Korea
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/79Qb4XpGv1rvoaxHnKfGC7/c00ec69f1c229d7e6efa026170aceaf3/meedan_report_covid-19-korea.png"
  introduction: South Korea is lauded globally for its early and swift response to
    the COVID-19 pandemic, but online and offline misinformation in some of the country’s
    religious groups played an outsize role in the spread of disease. The majority
    of South Korea’s “group infections”—which are defined as epidemics in specific
    areas, such as schools, companies, churches, or factories—originated in the country’s
    religious institutions. Churches [accounted for](https://www.cdc.go.kr/board/board.es?mid=a20501000000&bid=0015)
    nearly 40 per cent of the country’s total cases  since Jan. 20, when the first
    confirmed case of the coronavirus came out. The most widely reported example of
    this phenomenon was the Shincheonji Church of Jesus, but misinformation amongst
    other religious leaders also contributed to the spread of COVID-19 in the country.
    This report is an effort to outline group infections in religious centers in South
    Korea, and analyze the role that misinformation played in how the virus spread
    amongst members of some of the country’s churches.
  body: "### Shincheonji Church of Jesus\n\nOn February 18, 2020 a 61-year-old woman
    living in Daegu, South Korea was diagnosed as the country’s 31st COVID-19 patient.
    She was a member of the Shincheonji Church of Jesus, a secretive Christian group
    in South Korea. Lee Man-Hee, the founder of the church, calls himself as the Messiah
    and has been [accused of brainwashing members](https://www.nytimes.com/2020/03/10/world/asia/south-korea-coronavirus-shincheonji.html).
    Many South Koreans have called for the church to be disbanded, as it is widely
    perceived to be a cult. The 31st diagnosis in South Korea was a ‘super spreader’
    event, and it changed the trajectory of the disease in the country significantly.
    \ About [5,300 confirmed cases](https://www.lawtimes.co.kr/Legal-News/Legal-News-View?serial=162398)of
    COVID-19 in South Korea have now been linked by contact tracers to the Shincheonji
    Church, including direct and secondaryinfections. \n\nThe secretive operations
    of the Shincheonji Church means that members are not easy to identify. Accounts
    from a Shincheonji Church victim group reported that during the epidemic, the
    leader of the church texted church members’ cell phones, articulating that COVID-19
    was a “great disaster” predicted in the book of Revelation, and that church members
    should not worry about the virus. “He had misled his followers with such an absurd
    logic, saying that the Coronavirus was caused by demons, who saw the rapid growth
    of Shincheonji and tried to stop it,” said Yeon-ho Hong, former leader of the
    national coalition of Shincheonji victims. The church members, Yeon-ho described,
    would have to endure the virus and win the test. Members continued to worship
    in confined spaces in their churches, against government advice to halt mass gatherings.
    Yeon-ho said “church members were told not to be afraid of the illness.” Shincheonji’s
    spokespeople have articulated that the directives were never church policy, and
    that the church leaders are co-operating with the government.\n\nAccording to
    a [month-long investigation](https://www.cdc.go.kr/board/board.es?mid=a20501000000&bid=0015)
    by the South Korean federal government and the City of Daegu, between February
    22nd to March 12th 2020 about 4,100 COVID-19 cases originated from the Daegu Shincheonji
    center. At that time, the country’s total confirmed cases sat at around [8,000](https://www.cdc.go.kr/board/board.es?mid=a20501000000&bid=0015),
    according to Central Disease Control and Prevention Headquarters, making the church
    the origin of more than half the cases.\n\n### Eunhye River Church\n\nAt Eunhye
    River Church in Seongnam, Gyeonggi Province, seventy eight members were infected
    by the virus. Eunhye River Church leaders believed that [spraying salt water ](https://imnews.imbc.com/replay/2020/nwdesk/article/5673347_32524.html)in
    one’s mouth would prevent COVID-19. The false social media rumors online suggested
    that salt water would prevent coronavirus because of its antiseptic properties.
    \n\n“We strongly suspect that there was a confirmed case among the worshippers
    caused [by] salt water spray that was not disinfected. Sharing salt water spray
    [is] actually no different from direct [COVID-19] contact,” said Lee Hee-young,
    co-head of the Covid-19 emergency team in Gyeonggi Province. Closed circuit surveillance
    tape showed that the Eunhye River Church sprayed salt water [on] members, according
    an investigation by Ms. Hee-young’s team. She identified the spread of the misinformation
    as an ‘infodemic.’ \n\n### Manmin Central Church\n\nA ended up infecting a number
    of people in different workplaces, according to the [Korean ](https://www.cdc.go.kr/board/board.es?mid=a20501000000&bid=0015)[CDC](https://www.cdc.go.kr/board/board.es?mid=a20501000000&bid=0015).
    Another former pastor of Manmin Central church confirmed the church’s position
    on the virus, saying that “church members were told not to be afraid of the illness.”\n\n###
    General group transmission\n\nAt Bucheon Life Water church another 48 people were
    infected, 32 at the On-cheon Church in Dongnae-gu, Busan, 10 at Geochang Church
    in Gyeungnam Province, and nine at the Gyuamseong Church in Buyeo, South Chungcheong
    Province, according to the [Korean CDC](https://www.cdc.go.kr/board/board.es?mid=a20501000000&bid=0015).
    In these cases, it remains unclear if misinformation played a role in the churchgoers
    gathering against government regulations, but what is known is that mass gatherings
    took place and many of these infections reverberated throughout the country. Based
    on the [Korean government’s contact tracing](https://www.hankookilbo.com/News/Read/202003221339370665),
    a Eunhye Riber church member who was infected at church later infected his co-worker,
    who ended up infecting his wife.\n\nThe worst recorded case of the tertiary infection
    happened at a workplace of an infected Manmin Center church member. The church
    member’s workplace, a customer service call center, consists of multiple call
    booths in a cramped space and it created a cluster of infections [over 150 individuals](https://www.yna.co.kr/view/AKR20200325128700017).
    Korea CDC diligently traced all contacts of the infected people and confirmed
    this cluster.\n\nTo be sure, many churches in South Korea took steps to bring
    worship services online following news of the coronavirus outbreak and as an added
    counter-measure the South Korean government created a plan to place public health
    officers at multi-use facilities such as religious facilities. The measures include
    no entry to the religious facilities without wearing a mask or multiple entrance
    check points for any symptoms before entering the facilities.\n\nCompared to other
    countries, the virus has been successfully fought from spreading, thanks to a
    massive amount of testing, contact tracing, and preemptive quarantine measures.
    The South Korean government lifted the restrictions and further opened the country
    on May 6,, including the opening of theaters, restaurants, and shopping malls.
    The government also provided detailed guidelines such as mandatory wearing of
    masks and daily indoor air ventilation. The South Korean citizens are eager to
    go back to their daily lives. \n\nTo counter such future infection clusters at
    churches, the Seoul Metropolitan Government filed a complaint against the Sarang
    First Church, which did not follow the city’s ban on large assemblies and had
    a mass Sunday worship on March 29th. Kim Kang-lip, general coordinator of the
    Central Disaster and Safety Countermeasures Headquarters, said \"the spread of
    the Covid-19 is slowing down, but we should not let our guard down.\" He added
    \"we are fully informing each religious facility about the dangers. We are asking
    them to recognize the severity of the situation and to adopt preventive measures
    as much as possible.\"\n"
  alt_date: !ruby/object:DateTime 2020-08-20 00:00:00.000000000 Z
  author:
  - sys:
      id: 1SMJNVBnDuJPDRItwVdLrg
      created_at: !ruby/object:DateTime 2020-08-13 14:39:02.776000000 Z
      updated_at: !ruby/object:DateTime 2020-08-20 21:46:05.155000000 Z
      content_type_id: contributor
      revision: 4
    name: Taeksoo Cho
    bio: Mr. Cho is the news reporter at JTBC, which is one of the main broadcasting
      companies in Korea. His coverage focuses on investigative reporting as well
      as national disaster. After one year of the research in the University of Reno-Nevada,
      he is currently working on the area of the digital strategy and coverage of
      the media. Mr. Cho received his M.A. in Journalism and Mass Communication from
      the University of Wisconsin, Madison. His undergraduate major is Education Sociology
      from Korea University, Seoul, Korea.
  - sys:
      id: 3KjvkdV11QQA5ZylXuOkWu
      created_at: !ruby/object:DateTime 2020-08-20 15:20:25.891000000 Z
      updated_at: !ruby/object:DateTime 2020-08-20 21:46:24.018000000 Z
      content_type_id: contributor
      revision: 3
    name: Gi W Yun
    staff: false
    bio: Professor Yun is the director of Center for Advanced Media Studies at Reynolds
      School of Journalism, University of Nevada, Reno. His research focuses on social
      psychological aspects of media and communication. The topics of his publications
      range from theories of communication, internet research methodology, social
      media, health communication, agenda setting, newspaper’s community capital,
      and social network analysis to big social media data analysis. He enjoys thinking
      about tools and theories developed by media and communication scholars.
  publication:
  - sys:
      id: 2lQKVWqTIPTns0oj5UA78S
      created_at: !ruby/object:DateTime 2020-06-15 19:58:33.906000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:12:32.900000000 Z
      content_type_id: publication
      revision: 6
    title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
    short_title: 2020 Misinfodemic Report
    kicker: A Check Global Report
    description: 'Misinfodemic: when online misinformation contributes to real-world
      disease. In the coming months, Meedan will publish eight pieces of reporting
      and analysis in an effort to map the anatomy of the global COVID-19 information
      ecosystem.'
    slug: 2020-misinfodemic-report-covid-19-in-emerging-economies
    sections:
    - sys:
        id: 6K6TuAvZBDXdJATv8SynCi
    - sys:
        id: 1jkEcTAbvK0Y7O1NmysvKT
    - sys:
        id: 3ixzizcKVwnSXEFRRyVuxL
    cover_image:
      sys:
        id: Tu2ovEuxh8gztQD9kuF0G
        created_at: !ruby/object:DateTime 2020-06-23 05:08:56.807000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:33.955000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/Tu2ovEuxh8gztQD9kuF0G/0f5da326dfe06869775caf1002c330e6/meedan_report_2020-infodemic-report_01.png"
    publication_promo_graphic:
      sys:
        id: 5wkwfNNTjvV1Cgf2FAQFD4
        created_at: !ruby/object:DateTime 2020-04-16 20:11:43.061000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:37.263000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5wkwfNNTjvV1Cgf2FAQFD4/fbdc448905c4b9240944587574577654/Meedan_global-misinfodemic-report.png"
    social_card_image:
      sys:
        id: 5L7KfUhYU7lqikodPkJoQQ
        created_at: !ruby/object:DateTime 2020-06-22 23:00:20.044000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:39.651000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5L7KfUhYU7lqikodPkJoQQ/7e82797cad59e546a8fc7e5f4cb23a87/meedan_report_2020-misinfodemic-report_01.png"
    publication_promo: This report is part of a Check Global Report about [COVID-19
      in Emerging Economies](https://meedan.com/reports/2020-misinfodemic-report-covid-19-in-emerging-economies/).
    color:
      sys:
        id: 43kw5bcVj1xtfe7ua88F6V
  color:
    sys:
      id: 78KSqVjhG5uRVS8ckR7Sbu
      created_at: !ruby/object:DateTime 2020-08-13 03:39:44.551000000 Z
      updated_at: !ruby/object:DateTime 2020-08-20 15:22:46.457000000 Z
      content_type_id: color
      revision: 1
    name: Light Navy
    value: "#7798FF"
    thumbnail:
      sys:
        id: 5OujIFDf4DXlhZXZqrSlof
        created_at: !ruby/object:DateTime 2020-08-13 03:39:50.753000000 Z
        updated_at: !ruby/object:DateTime 2020-08-13 03:40:04.614000000 Z
      title: 7798FF
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5OujIFDf4DXlhZXZqrSlof/a3a7daaba6a5bd053f88913b71553693/7798FF"
    text: "#262626"
  image: https://images.ctfassets.net/g118h5yoccvd/79Qb4XpGv1rvoaxHnKfGC7/c00ec69f1c229d7e6efa026170aceaf3/meedan_report_covid-19-korea.png
- sys:
    id: 6ZJdYXyGDsYe5mgSIzXsoP
    created_at: !ruby/object:DateTime 2020-09-21 22:38:49.519000000 Z
    updated_at: !ruby/object:DateTime 2020-11-12 01:04:34.147000000 Z
    content_type_id: report
    revision: 2
    space:
      sys:
        id: g118h5yoccvd
    type: Entry
    content_type:
      sys:
        id: report
    environment:
      sys:
        id: master
        type: Link
        linkType: Environment
    locale: en-US
  title: Bolsonaro’s Messaging Sows Misinformation, Confusion Throughout Latin America
  slug: bolsonaros-messaging-sows-misinformation-confusion-throughout-latin-america
  description: Amid thousands of deaths, economics take priority over health
  lead_image:
    sys:
      id: 22CxIbHZupBky5cIAtRozz
      created_at: !ruby/object:DateTime 2020-09-22 02:55:37.712000000 Z
      updated_at: !ruby/object:DateTime 2020-09-22 22:45:16.346000000 Z
    title: Bolsonaro’s Messaging Sows Misinformation, Confusion Throughout Latin America
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/22CxIbHZupBky5cIAtRozz/c951e9d6a486d5cae3b67d229286a647/meedan_report_covid-19-brazil-illus.png"
  social_card_image:
    sys:
      id: 5GuNPCsu4xxtrJDuIZEtwg
      created_at: !ruby/object:DateTime 2020-09-22 02:58:06.605000000 Z
      updated_at: !ruby/object:DateTime 2020-09-22 22:45:20.621000000 Z
    title: Bolsonaro’s Messaging Sows Misinformation, Confusion Throughout Latin America
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/5GuNPCsu4xxtrJDuIZEtwg/209e6c685c16ab51b6c61b2c0d851bbe/meedan_report_covid-19-brazil.png"
  introduction: At the beginning of the COVID-19 pandemic, Brazilian president Jair
    Bolsonaro told the public without evidence that the now-disproven treatment hydroxychloroquine
    [could “cure COVID-19](https://br.reuters.com/article/idUSKBN249396)." That misinformation,
    delivered definitively by the country’s leader, provoked the Brazlian public to
    try to [gain access to the medicine](https://www.nytimes.com/2020/06/13/world/americas/virus-brazil-bolsonaro-chloroquine.html).
    People with critical diseases like lupus, who need the drug for scientifically-backed
    treatment purposes were hospitalized after being unable[ to access the medication](https://noticias.r7.com/saude/falta-de-cloroquina-causa-internacao-de-pacientes-com-lupus-08062020),
    due to hoarding by misinformed citizens.
  body: "On July 9, 2020 Bolsonaro appeared in public claiming he had contracted the
    disease, and said that he would take [hydroxychloroquine](https://learnaboutcovid19.org/questions/what-does-the-scientific-literature-say-about-using-hydroxychloroquine-to-prevent-treat-or-cure-covid-19/)
    as a treatment. The president then falsely sold the medicine, again, as a \"savior.”
    When Bolsonaro’s Minister of Health Henrique Mandetta rejected [hydroxychloroquine](https://www.hindustantimes.com/world-news/bolsonaro-s-love-for-hydroxychloroquine-costs-brazil-another-health-minister/story-jkEFmRFR4sZYneLZxQjb1J.html)
    as a drug for the virus, Bolsonaro replaced him with another Minister, who left
    the portfolio for similar reasons. \n\nThis is just one example of the Brazilian
    president's failings amid our current international crisis. He has opposed national
    and international health agencies, hampered the actions of state governors and
    worries primarily about the economy, taking a strong stand against isolation and
    distancing measures that are known to prevent the disease. The head of the nation
    told the public he was going to [have a barbecue](https://g1.globo.com/politica/noticia/2020/05/09/depois-de-anunciar-churrasco-bolsonaro-diz-que-informacao-e-fake-e-critica-jornalistas.ghtml),
    and [paraded through crowds unmasked. ](https://www.istoedinheiro.com.br/bolsonaro-um-presidente-provocador-e-sem-mascara/)\n\nBrazil’s
    population banged [pots on windows](https://oglobo.globo.com/brasil/panelacos-em-capitais-expoem-desgaste-de-bolsonaro-apos-crise-do-coronavirus-24313870)
    in protest of their government. The country’s leadership is now considered one
    of the worst in the face of the COVID-19 crisis. Bolsonaro has also downplayed
    the severity of COVID-19, and questioned the work of scientific experts. When
    the number of deaths reached records of more than 1,000 a day, Bolsonaro doubted
    the case counts, and [removed months of data from an official website](https://www.bbc.com/news/world-latin-america-52952686).
    A [consortium of traditional Brazilian newspapers](https://g1.globo.com/politica/noticia/2020/06/08/veiculos-de-comunicacao-formam-parceria-para-dar-transparencia-a-dados-de-covid-19.ghtml)
    formed to make the count independent of the national government, through a task
    force that updated the statistics with the Brazilian state agencies. \n\nDébora
    D'Ávila, director of Scientific Dissemination at the University of Minas Gerais
    (UFMG) in Belo Horizonte, said, \"People cannot trust anything that he (Bolsonaro)
    talks about. His inconsistency ... the speech changes depending on the interest.”\n\nThese
    issues are exacerbated by an online environment in which governments compete with
    online content creators for the attention of their citizens. “We have influencers
    and Youtubers expressing their opinion, gaining attention that investigative journalists
    do not have,\" said Janine Miranda Cardoso, professor of the graduate program
    of ICICT - Institute of Communication and Scientific Information and Health Technology
    Foundation of the Oswaldo Cruz Foundation[ (Fiocruz).](https://portal.fiocruz.br/)
    \"Combating misinformation is necessary, but insufficient. For an epidemic, the
    path is to strengthen health services, which are without resources,” Cardoso added.
    \n\nBolsonaro has in fact criticized the way state [governors](https://www1.folha.uol.com.br/poder/2020/03/veja-o-que-bolsonaro-ja-disse-sobre-coronavirus-de-certa-histeria-a-fantasia-e-nerouse.shtml)
    reacted to the pandemic, saying the people were being deceived by them and the
    media. He spoke of allegedly excessive measures, which he called [\"hysteria,”
    that \"would hurt the economy.”](https://www1.folha.uol.com.br/poder/2020/03/veja-o-que-bolsonaro-ja-disse-sobre-coronavirus-de-certa-histeria-a-fantasia-e-nerouse.shtml)\n\nBolsonaro's
    inability to strategize an effective COVID-19 response led to major Latin America
    partners [closing borders with Brazil](https://www.wilsoncenter.org/blog-post/contagion-brazils-covid-19-catastrophe-spills-over-borders).
    Paraguay, Uruguay, Argentina, French Guiana and Venezuela ceased travel to and
    from Brazil, with multiple leaders explicitly pointing to Brazil's COVID-19 response
    as a major contributing factor. \n\nThe president’s response to COVID-19 impacts
    the entire global market. Brazil has the [ninth largest GDP in the world](https://www.mercadoeconsumo.com.br/2020/01/10/brasil-deve-ser-a-9a-maior-economia-do-mundo-no-fim-da-decada/),
    is the second largest producer of soybeans, is among the 30 countries that export
    the most products, including beef, corn and oil. \n\nA stalled Brazillian economy
    will indeed have supply chain impacts far beyond its borders, but as the disease
    progresses in the country it has become clear that in the Bolsonaro administration,
    economic consequences take priority over the health of the population. \n\nBrazilians
    live feeling that they don’t know the real state of COVID-19 in their country,
    don’t know what information to trust, and aren’t sure that leaders are taking
    all necessary measures to prevent and reduce damage."
  author:
  - sys:
      id: 4h6BuKbk1d9fXmcm4ybfAz
      created_at: !ruby/object:DateTime 2020-09-21 22:39:38.933000000 Z
      updated_at: !ruby/object:DateTime 2020-09-23 14:02:31.239000000 Z
      content_type_id: contributor
      revision: 1
    name: Joana Suarez
    bio: Suarez is a freelance reporter, focused on in-depth journalism about human
      rights, feminism, health, education and the environment.
  publication:
  - sys:
      id: 2lQKVWqTIPTns0oj5UA78S
      created_at: !ruby/object:DateTime 2020-06-15 19:58:33.906000000 Z
      updated_at: !ruby/object:DateTime 2020-07-28 23:12:32.900000000 Z
      content_type_id: publication
      revision: 6
    title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
    short_title: 2020 Misinfodemic Report
    kicker: A Check Global Report
    description: 'Misinfodemic: when online misinformation contributes to real-world
      disease. In the coming months, Meedan will publish eight pieces of reporting
      and analysis in an effort to map the anatomy of the global COVID-19 information
      ecosystem.'
    slug: 2020-misinfodemic-report-covid-19-in-emerging-economies
    sections:
    - sys:
        id: 6K6TuAvZBDXdJATv8SynCi
    - sys:
        id: 1jkEcTAbvK0Y7O1NmysvKT
    - sys:
        id: 3ixzizcKVwnSXEFRRyVuxL
    cover_image:
      sys:
        id: Tu2ovEuxh8gztQD9kuF0G
        created_at: !ruby/object:DateTime 2020-06-23 05:08:56.807000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:33.955000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/Tu2ovEuxh8gztQD9kuF0G/0f5da326dfe06869775caf1002c330e6/meedan_report_2020-infodemic-report_01.png"
    publication_promo_graphic:
      sys:
        id: 5wkwfNNTjvV1Cgf2FAQFD4
        created_at: !ruby/object:DateTime 2020-04-16 20:11:43.061000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:37.263000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5wkwfNNTjvV1Cgf2FAQFD4/fbdc448905c4b9240944587574577654/Meedan_global-misinfodemic-report.png"
    social_card_image:
      sys:
        id: 5L7KfUhYU7lqikodPkJoQQ
        created_at: !ruby/object:DateTime 2020-06-22 23:00:20.044000000 Z
        updated_at: !ruby/object:DateTime 2020-06-24 01:05:39.651000000 Z
      title: '2020 Misinfodemic Report: COVID-19 in Emerging Economies'
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5L7KfUhYU7lqikodPkJoQQ/7e82797cad59e546a8fc7e5f4cb23a87/meedan_report_2020-misinfodemic-report_01.png"
    publication_promo: This report is part of a Check Global Report about [COVID-19
      in Emerging Economies](https://meedan.com/reports/2020-misinfodemic-report-covid-19-in-emerging-economies/).
    color:
      sys:
        id: 43kw5bcVj1xtfe7ua88F6V
  color:
    sys:
      id: 2LJYKjp37UnCuU6jPxjatl
      created_at: !ruby/object:DateTime 2020-09-21 22:39:54.901000000 Z
      updated_at: !ruby/object:DateTime 2020-09-25 17:26:05.916000000 Z
      content_type_id: color
      revision: 1
    name: Pale Red
    value: "#F2A7BE"
    thumbnail:
      sys:
        id: 5ECSDr31TvM3aZk10tnjDi
        created_at: !ruby/object:DateTime 2020-09-21 22:40:29.142000000 Z
        updated_at: !ruby/object:DateTime 2020-09-21 22:40:41.385000000 Z
      title: F2A7BE
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5ECSDr31TvM3aZk10tnjDi/c32b3510cae5515eee03bcfbab17aa61/F2A7BE"
    text: "#262626"
  image: https://images.ctfassets.net/g118h5yoccvd/5GuNPCsu4xxtrJDuIZEtwg/209e6c685c16ab51b6c61b2c0d851bbe/meedan_report_covid-19-brazil.png
- sys:
    id: odqEjZ4nMa3Juk671k8wP
    created_at: !ruby/object:DateTime 2020-11-09 16:37:00.535000000 Z
    updated_at: !ruby/object:DateTime 2020-11-17 20:15:15.493000000 Z
    content_type_id: report
    revision: 0
    space:
      sys:
        id: g118h5yoccvd
    type: Entry
    content_type:
      sys:
        id: report
    environment:
      sys:
        id: master
        type: Link
        linkType: Environment
    locale: en-US
  title: 'Content Moderation & Vulnerable Populations: LGBTQ Communities'
  slug: content-moderation-and-vulnerable-populations-lgbtq-communities
  lead_image:
    sys:
      id: 7V6pLKsHl8oEzDvmpXodX
      created_at: !ruby/object:DateTime 2020-11-09 17:01:26.688000000 Z
      updated_at: !ruby/object:DateTime 2020-11-09 20:39:37.551000000 Z
    title: 'Content Moderation & Vulnerable Populations: LGBTQ Communities'
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/7V6pLKsHl8oEzDvmpXodX/fb2b1a44173a9a915c0b0b500d6b7b97/content-moderation-_-vulnerable-populations_LGBTQ-communities_lead-image.png"
  social_card_image:
    sys:
      id: 2KuL0ZHoxLnatKNzrugbpD
      created_at: !ruby/object:DateTime 2020-11-09 17:02:32.502000000 Z
      updated_at: !ruby/object:DateTime 2020-11-17 20:00:37.092000000 Z
    title: 'Content Moderation & Vulnerable Populations: LGBTQ Communities'
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/2KuL0ZHoxLnatKNzrugbpD/730a179c111581e4a85ddc59443eb109/content-moderation-_-vulnerable-populations_LGBTQ-communities_social-card.png"
  introduction: Content moderation and platform policy adversely affect marginalized
    groups of people on social media. For example, members of the LGBTQ community
    face challenges such as risks to privacy and personal safety depending on how
    a platform manages and displays their personal information. Furthermore, LGBTQ
    people face risks in seeking out resources related to their identity if they are
    in the closet or living and working in a hostile environment. When LGBTQ people
    encounter hate crimes online, evidence shows that they are not likely to report
    the crime. These findings are troubling, especially because online communities
    can often be the only space where LGBTQ people have access to a like-minded community,
    mentors, and resources specific to their identity. Keeping all of these challenges
    in mind, how do we design online spaces and make policy decisions in a way that
    protects our most vulnerable users?
  body: "This document seeks to shed light on this question by reviewing the findings
    of recent literature. The document first examples the tension between the need
    for privacy and the need to connect with community online for LBGTQ people. It
    then discusses underlying anxieties and safety concerns that drive user wants
    and needs before turning to explore proposed design solutions. Interwoven are
    three short example cases—Pride Month & Twitch, Streamers & Community Safety,
    and Fandom & Community Regulation—that serve to illustrate various points.\n\n##
    Privacy Needs Versus Finding Community\n\nThere is often a tension between the
    need for privacy and the need to connect with community online for LBGTQ people.
    Alexander Cho (2018), Carrasco & Kerne (2018), and DeVito et al. (2018) discuss
    how LGBTQ people navigate their identity from platform to platform. LGBTQ people
    are more likely to create alternative accounts on social media sites where they
    can comfortably present as LGBTQ, with their “official” account reserved for family
    and offline connections where they might not be out or might not be comfortable
    “acting” LGBTQ. Cho conceptualizes “default publicness,” a standard that many
    social media platforms employ. By designing social media mechanisms to work best
    in a “public” setting, or prioritize creating accounts using a “real” identity,
    platforms favor types of interactions that LGBTQ people could consider risky such
    as posting a photo from an LGBTQ event or with a romantic partner, commenting
    on a public article, or interacting with LGBTQ content.\n\nLGBTQ identity, and
    the presentation of that identity, changes across an ecosystem of social media
    sites (DeVito et al., 2018). Primarily, this means that many LGBTQ people have
    different expectations for how “gay” they can act on different social media sites.
    DeVito et al. (2018) note that research participants did not identify missing
    features that could be implemented to help with this. Instead, their findings
    suggest that adding more privacy features at the account level might put more
    burden on the user. Previous findings have suggested that an “opt-in” model might
    lessen user burden and anxiety around privacy (Carrasco & Kerne, 2018). The authors
    note that Facebook’s “Limit Past Posts” tool offers some of the reflexivity needed
    around LGBTQ privacy concerns, though the tool certainly benefits more than just
    LGBTQ users but, of course, does not solve all the challenges faced by these users.\n\n**Case
    Study: Pride Month & Twitch**\n\n*Since 2018, Twitch has hosted Pride Month events
    each June highlighting LGBTQ content creators. This has included daily highlighting
    of one or more of these content creators on Twitch’s homepage, which drives traffic
    to their livestreams, in some cases increasing their number of concurrent viewers
    by a factor of as much as ten. While visibility is important for these communities,
    the sudden influx of viewers and chatters presents significant moderation challenges.
    New users aren’t familiar with how to behave, and the communities’ normal socialization
    processes cannot handle so many newcomers at once. Malicious newcomers can also
    be a problem. This influx can put content creators in a difficult situation; they
    must decide how to balance welcoming new users with protecting themselves and
    their communities.*\n\nFurthermore, LGBTQ people fear that content amplification
    might bring unwanted attention or recognition to their social media (Dym & Fiesler,
    2018). Participants in Dym & Fiesler’s study described tension around participating
    in online communities, especially people creating artistic content. They described
    wanting to share their LGBTQ-themed art and stories in online communities like
    Tumblr, but worried that if something got too popular or spread too far, people
    who knew them offline would see the content and thus out the person as LGBTQ before
    they were ready. Similar work in Blackwell et al. (2016) has found that LGBTQ
    users on Facebook believed they had a right to privacy and control over their
    information, but also believed sharing some information was necessary for the
    benefit of the community. These findings point to a complex system of heuristics
    that people rely on to determine when and how information is shareable, and to
    whom, especially among vulnerable, marginalized populations.\n\nLGBTQ people have
    also described finding community and valuable resources in online LGBTQ spaces
    (Dym et al., 2019), making the cost of leaving social media platforms, despite
    the risks, far too high. LGBTQ people are uniquely vulnerable in that they often
    cannot afford to disengage with social media if they want to have access to a
    community. People have described learning about their identity for the first time
    online, with many people not even knowing what an LGBTQ identity was before finding
    discussion about them online (Dym et al., 2019). Much like LGBTQ parents viewed
    certain social media activities as advocacy work (Blackwell et al., 2016), LGBTQ
    people in other social media circles described reaching a point where they felt
    as though they owed mentorship and guidance to younger LGBTQ people entering social
    media spaces for the first time.\n\nJean Hardy (2019) describes the information-seeking
    habits of gay men as a type of *queer information literacy* that spans across
    multiple social media platforms, with both online and offline sources complimenting
    one another. These information seeking habits are informed by the risks accompanied
    with seeking information from the wrong source, or a lack of information available
    offline (Dym et al., 2019). Keeping these accounts in mind, it is necessary to
    point out that many LGBTQ people suffer from a kind of information poverty (Lingel
    & boyd, 2013). LGBTQ issues around *information poverty* are different from the
    original definition of information poverty, in which socioeconomic status and
    education informs access and ability to use technology. Instead, LGBTQ people
    suffer from a type of information poverty in which they put themselves at risk
    simply by accessing the information.\n\nThis research demonstrates that LGBTQ
    people have varied and complex privacy needs on social media spaces. And, because
    access to information about LGBTQ identity can be scarce, these online communities
    and resources are sometimes a necessity to LGBTQ people, not a choice. People
    strongly express a desire for tight privacy controls while also wanting to remain
    discoverable under the “right” circumstances. Prior research has theorized that
    online LGBTQ communities maintain an obscured or “selective” visibility that allows
    them to be discoverable only by those seeking LGBTQ support, allowing bad actors
    or unwitting parents to pass by (Dym et al., 2019). Alternatively, platforms can
    design information sharing with personal visibility in mind, meaning that LGBTQ
    people can easily select what characteristics about themselves are visible to
    which audiences (Carrasco & Kerne, 2018).\n\n### These articles strongly recommend:
    \n\n1. Enabling granular privacy controls that focus on how personal content is
    spread or interacted with\n2. Prioritizing obscurity in online communities\n3.
    Maintaining discoverability in non-threatening ways by giving community members
    control to determine how and where their content is seen. \n\nThese sorts of design
    principles can be enacted at an individual level, moderator’s level, or platform
    policy level. The next section discusses underlying anxieties that drive user
    wants and needs.\n\n### Safety Concerns\n\nOverwhelmingly, these findings speak
    to user anxieties around safety. People want to remain safe from bad actors online
    as well as from potentially harmful people in their offline lives. Because accessing
    online LGBTQ spaces can be a vital resource, LGBTQ people might feel trapped into
    using technology that otherwise puts them in danger. Because individual users
    cannot always specify how they wish to control their content, technology platforms
    might consider ways to enact these controls at a community level, thereby distributing
    the burden of safety. By designing content moderation controls that speak to these
    specific anxieties, platforms can empower community spaces to regulate themselves
    and keep marginalized communities safe. Furthermore, platform policies can shift
    their values to align better with user needs that emphasize privacy first.\n\nIn
    order to understand exactly what those policies should support and how to design
    better privacy and moderation controls, we must understand the specific safety
    concerns and anxieties of LGBTQ people. Scheurerman et al. (2018) investigate
    the safety concerns of transgender people in online spaces. In their findings,
    they advocate for designers to amplify marginalized voices. In this particular
    study, this meant talking to transgender people and understanding what parts of
    the system make them feel unsafe or insecure. Prioritizing the concerns of marginalized
    users ensures a design that protects the most vulnerable people as well as everyday
    users.\n\nFurther research by Fernandez & Birnholtz (2019) demonstrates that transgender
    people weigh tensions over whether or not to disclose their identity, especially
    on dating sites. For example, transgender people stated that they did not want
    to waste their time on anyone who would be unaccepting of their identity, but
    also did not want to open themselves up to harassment by openly advertising their
    status as a transgender person. Ultimately, their research indicates that transgender
    people would rather not engage with transphobic people altogether. While participants
    handled that through early self-disclosure, this tactic also exposed them to vitriolic
    reactions. This design space is messy, and features to improve security might
    be explored through group spaces with moderators.\n\n**Case Study: Streamers &
    Community Safety**\n\n*One interviewee in research by Seering et al. (2019) described
    her goal in managing her Twitch community as first and foremost to provide a safe
    space for transgender youth. For this reason, she said that she was very quick
    to remove new users who gave even a hint of threatening this safety. She stated
    that she would strongly prefer to falsely ban a potentially positive contributor
    than to fail to remove a malicious user who ended up hurting her community.*\n\nThe
    research reviewed here all points toward solutions that could potentially reside
    within small-community moderation, or encouraging the development of grassroots
    communities. While not all of social media can or should be housed within small
    communities, platform designers might consider ways in which LGBTQ people can
    better regulate their own community spaces as top-down regulation and moderation
    efforts can adversely affect LGBTQ communities (Fiesler & Dym, 2018). The privacy
    and safety concerns that LGBTQ people face are not necessarily more complex from
    the problems that anyone might encounter on the internet, but the context surrounding
    these issues is underexplored and often too-little designed for. Viewed in connection
    with work around privacy and community, these articles suggest: 1) Exploring privacy
    and safety controls that prioritize “opt-in” mechanics and obscurity, especially
    at a community level; 2) Communicating with stakeholders from marginalized populations
    before implementing changes that will affect them; 3) Designing for community
    building by providing robust moderation tools and flexibility in online group
    spaces rather than providing top-down moderation solutions. The key component
    to these recommendations is the emphasis on *community*. Challenges facing LGBTQ
    people are often situated within a broader community. Designers can explore methods
    for resolving these tensions around privacy and safety by building community-level
    moderation tools that address these concerns. The next section explores proposed
    design solutions that researchers have explored through various methodologies.\n\n###
    Persisting Challenges in Design\n\nDespite researchers and designers working to
    overcome these challenges, there are certain social and cultural barriers we must
    keep in mind when working with vulnerable populations like LGBTQ people. For example,
    LGBTQ people have a history of underreporting hate crimes and disturbances they
    experience online (Gatehouse et al., 2018). Furthermore, online support spaces
    designed for outreach and intervention for struggling LGBTQ youth can struggle
    to identify at-risk people (Homan et al., 2014). Even proposed tools to assist
    vulnerable LGBTQ people can occupy an ethical gray area and must be considered
    carefully before implementation into a community (Saha et al., 2019). In addition
    to these challenges, recent work illustrates the importance of understanding which
    social media sites are best suited for supporting LGBTQ identity processes, such
    as gender transition, and which ones need to remain separate from those processes
    (Haimson, 2018).\n\nUnderstanding what kind of moderation tools can help, and
    how, is a key part to improving the safety conditions and resilience of LGBTQ
    communities online. However, content moderation tools mean very little to LGBTQ
    people if they are discouraged from or ambivalent toward using them. One important
    feature that enables moderation is reporting harmful content or encounters in
    online spaces, especially to authority figures who are empowered to intervene.
    Gatehouse et al. (2018) found that LGBTQ people can be ambivalent toward or distrusting
    of the process of reporting hate crimes. They encouraged design that imbues LGBTQ
    people with a sense of agency and power over harmful situations. For example,
    design artifacts from their workshops focused on correcting or standing up to
    aggressions against LGBTQ people, such as a “pronoun corrector” that magically
    fixes misstated pronouns.\n\nAnother design challenge especially important to
    vulnerable populations centers on identifying people who might be suicidal or
    at risk of self-harm. In spaces dedicated to monitoring at-risk individuals, prior
    research has found that monitoring people for warning signs only works when the
    person participates in the group (Homan et al., 2014). Furthermore, tools such
    as a classifier that can label posts and flag at-risk users should only be used
    with the user’s consent, and can still make mistakes in identifying users or discourage
    people from speaking up (Saha et al., 2019). Saha et al. suggest that their classifier
    works best as an opt-in content moderation tool for large LGBTQ online communities.
    However, tools like this raise the issue of what a content moderator’s role is.
    As a community volunteer, a content moderator is not typically trained in crisis
    response or other essential therapeutic techniques for assisting an at-risk person.
    Professional content moderators might be able to receive that training, or be
    able to connect an at-risk person to someone qualified to help, but these workflows
    are currently under-explored in moderation communities that receive little oversight
    or intervention from the platform itself.\n\nFurther research is needed to understand
    1) how effective volunteer community moderators are and what their roles in moderation
    should be as well as 2) what sort of content moderation the platform itself must
    engage in. Currently, there is little research that explores the exact dividing
    lines between work conducted by volunteer and paid moderators. Any work conducted
    in this space should seek to understand *what active duties* they expect volunteers
    to take on, the *responsibilities of the platform*, and the *desired autonomy
    of the community* in online governance.\n\n**Case Study: Fandom & Community Regulation**\n\n*The
    online community known as “transformative fandom” is home to a majority LGBTQ
    population of content creators and consumers, primarily of fanfiction and fanart.
    The community houses its creative works primarily on the site* Archive of Our
    Own *(AO3), a platform built by its own community, in service of its own community.
    Organizing and tagging new content published to the site falls on the shoulders
    of community volunteers, who then flag inappropriate content—along with the help
    of everyday site visitors—for review by the site’s staff. In this case, the community
    is hands-on in its moderation efforts, and would object to any automated moderation
    techniques considering the highly contextual and expressive nature of fanworks
    uploaded to the site. The community is willing to take on the responsibility of
    reporting inappropriate content as it is uploaded through the appropriate channels.
    This is possible on AO3 primarily because the traffic is not nearly as overwhelming
    as other social media sites, the content uploaded is primarily text, and AO3’s
    users primarily identify as all belonging to the broader community of “fandom.”
    What AO3 demonstrates is the willingness of a community to self-regulate, and
    what successful volunteer-run communities might look like.* \n\nOne last challenge
    that designers face when working with vulnerable communities is the perceived
    homogeneity of marginalized groups and the harm this perception can cause. Hardy
    and Lindtner (2017) explore how a gay dating app is designed to function for urban
    gay men, missing design opportunities for men living in rural areas. Furthermore,
    applications designed with certain perceptions of a population in mind can have
    isolating or othering effects on the users when their experiences do not match
    the application’s promise. For example, women seeking women on Tinder, whether
    in remote areas or otherwise, have reported feeling isolated and alone from using
    the app (Duguay, 2019). Neither of these articles necessarily offer design solutions
    geared toward LGBTQ people and content moderation, but they point to how apps
    designed to supposedly connect LGBTQ people can also misunderstand the design
    space and cause feelings of isolation. \n\nThus, it’s important to recognize that
    designers of content moderation tools for LGBTQ people and other vulnerable populations–
    particularly for marginal populations that commonly evade identification– risk
    designing for communities within that population whose needs are most clear to
    them, at the cost of the needs of the population beyond those communities. Different
    communities in a population have differing needs in how to group and manage their
    own community spaces online and therefore require platforms that enable bespoke
    solutions to accommodate a diversity of participation styles.\n\n### Conclusion\n\nWe
    as researchers and designers need to work toward building content moderation tools
    that work better with the needs of marginalized populations by seeking to understand
    those complex and ever-evolving needs. We also need to explore methods to empower
    marginalized communities with volunteer moderator tools that grant control back
    to the community where they desire it. Moving forward, this two-pronged exploration
    will hopefully help drive innovative solutions toward building safer and more
    inclusive online communities.\n"
  references: "- Blackwell, L., Hardy, J., Ammari, T., Veinot, T., Lampe, C., & Schoenebeck,
    S. (2016, May). LGBT parents and social media: Advocacy, privacy, and disclosure
    during shifting social movements. *In Proceedings of the 2016 CHI conference on
    human factors in computing systems* (pp. 610-622). ACM.\n\n- Carrasco, M., & Kerne,
    A. (2018, April). Queer Visibility: Supporting LGBT+ Selective Visibility on Social
    Media. In *Proceedings of the 2018 CHI Conference on Human Factors in Computing
    Systems* (p. 250). ACM.\n\n- Cho, A. (2018). Default publicness: Queer youth of
    color, social media, and being outed by the machine. *New Media & Society*, 20
    (9), 3183-3200.\n\n- DeVito, M. A., Walker, A. M., & Birnholtz, J. (2018). 'Too
    Gay for Facebook': Presenting LGBTQ+ Identity Throughout the Personal Social Media
    Ecosystem. *Proceedings of the ACM on Human-Computer Interaction, 2*(CSCW), 44.\n\n-
    Duguay, S. (2019). “There’s no one new around you”: Queer Women’s Experiences
    of Scarcity in Geospatial Partner-Seeking on Tinder. In *The Geographies of Digital
    Sexuality* (pp. 93-114). Palgrave Macmillan, Singapore.\n\n- Dym, B., & Fiesler,
    C. (2018, October). Vulnerable and Online: Fandom's Case for Stronger Privacy
    Norms and Tools. In *Companion of the 2018 ACM Conference on Computer Supported
    Cooperative Work and Social Computing* (pp. 329-332). ACM.\n\n- Dym, B., Brubaker,
    J., Fiesler, C., & Semaan, B. (2019). “Coming Out Okay”: Community Narratives
    for LGBTQ Identity Recovery Work. In *Proc. ACM Hum.-Comput. Interact. 3, CSCW*.
    29 pages. ACM.\n\n- Fernandez, J., & Birnholtz, J. (2019). “I Don’t Want Them
    to Not Know”: Investigating Decisions to Disclose Transgender Identity on Dating
    Platforms.  In Proc. ACM Hum.-Comput. Interact. 3, CSCW.  21 pages. ACM.\n\n-
    Fiesler, C., & Dym, B. (December 5, 2018). [Fandom’s Fate is not Tied to Tumblr.](https://slate.com/technology/2018/12/tumblr-fandom-adult-content-ban-livejournal.html
    \"Fandom’s Fate is not Tied to Tumblr\") In *Slate*. \n\n- Gatehouse, C., Wood,
    M., Briggs, J., Pickles, J., & Lawson, S. (2018, April). Troubling Vulnerability:
    Designing with LGBT Young People's Ambivalence Towards Hate Crime Reporting. In
    *Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems*
    (p. 109). ACM.\n\n- Haimson, O. (2018). Social media as social transition machinery.
    *Proceedings of the ACM on Human-Computer Interaction, 2*(CSCW), 63.\n\n- Hardy,
    J., & Lindtner, S. (2017, February). Constructing a desiring user: Discourse,
    rurality, and design in location-based social networks. In *Proceedings of the
    2017 ACM Conference on Computer Supported Cooperative Work and Social Computing*
    (pp. 13-25). ACM.\n\n- Hardy, J. (2019). Queer information literacies: social
    and technological circulation in the rural Midwestern United States. *Information,
    Communication & Society,* 1-16.\n\n- Homan, C. M., Lu, N., Tu, X., Lytle, M. C.,
    & Silenzio, V. (2014, February). Social structure and depression in TrevorSpace.
    In *Proceedings of the 17th ACM conference on Computer supported cooperative work
    & social computing* (pp. 615-625). ACM.\n\n- Lingel, J., & boyd, d. (2013). “Keep
    it secret, keep it safe”: Information poverty, information norms, and stigma.
    *Journal of the American Society for Information Science and Technology, 64*(5),
    981-991.\n\n- Saha, K., Kim, S.C., Reddy, M.D., Carter, A.J., Sharma, E., Haimson,
    O., & Choudhury, M. 2019. The Language of LGBTQ+ Minority Stress Experiences on
    Social Media. Proc. *ACM Hum.-Comput. Interact. 3*, CSCW, Article 89 (November
    2019), 22 pages.\n\n- Scheuerman, M. K., Branham, S. M., & Hamidi, F. (2018).
    Safe spaces and safe places: Unpacking technology-mediated experiences of safety
    and harm with transgender people. *Proceedings of the ACM on Human-Computer Interaction,
    2*(CSCW), 155.\n\n- Seering, J., Wang, T., Yoon, J., & Kaufman, G. (2019). Moderator
    engagement and community development in the age of algorithms.* New Media & Society*,
    1461444818821316.\n"
  alt_date: !ruby/object:DateTime 2020-11-17 00:00:00.000000000 Z
  author:
  - sys:
      id: 55lvpa6OODrYw65JImLBDO
      created_at: !ruby/object:DateTime 2020-03-24 00:44:57.806000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:04:55.247000000 Z
      content_type_id: contributor
      revision: 3
    name: Kat Lo
    staff: true
    role: Product Strategy Researcher<br>Content Moderation Lead
    location: Los Angeles
    bio: |-
      Kat Lo serves as Content Moderation Lead and a product researcher at Meedan. Drawing from more than ten years of experience running online communities, Kat works with researchers, industry professionals, and community leaders to develop resources for fostering healthy social environments in games and social media. She has spent the past 5 years working with Intel, Microsoft Research, Instagram, Southern Poverty Law Center, AnyKey, and other organizations to study issues around diversity, inclusion and trust & safety. She has won fellowships from the National Science Foundation, Google, and the UC Berkeley Center for Technology, Society & Policy for her research on diversity and inclusion in technology.

      Kat is currently an industry consultant and researcher at the University of California Irvine, where she studies moderation practices in online spaces. Drawing from methods across user experience research and data science, her work focuses on the development of tools and community management best practices to mitigate toxic online environments. She advises non-profit organizations in mental health and online harassment, and assists in crisis support for targets of online harassment.
  publication:
  - sys:
      id: 1EPFTrNepDeTMUR1FbT1LV
      created_at: !ruby/object:DateTime 2020-11-09 15:59:09.591000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:06:32.246000000 Z
      content_type_id: publication
      revision: 1
    title: Content Moderation Toolkit
    short_title: Content Moderation Toolkit
    description: Content moderation is a process, not a solution in and of itself.
      With the increasing complexity of content on the internet and norms around governance,
      new standards are needed to help bridge the gap between industry and those seeking
      to engage with industry. With support from the Swedish International Development
      Agency and the Facebook Journalism Project, Meedan presents this work toward
      a common vocabulary and methodology for content moderation as a toolkit for
      civil society and platforms alike grappling with these issues.
    slug: content-moderation-toolkit
    sections:
    - sys:
        id: 2ldDGlFH61yvNc2x9DF2ex
    author:
    - sys:
        id: 55lvpa6OODrYw65JImLBDO
        created_at: !ruby/object:DateTime 2020-03-24 00:44:57.806000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:55.247000000 Z
        content_type_id: contributor
        revision: 3
      name: Kat Lo
      staff: true
      role: Product Strategy Researcher<br>Content Moderation Lead
      location: Los Angeles
      bio: |-
        Kat Lo serves as Content Moderation Lead and a product researcher at Meedan. Drawing from more than ten years of experience running online communities, Kat works with researchers, industry professionals, and community leaders to develop resources for fostering healthy social environments in games and social media. She has spent the past 5 years working with Intel, Microsoft Research, Instagram, Southern Poverty Law Center, AnyKey, and other organizations to study issues around diversity, inclusion and trust & safety. She has won fellowships from the National Science Foundation, Google, and the UC Berkeley Center for Technology, Society & Policy for her research on diversity and inclusion in technology.

        Kat is currently an industry consultant and researcher at the University of California Irvine, where she studies moderation practices in online spaces. Drawing from methods across user experience research and data science, her work focuses on the development of tools and community management best practices to mitigate toxic online environments. She advises non-profit organizations in mental health and online harassment, and assists in crisis support for targets of online harassment.
    additional_attribution: Lead, Meedan Content Moderation Project
    cover_image:
      sys:
        id: 6VYAFLYRLJnOVMBZSy1KD2
        created_at: !ruby/object:DateTime 2020-11-09 16:18:18.284000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:49.395000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/6VYAFLYRLJnOVMBZSy1KD2/4e40e178f196d8b024f0964ac95051e9/content-moderation-toolkit_cover-image.png"
    publication_promo_graphic:
      sys:
        id: 1DzzJ94lvtgdJnCaYLRegg
        created_at: !ruby/object:DateTime 2020-11-09 16:22:38.136000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:45.742000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/1DzzJ94lvtgdJnCaYLRegg/c3690f778b91576bebf6d62b5a14091f/content-moderation-toolkit_publication-promo-graphic.png"
    social_card_image:
      sys:
        id: 2biBznblY8VQEDW3NaheSe
        created_at: !ruby/object:DateTime 2020-11-17 19:26:39.227000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:41.047000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/2biBznblY8VQEDW3NaheSe/abdd8d11f56fd989a34d6d623428bbf3/Content-moderation-toolkit_executive-summary_social-card.png"
    publication_promo: This report is part of the [Content Moderation Toolkit](https://meedan-dev.netlify.app/reports/content-moderation-toolkit/)
    color:
      sys:
        id: sw3ntNjhkQRzLXqLpNM2m
        created_at: !ruby/object:DateTime 2020-11-09 16:06:32.559000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:35.341000000 Z
        content_type_id: color
        revision: 1
      name: Magenta
      value: "#ff4d7e"
      thumbnail:
        sys:
          id: ewZMwgt0xilyUB6umRuQs
          created_at: !ruby/object:DateTime 2020-11-09 16:07:07.170000000 Z
          updated_at: !ruby/object:DateTime 2020-11-18 20:04:32.936000000 Z
        title: Magenta
        description: "#ff4d7e"
        url: "//images.ctfassets.net/g118h5yoccvd/ewZMwgt0xilyUB6umRuQs/68b8db4246a336ad25a4c7658c0fafd8/magenta-01.png"
      text: "#262626"
  color:
    sys:
      id: 1BeoyOjqllQNlOfc4Zoyzv
      created_at: !ruby/object:DateTime 2020-11-09 20:11:09.041000000 Z
      updated_at: !ruby/object:DateTime 2020-11-09 20:12:54.048000000 Z
      content_type_id: color
      revision: 0
    name: Coral
    value: "#ff864d"
    thumbnail:
      sys:
        id: 1WuMP07zX7YYge7hfueHxK
        created_at: !ruby/object:DateTime 2020-11-09 20:12:47.974000000 Z
        updated_at: !ruby/object:DateTime 2020-11-09 20:13:00.400000000 Z
      title: Coral
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/1WuMP07zX7YYge7hfueHxK/da948838120c932d430a071427a7aa2b/coral-01.png"
  image: https://images.ctfassets.net/g118h5yoccvd/2KuL0ZHoxLnatKNzrugbpD/730a179c111581e4a85ddc59443eb109/content-moderation-_-vulnerable-populations_LGBTQ-communities_social-card.png
- sys:
    id: 4u68N8i5XJdHR1idRXTjOu
    created_at: !ruby/object:DateTime 2020-11-09 20:26:26.449000000 Z
    updated_at: !ruby/object:DateTime 2020-11-18 20:06:08.979000000 Z
    content_type_id: report
    revision: 1
    space:
      sys:
        id: g118h5yoccvd
    type: Entry
    content_type:
      sys:
        id: report
    environment:
      sys:
        id: master
        type: Link
        linkType: Environment
    locale: en-US
  title: Executive Summary
  slug: executive-summary
  lead_image:
    sys:
      id: JypirkuzlbP67jl1FB2Sg
      created_at: !ruby/object:DateTime 2020-11-09 21:29:20.057000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:05:11.475000000 Z
    title: 'Content Moderation Toolkit: Executive Summary'
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/JypirkuzlbP67jl1FB2Sg/d08503deaaef1e5e56129ef61eb56e04/content-moderation-toolkit_executive-summary_lead-image.png"
  social_card_image:
    sys:
      id: 2biBznblY8VQEDW3NaheSe
      created_at: !ruby/object:DateTime 2020-11-17 19:26:39.227000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:04:41.047000000 Z
    title: Content Moderation Toolkit
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/2biBznblY8VQEDW3NaheSe/abdd8d11f56fd989a34d6d623428bbf3/Content-moderation-toolkit_executive-summary_social-card.png"
  introduction: 'In the current digital media ecosystem, content moderation has become
    central to conversations about managing violations of human rights on and through
    technology platforms. Solutions for effective and equitable content moderation
    cannot be led by any one sector but, at the same time, challenges in coordination
    among stakeholders and disciplines complicate efforts for effective change. '
  body: "In order to address these challenges, Meedan’s Content Moderation Project
    has developed a working group as an ongoing collaboration between stakeholders
    across industry, academia, policy, and civil society groups to develop clear pathways
    to responsible content moderation for technology platforms. Our goal is to standardize
    knowledge across sectors by developing vocabularies, taxonomies, and a body of
    research to enable informed, actionable discussions between different disciplines,
    stakeholders, and the public.\n\n- Through this working group, Meedan’s work focused
    on three central themes: \n  - *pertinent content moderation challenges*, summarized
    into a literature review and stakeholder report;\n  - *content moderation in practice*,
    which culminated into an applied toolkit useful for civil society organizations;
    \n  - and *applied research* through the 2019 Housing and Population Census in
    Kenya, where frameworks for how to both standardize and contextualize responses
    to hate speech and harassment were developed and tested. \n\n## Outputs\n\nTo
    meet these objectives, we have developed four documents that contribute to improved
    research and practice at the intersection of product development, resource development,
    content moderation, and hate speech. \n\n### 1. A report covering our requirements
    gathering research on the state of content moderation issues identified by different
    affected stakeholders. \n\nThis research paid particular attention to the needs
    of industry as they intersect with those of other stakeholder groups, with an
    eye toward facilitating more productive and impactful conversations between industry,
    external organizations, and advocates raising issues and questions around content
    moderation practices and policies.\n\nFindings from this research demonstrated
    an absence of baseline expertise in content moderation (and its externalities)
    as a domain, inconsistent vocabulary within and between sectors, ill-defined benchmarks
    for accountability, and instability in the maintenance of working relationships
    across industry, civil society, regulatory bodies, and academia.\n\nAs a result,
    we emphasize the importance of the Content Moderation Toolkit serving as a bridge
    amongst industry, academia, and civil society, connecting policy decisions and
    conversations with practical experience from industry, building data collection
    and case study opportunities to inform practical applications of content moderation
    approaches that are standardized and scalable, but allow for contextualization
    across languages and regions. \n\nThis research enabled us to successfully implement
    an applied research initiative in Nairobi, Kenya, focused on the tools and processes
    needed to address content moderation challenges in multi-language and unique digital
    media ecosystems. \n\n### 2. A review of literature on topic-specific content
    moderation issues, summarizing important insights from the academic research field
    for industry stakeholders. Our findings are focused on two key thematic areas:
    \n\n**Hate Speech and Machine Learning**\nIn this section, we focus on the importance
    of and challenges associated with different definitions of hate speech, identify
    technical issues in the automated identification of hate speech, and examine the
    experience of hate speech moderation in practice. In each part, we summarize the
    challenges faced by platforms, academics, NGOs, and users in addressing hate speech,
    and propose opportunities to deepen cooperation.\n\n**Content Moderation & Vulnerable
    Populations: LGBTQ Communities**\nWe first examine the tension between the need
    for privacy and the need to connect with community online for LBGTQ people, and
    follow this with a discussion about the underlying anxieties and safety concerns
    that drive user wants and needs before turning to explore proposed design solutions.
    We explore this through three short example cases—Pride Month & Twitch, Streamers
    & Community Safety, and Fandom & Community Regulation—that serve to illustrate
    various points.\n\n### 3. [A content moderation toolkit for civil society](https://meedan-dev.netlify.app/reports/toolkit-for-civil-society-and-moderation-inventory/
    \"A content moderation toolkit for civil society\") that articulates a common
    vocabulary for stakeholders in civil society, academia, and policy to have meaningful,
    practical conversations with industry, through the development of a methodology
    for classifying moderation methods and an inventory of moderation tools.\n\nThe
    methodology for classifying moderation methods distinguishes components of the
    process that underlies a ‘content moderation decision,’ and defines critical properties
    (contexts, operators, and orientations) that characterize moderation product features.
    This toolkit inventory applies the above  methodology by cataloguing the main
    moderation product features, or tools, used to enforce content moderation decisions
    on social media platforms. \n\nIn order to enable discussions of moderation features
    across platforms, this inventory applies a proposed set of **classes of tools**
    that correspond to content moderation actions as they are understood and named
    by the public, and articulates variations of how platforms implement tools in
    these classes. \n\nThis toolkit and inventory aim to inform stakeholders outside
    of industry how to advocate for product or policy changes in an actionable language
    that is informed by industry practices. This includes having a more intricate,
    nuanced understanding of the different types of moderation and the types of actions
    they perform, and how these actions can connect to the change stakeholders desire
    to see. At the same time we aim to allow stakeholders to better understand the
    infrastructure/tooling needed to support the implementation and enforcement of
    those policies (or their goals/needs). \n\nUltimately, content moderation sits
    at the intersection of a balance between harms to individuals and harms to society
    overall, with no easy solutions. Our intent is for the vocabulary that emerges
    from this report to enable more robust discussions around the trade-offs inherent
    in these topics.\n\n### 4. A case study from the Kenyan Election and Census along
    with a draft set of indicators. \n\nIn August of 2019, the Meedan team, in collaboration
    with Article19 and the National Democratic Institute, held a coworking pop-up
    in Nairobi on the topics of hate speech, content moderation and disinformation,
    focused on collaboration and skill-sharing, with specific case examples through
    Kenya’s 2019 Population and Housing Census. The goal of this coworking space pop-up
    was to create a shared gathering area where CSOs could share ideas and collaborate,
    gain and share skills, and explore together how tools and resources can best support
    organizational work in addressing disinformation challenges during key large-scale
    events, including the Census, and through the implementation of the new digital
    identification system, Huduma Namba. \n\nDuring this event, participating Civil
    Society Organizations (CSOs) political activists and human rights groups used
    the space to conduct their work as usual, with opportunities to attend workshop
    sessions and presentations surrounding hate speech modalities in the Kenyan context,
    how memes intersected with hate speech through the 2019 Census, an efforts of
    various platforms to address and mitigate the impacts of harassment and hate speech
    online in the Kenyan digital media ecosystem. Kenyan thought leaders and organizations
    presented on their own work, including fact-checking organizations Pesa Check,
    Defy Hate Now and AfricaCheck, and conducted training workshops on applying Meedan’s
    Check tool for building standardized annotation efforts to address hate speech
    and harassment online. \n\nWe generated our initial insights from two pre-workshop
    surveys, participatory design activities, workshop outputs and informal semi-structured
    interviews conducted over the course of the week.\n\n## Key findings\n\n- **1.
    Social media global standards for content moderation do not represent how hate
    speech, misinformation, and disinformation manifest in the Kenyan context.**\nSocial
    media posts that had been reported and remained visible on social media platforms
    were reviewed by workshop participants; they identified many of these posts as
    false propaganda, overt hate speech, harassment, and incitement of violence. We
    developed a set of indicators (e.g., Incitement to violence) that mapped to a
    subset of global standards for social media content moderation policies. We translated
    these indicators into questions (e.g., Does the post use a call to action?) to
    annotate social media posts. Participants struggled to annotate content that expressed
    ethnic hate, violent misogyny, and anti-LGBTQI sentiment through contemporary
    cultural or political references on social media; some additionally noted a limited
    ability to articulate what the gaps in the indicators were. \n\n- **2. Standard
    definitions and precise cues to identify hate speech and disinformation are a
    critical step for enabling interoperability between CSO initiatives.**\nCivil
    society stakeholders in our workshops used different cues to determine whether
    an article or social media post contained hate speech, harassment, and disinformation.
    Throughout the workshops, participants collaboratively reconciled different Kenyan
    perspectives to work towards functional common definitions and regional cues that
    are able to represent diverse cultures within Kenya. Through the process of developing
    these definitions and cues, participants identified types of instances where social
    media global standards for content moderation fail in the Kenyan context, and
    organized around systematically investigating and documenting these gaps. \n\n-
    **3. Identifying interventions for hate speech, misinformation, and disinformation
    in Kenya requires ongoing collaboration with local stakeholders embedded in the
    Kenyan context.** \nAssessing the impact of social media posts and articles requires
    deep contextual knowledge of cultural discussion around current events, historical
    background, and how online social media impacts people offline. CSO stakeholders
    did not only have crucial knowledge for interpreting coded terms, political incentives,
    and practices of media production and consumption on social media platforms in
    Kenya, but extensive knowledge on the ways marginalized and vulnerable populations
    were disproportionately affected by hate speech, misinformation, and disinformation
    online and offline. \n"
  alt_date: !ruby/object:DateTime 2020-11-18 00:00:00.000000000 Z
  author:
  - sys:
      id: 55lvpa6OODrYw65JImLBDO
      created_at: !ruby/object:DateTime 2020-03-24 00:44:57.806000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:04:55.247000000 Z
      content_type_id: contributor
      revision: 3
    name: Kat Lo
    staff: true
    role: Product Strategy Researcher<br>Content Moderation Lead
    location: Los Angeles
    bio: |-
      Kat Lo serves as Content Moderation Lead and a product researcher at Meedan. Drawing from more than ten years of experience running online communities, Kat works with researchers, industry professionals, and community leaders to develop resources for fostering healthy social environments in games and social media. She has spent the past 5 years working with Intel, Microsoft Research, Instagram, Southern Poverty Law Center, AnyKey, and other organizations to study issues around diversity, inclusion and trust & safety. She has won fellowships from the National Science Foundation, Google, and the UC Berkeley Center for Technology, Society & Policy for her research on diversity and inclusion in technology.

      Kat is currently an industry consultant and researcher at the University of California Irvine, where she studies moderation practices in online spaces. Drawing from methods across user experience research and data science, her work focuses on the development of tools and community management best practices to mitigate toxic online environments. She advises non-profit organizations in mental health and online harassment, and assists in crisis support for targets of online harassment.
  publication:
  - sys:
      id: 1EPFTrNepDeTMUR1FbT1LV
      created_at: !ruby/object:DateTime 2020-11-09 15:59:09.591000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:06:32.246000000 Z
      content_type_id: publication
      revision: 1
    title: Content Moderation Toolkit
    short_title: Content Moderation Toolkit
    description: Content moderation is a process, not a solution in and of itself.
      With the increasing complexity of content on the internet and norms around governance,
      new standards are needed to help bridge the gap between industry and those seeking
      to engage with industry. With support from the Swedish International Development
      Agency and the Facebook Journalism Project, Meedan presents this work toward
      a common vocabulary and methodology for content moderation as a toolkit for
      civil society and platforms alike grappling with these issues.
    slug: content-moderation-toolkit
    sections:
    - sys:
        id: 2ldDGlFH61yvNc2x9DF2ex
    author:
    - sys:
        id: 55lvpa6OODrYw65JImLBDO
        created_at: !ruby/object:DateTime 2020-03-24 00:44:57.806000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:55.247000000 Z
        content_type_id: contributor
        revision: 3
      name: Kat Lo
      staff: true
      role: Product Strategy Researcher<br>Content Moderation Lead
      location: Los Angeles
      bio: |-
        Kat Lo serves as Content Moderation Lead and a product researcher at Meedan. Drawing from more than ten years of experience running online communities, Kat works with researchers, industry professionals, and community leaders to develop resources for fostering healthy social environments in games and social media. She has spent the past 5 years working with Intel, Microsoft Research, Instagram, Southern Poverty Law Center, AnyKey, and other organizations to study issues around diversity, inclusion and trust & safety. She has won fellowships from the National Science Foundation, Google, and the UC Berkeley Center for Technology, Society & Policy for her research on diversity and inclusion in technology.

        Kat is currently an industry consultant and researcher at the University of California Irvine, where she studies moderation practices in online spaces. Drawing from methods across user experience research and data science, her work focuses on the development of tools and community management best practices to mitigate toxic online environments. She advises non-profit organizations in mental health and online harassment, and assists in crisis support for targets of online harassment.
    additional_attribution: Lead, Meedan Content Moderation Project
    cover_image:
      sys:
        id: 6VYAFLYRLJnOVMBZSy1KD2
        created_at: !ruby/object:DateTime 2020-11-09 16:18:18.284000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:49.395000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/6VYAFLYRLJnOVMBZSy1KD2/4e40e178f196d8b024f0964ac95051e9/content-moderation-toolkit_cover-image.png"
    publication_promo_graphic:
      sys:
        id: 1DzzJ94lvtgdJnCaYLRegg
        created_at: !ruby/object:DateTime 2020-11-09 16:22:38.136000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:45.742000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/1DzzJ94lvtgdJnCaYLRegg/c3690f778b91576bebf6d62b5a14091f/content-moderation-toolkit_publication-promo-graphic.png"
    social_card_image:
      sys:
        id: 2biBznblY8VQEDW3NaheSe
        created_at: !ruby/object:DateTime 2020-11-17 19:26:39.227000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:41.047000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/2biBznblY8VQEDW3NaheSe/abdd8d11f56fd989a34d6d623428bbf3/Content-moderation-toolkit_executive-summary_social-card.png"
    publication_promo: This report is part of the [Content Moderation Toolkit](https://meedan-dev.netlify.app/reports/content-moderation-toolkit/)
    color:
      sys:
        id: sw3ntNjhkQRzLXqLpNM2m
        created_at: !ruby/object:DateTime 2020-11-09 16:06:32.559000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:35.341000000 Z
        content_type_id: color
        revision: 1
      name: Magenta
      value: "#ff4d7e"
      thumbnail:
        sys:
          id: ewZMwgt0xilyUB6umRuQs
          created_at: !ruby/object:DateTime 2020-11-09 16:07:07.170000000 Z
          updated_at: !ruby/object:DateTime 2020-11-18 20:04:32.936000000 Z
        title: Magenta
        description: "#ff4d7e"
        url: "//images.ctfassets.net/g118h5yoccvd/ewZMwgt0xilyUB6umRuQs/68b8db4246a336ad25a4c7658c0fafd8/magenta-01.png"
      text: "#262626"
  color:
    sys:
      id: sw3ntNjhkQRzLXqLpNM2m
      created_at: !ruby/object:DateTime 2020-11-09 16:06:32.559000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:04:35.341000000 Z
      content_type_id: color
      revision: 1
    name: Magenta
    value: "#ff4d7e"
    thumbnail:
      sys:
        id: ewZMwgt0xilyUB6umRuQs
        created_at: !ruby/object:DateTime 2020-11-09 16:07:07.170000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:32.936000000 Z
      title: Magenta
      description: "#ff4d7e"
      url: "//images.ctfassets.net/g118h5yoccvd/ewZMwgt0xilyUB6umRuQs/68b8db4246a336ad25a4c7658c0fafd8/magenta-01.png"
    text: "#262626"
  image: https://images.ctfassets.net/g118h5yoccvd/2biBznblY8VQEDW3NaheSe/abdd8d11f56fd989a34d6d623428bbf3/Content-moderation-toolkit_executive-summary_social-card.png
- sys:
    id: 5ZXTHAj2Ovrzx3PfcuiEtF
    created_at: !ruby/object:DateTime 2020-11-09 21:56:38.417000000 Z
    updated_at: !ruby/object:DateTime 2020-11-17 20:14:17.461000000 Z
    content_type_id: report
    revision: 0
    space:
      sys:
        id: g118h5yoccvd
    type: Entry
    content_type:
      sys:
        id: report
    environment:
      sys:
        id: master
        type: Link
        linkType: Environment
    locale: en-US
  title: Working Group and Literature Review Report
  slug: working-group-and-literature-review-report
  lead_image:
    sys:
      id: 1h296Opl5S4e3LWCjCp3Lj
      created_at: !ruby/object:DateTime 2020-11-09 22:00:54.302000000 Z
      updated_at: !ruby/object:DateTime 2020-11-17 19:50:58.209000000 Z
    title: Working Group and Literature Review Report
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/1h296Opl5S4e3LWCjCp3Lj/7e6b3c07020142f842ed1538a1e93a9b/content-moderation-toolkit_working-group-and-literature-review-report_lead-image.png"
  social_card_image:
    sys:
      id: 4EFqJXtvJSMQkzMcVINrNN
      created_at: !ruby/object:DateTime 2020-11-17 19:49:02.503000000 Z
      updated_at: !ruby/object:DateTime 2020-11-17 19:50:26.853000000 Z
    title: Working Group and Literature Review Report
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/4EFqJXtvJSMQkzMcVINrNN/835d8659984c28b6097a54512536b8cf/content-moderation-toolkit_working-group-and-literature-review-report_social-card.png"
  introduction: Advancements in content moderation on any platform strengthens the
    safety and wellbeing of people across the current ecosystem of platforms and beyond.
    We see an urgent need to develop consensus and shared collective knowledge among
    industry members and outside stakeholders to collectively identify and codify
    moderation solutions to work towards transparency and accountability of content
    moderation systems.
  body: "Regulatory response increasingly demands modes of transparency that do not
    sufficiently engage with the complexity of content moderation systems and subsequently
    do not produce desired outcomes in implementation. In order to move towards responsible
    accountability of content moderation decisions, there is a need to identify models
    of transparency and clarity for content decisions, in concert with stakeholders
    across industry and the public, that recognize the trade-offs and intricacies
    inherent in content moderation work.\n\nThe Content Moderation Project (CMP) seeks
    to enable dialogue between the multiple platforms engaged in content moderation
    and also enable dialogue and shared understandings between platforms, academics,
    and civil society organizations. The project focused on four key questions:\n\n1.
    How do we define content moderation?\n2. What are the barriers to transparency
    in moderation decisions? (e.g., proprietary information, obligations to investors,
    adversarial use, compromising user privacy, inconsistent definition of transparency
    outcomes)\n3. How can collaboration across public and industry stakeholders be
    improved?\n4. What are the professional standards of the field of content moderation?\n\nIn
    order to explore these questions, CMP established a working group of industry
    experts and academics. A key goal of the working group was to identify processes
    by which the industry can work with outside stakeholders to increase accountability
    in moderation decisions. The project has also sought to identify differences in
    terminology and create a shared vocabulary and consistent frameworks to assess
    and increase clarity about the operations of content moderation systems. Finally,
    CMP has strengthened public discourse and clarity around content moderation by
    fostering communication across policy-makers, media, academia, industry, and civil
    society that is grounded in the operational intricacies of content moderation
    enforcement.\n\n## The State of Content Moderation Issues: Requirements Gathering\n\nIn
    order to better meet the challenges of achieving transparency and accountability
    of content moderation decisions, the first phase of CMP was dedicated to requirements
    gathering with stakeholders across industry, civil society, regulators, and research
    institutions. The principal investigator (PI) conducted over 60 one-on-one interviews
    with these groups. She also attended meetings and events with product, policy
    and trust and safety teams from a range of social media companies, including Twitch,
    Facebook, Patreon, Mozilla, Reddit and Google, among others, representing a cross
    section of concerns in terms of both scale and content types. Further, she spoke
    at and attended key industry and academic conferences, including Mozilla Festival,
    the International Association of Privacy Professionals Content Moderation Workshop,
    the Association of Computational Linguistics Workshop on Abusive Language Online,
    and the ACM Conference on Computer-Supported Cooperative Work and Social Computing.
    \n\nThe research paid particular attention to the needs of industry as they intersect
    with those of other stakeholder groups, with an eye toward facilitating more productive
    and impactful conversations between industry, external organizations, and advocates
    raising issues and questions around content moderation practices and policies.
    Over the course of the conversation, we found that stakeholders had a range of
    varying concerns:\n\n### Industry\n\n- **Lack of a baseline expertise for content
    moderation as a singular discipline:** Disciplinary challenges arise in coordinating
    around solutions for content moderation; practitioners expressed difficulty in
    communicating between multiple industry members with differential knowledge or
    expertise without a common standard baseline expertise in content moderation.
    A substantial diversity of disciplinary roles engage with issues around platform
    content moderation, including user experience researchers, product designers,
    product managers, engineers, data scientists (with a range of specializations
    including experimental methods and machine learning), community managers, program
    managers, trust and safety strategists, policy specialists, and legal counsel.
    Industry members working in online moderation have begun organizing and standardizing
    this work under the umbrella term of “Trust & Safety.”\n- **Barriers in intra-organizational
    communication and conflicting priorities between teams:** Characteristics of content
    moderation work that practitioners identified as contributors to these challenges
    included (1) teams that work on content moderation or issues pertinent to content
    moderation frequently fall under different, but overlapping, purviews and associated
    names, including Trust & Safety, moderation, integrity, community operations,
    policy, legal, analytics, safety, health and (2) within and between social media
    companies, teams comprise distinct configurations of disciplinary roles, priorities,
    KPIs, and management structures.\n- **Unclear benchmarks for desired content moderation
    outcomes:** industry practitioners are currently unable to converge on stable,
    measurable outcomes due to variable and conflicting recommendations from different
    issue-driven civil society groups prioritizing their respective core missions.\n-
    **Limited outsider knowledge of industry practices:** CSOs, regulatory bodies,
    and academic institutions have insufficient knowledge of content moderation systems
    and platform policy, often making their recommendations, criticisms, and regulations
    impractical. Industry practitioners considered many external recommendations too
    broad to translate to product features or policy changes or unrealistic to implement
    in practice across many populations and use cases. These recommendations typically
    failed to engage with three prominent categories of knowledge: \n\t- The affordances
    and limitations of current technology in developing solutions to online harms–
    in many cases, external organizations overestimated the capability of algorithmic
    systems to detect harassment or hate speech.\n\t- Policy and design trade-offs
    inherent in decisions to prioritize certain outcomes or populations over others.
    One instance repeated almost uniformly across industry members was the negotiation
    of trade-offs between privacy and equitable participation online. In one high
    profile example, social media platforms are facing challenges in reducing the
    collection of user data and increasing encryption of user activities while minimizing
    the restrictions they impose on improving measures to monitor, verify, and moderate
    harmful activity on the platform.\n\t- Operational limitations of changing content
    moderation processes, re-training content moderators, and gathering contextual
    information, and the need for standardized processes and data to operate at scale
    and maximize consistency.\n\n- **Limited opportunities to translate research into
    actionable outcomes:** Tracking, evaluating, and translating both historical and
    contemporary research across a wide breadth of related fields makes it practically
    difficult to identify actionable product and policy implications. Industry practitioners
    frequently mentioned a need to conduct topic-specific research to satisfy urgent
    product needs (e.g., cyberbullying within certain populations, trauma mitigation,
    effects of witnessing self-harm on teenagers, pathways of radicalization, patterns
    of online disclosure in LGBTQ populations). Urgent timelines  and condensed roadmaps
    in trust and safety industry work, however, limit their time and resources for
    conducting sufficiently comprehensive reviews that are representative of the state
    of the relevant literature. Further constraints expressed were a need for specialized
    disciplinary training to interpret and curate the research, a lack of familiarity
    or patience with academic writing, and the absence of best practices for evaluating
    the generalizability of findings from different methodologies for product implications.\n\n###
    Civil Society Organizations (CSOs)\n- **Insufficient resources**: The demands
    of researching, verifying, archiving and mitigating harmful impacts of social
    media platforms require significantly more support. This includes the following
    key challenges:\n\n\t- __Limited funding sources:__ CSO stakeholders express anxiety
    about “limited oxygen” among organizations doing similar work (e.g., finite funding,
    partnerships, and publicity that organizations must compete for to continue operating).
    CSOs in the Global South reported knowing very few funding organizations in their
    country and identified social media companies as their most promising source of
    funding.\n\t- __Limited access to platform data:__ In the absence of access to
    detailed platform data, CSOs must pursue alternative methods to identify patterns
    of disinformation, misinformation, abuse, propaganda, and other sources of harm
    towards their target populations. These methods are frequently more resource-intensive
    and frequently yield fragmented or very local data with few options for validating
    that data. They may also require actions (e.g., scraping data from a platform)
    that violate platform terms of service, causing concerns among CSO stakeholders
    about putting existing or prospective relationships with the social media company
    at risk. \n\n- **Knowledge gaps with industry:** CSOs report having limited knowledge
    of content moderation processes, operations, technical challenges, enforcement
    standards, and factors in decision-making within a social media company.\n\n-
    **Difficulty establishing and maintaining relationships at social media companies
    with clear impact (and lacking agency in sustaining those relationships):**\n\t-
    CSO stakeholders expressed frustration with the difficulty of maintaining and
    further developing impactful relationships with platform representatives in the
    long term, describing precarious relationships with companies even after establishing
    contact, holding meetings, and working with that company on topic or region-specific
    issues. Some perceived a bottleneck in CSO access to social media companies as
    a result of a limited number or platform representatives dedicated to their region
    or issue. These concerns were strongly featured in discussions at events convening
    activists and CSOs, which included sharing strategies for establishing contact
    with effective employees at social media companies, finding employees to advocate
    for their organization within a company (the term “champion” was featured across
    multiple contexts), and how to recover from a champion leaving a social media
    platform.\n\t- Several advocates mentioned seeing higher impact from engaging
    directly with certain employees at social media companies over others. In particular,
    they emphasized the importance of communicating with product designers and engineers
    over policy representatives, who were often perceived as interventions by the
    company to manage public relations with external organizations. \n\n### Regulatory
    Bodies (Government and Private)\n\n- **Informing the public:** Development of
    an informed public can drive responsible and timely legislation towards platform
    accountability.\n\n- **Lack of clear definitions:** Grounding and refining inconsistent
    and vague definitions around key issues are necessary before instituting concrete,
    measurable, and enforceable outcomes through regulation. Professionals working
    in EU and US regulation highlighted Transparency, Accountability, Online harms,
    Radicalization, Harmful speech and Hate speech as key terms that were ill-defined
    for social media regulation.\n\n- **Anticipating and mitigating externalities
    of social media regulation:** This includes increasing risk of harms towards vulnerable
    populations, enabling the identification and suppression of dissidents by violent
    or authoritarian regimes, and obstructing the documentation of human rights violations
    captured through citizen journalism methods using social media platforms for dissemination.
    \n\n- **Knowledge transfer:** Validating and contextualizing metrics released
    in social media transparency reports supports organizations’ aims.\n\n### Research
    Institutions\n\n- **Difficulty in building integrated, sustained research partnerships
    with technology platforms.**\n\n- **Communicating the implications of their research:**
    This especially includes communications to industry practitioners and facilitating
    productive applications of their research findings.\n\n- **Data collection:**
    Collecting data becomes difficult with decreasing access to information from platform
    APIs and limited public datasets.\n\n- **Knowledge gaps with industry:** Researchers
    lack insight into the challenges of online content moderation due to opacity in
    how public platform policies (or community standards) are implemented in algorithmic
    systems, content moderator work, and platform design at social media companies.
    \n\nIn addition, we identified key adjacent public efforts, including the [Dangerous
    Speech Project](https://dangerousspeech.org \"Dangerous Speech Project\"), the
    [Change the Terms](https://www.changetheterms.org \"Change the Terms\") initiative,
    the [Trust and Safety Professional Association](https://www.tspa.info \"Trust
    and Safety Professional Association\"), the [Santa Clara Principles](https://www.santaclaraprinciples.org
    \"Santa Clara Principles\") working group, [Content Moderation at Scale](https://comoatscale.com
    \"Content Moderation at Scale\") series, and the ongoing development of experimental
    course on [Trust and Safety Engineering](https://online.stanford.edu/courses/cs152-trust-and-safety-engineering?courseId=93512631&method=load
    \"Trust and Safety Engineering\") taught by Alex Stamos at Stanford University.
    \n\nWe see our work at the Content Moderation Project as complementing these existing
    efforts and responding to the needs finding research in the following key ways:\n1.
    Serving as a bridge amongst industry, academia and civil society\n2. Connecting
    policy decisions and conversations with practical experience from industry\n3.
    Collecting data and developing case studies to inform this work\n\n## Description
    and Structure and Participants of Working Group\n\n### Academic Working Group
    Composition\n\nA key objective in convening an academic research working group
    is to meet the needs of industry practitioners who must balance investigating
    relevant internal and external research with the demanding, and often urgent,
    timelines of moderation product development. The working group reviewed a wide
    range of external research and extracted and summarized industry-relevant topics,
    implications, and language to aid practitioners in identifying actionable product,
    operations, and policy recommendations. Importantly, this reflects a working process
    by which the group can engage in further industry-impactful research around a
    variety of content moderation-related topics. The key elements of the literature
    reviews are being added to a searchable database for future reference, to help
    guide conversations and serve as a resource for other communities working on these
    topics.\n\nOver the course of the year, our academic research working group engaged
    in Slack conversations and one-on-one calls and small groups calls discussing
    key topics of interest. This group includes the following core participants, the
    academics of whom received a small stipend for their contributions. We ensured
    that this working group does not include affiliated individuals who: have been
    interviewed or contributed peripherally to background research and none of these
    individuals are participating on behalf of their organizations. \n\n1. Stevie
    Chancellor (Northwestern University)\n2. Brianna Dym (University of Colorado,
    Boulder)\n3. Scott Hale (Meedan/Oxford Internet Institute)\n4. Claudia Lo (Wikimedia
    Foundation)\n5. Grace Mutung’u (Kenya ICT Action Network)\n6. Fayika Nova (Marquette
    University)\n7. Priscilla Regina Da Silva (Institute for Technology and Society
    of Rio de Janeiro)\n8. Joseph Seering (Carnegie Mellon University)\n9. Principle
    Investigator: Katherine Lo (Meedan and UC Irvine)*\n\n### Advisors\n1. Sarah Roberts
    (UCLA Center for Critical Internet Inquiry)\n2. Alice Marwick (University of North
    Carolina at Chapel Hill, Data & Society)\n3. Casey Fiesler (University of Colorado
    Boulder)\n4. Amy Hasinoff (University of Colorado Denver)\n5. Sean Li (Discord)\n\n###
    Organizational Partners\n1. UCLA Center for Critical Internet Inquiry (C2I2) \n2.
    Article 19\n3. Lawyers Hub Kenya\n4. The International Association of Women in
    Radio & Television (IAWRT), Kenya\n\n## Literature Reviews\n\n### Hate Speech
    and Machine Learning: A Review of the Literature\n\nThis output tackles three
    major topics: (1) definitions of hate speech, (2) technical issues in the automated
    identification of hate speech, and (3) the experience of hate speech moderation
    in practice. In each part, there are clear challenges but also opportunities for
    platforms, academics, NGOs, and users to deepen cooperation. \n\nResearchers,
    civil society, and platforms have all defined hate speech in subtly different
    ways, which often leads to confusion about what is or is not hate speech. In particular,
    the definitions differ in the protected groups or characteristics they list. Most
    definitions specifically call attention to gender and race, but some include other
    characteristics such as “veteran status” or no characteristics at all. Agreeing
    on a common concept is a major, but often overlooked, element of effectively detecting
    and countering hate speech online.\n\nOnce a definition is chosen, the challenges
    shift towards the technical issues in detecting hate speech content. Among others,
    there are issues in operationalizing concepts (Is hate speech binary or continuous?),
    issues in obtaining data (Where does the data come from? Who has annotated it?),
    and issues in algorithmic implementation (What features? What algorithms?).\n\nIn
    practice, moderators are concerned about overly broad or narrow systems as well
    as a lack of transparency. These issues have different manifestations depending
    on the context (live-streaming or not) and who is doing the moderation (community
    members or employees/contractors of the platforms). This document only briefly
    touches upon this final issue, which is treated in-depth in a document of its
    own.\n\n### Content Moderation & Vulnerable Populations: LGBTQ Communities\n\nContent
    moderation and platform policy adversely affect marginalized groups of people
    on social media. For example, members of the LGBTQ community face challenges such
    as risks to privacy and personal safety depending on how a platform manages and
    displays their personal information. Furthermore, LGBTQ people face risks in seeking
    out resources related to their identity if they are in the closet or living and
    working in a hostile environment. When LGBTQ people encounter hate crimes online,
    evidence shows that they are not likely to report the crime. These findings are
    troubling, especially because online communities can often be the only space where
    LGBTQ people have access to a like-minded community, mentors, and resources specific
    to their identity. Keeping all of these challenges in mind, how do we design online
    spaces and make policy decisions in a way that protects our most vulnerable users?\n\nThis
    output seeks to shed light on this question by reviewing the findings of recent
    literature. The output first examples the tension between the need for privacy
    and the need to connect with community online for LBGTQ people. It then discusses
    underlying anxieties and safety concerns that drive user wants and needs before
    turning to explore proposed design solutions. Interwoven are three short example
    cases—Pride Month & Twitch, Streamers & Community Safety, and Fandom & Community
    Regulation—that serve to illustrate various points.\n"
  alt_date: !ruby/object:DateTime 2020-11-17 00:00:00.000000000 Z
  author:
  - sys:
      id: 55lvpa6OODrYw65JImLBDO
      created_at: !ruby/object:DateTime 2020-03-24 00:44:57.806000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:04:55.247000000 Z
      content_type_id: contributor
      revision: 3
    name: Kat Lo
    staff: true
    role: Product Strategy Researcher<br>Content Moderation Lead
    location: Los Angeles
    bio: |-
      Kat Lo serves as Content Moderation Lead and a product researcher at Meedan. Drawing from more than ten years of experience running online communities, Kat works with researchers, industry professionals, and community leaders to develop resources for fostering healthy social environments in games and social media. She has spent the past 5 years working with Intel, Microsoft Research, Instagram, Southern Poverty Law Center, AnyKey, and other organizations to study issues around diversity, inclusion and trust & safety. She has won fellowships from the National Science Foundation, Google, and the UC Berkeley Center for Technology, Society & Policy for her research on diversity and inclusion in technology.

      Kat is currently an industry consultant and researcher at the University of California Irvine, where she studies moderation practices in online spaces. Drawing from methods across user experience research and data science, her work focuses on the development of tools and community management best practices to mitigate toxic online environments. She advises non-profit organizations in mental health and online harassment, and assists in crisis support for targets of online harassment.
  publication:
  - sys:
      id: 1EPFTrNepDeTMUR1FbT1LV
      created_at: !ruby/object:DateTime 2020-11-09 15:59:09.591000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:06:32.246000000 Z
      content_type_id: publication
      revision: 1
    title: Content Moderation Toolkit
    short_title: Content Moderation Toolkit
    description: Content moderation is a process, not a solution in and of itself.
      With the increasing complexity of content on the internet and norms around governance,
      new standards are needed to help bridge the gap between industry and those seeking
      to engage with industry. With support from the Swedish International Development
      Agency and the Facebook Journalism Project, Meedan presents this work toward
      a common vocabulary and methodology for content moderation as a toolkit for
      civil society and platforms alike grappling with these issues.
    slug: content-moderation-toolkit
    sections:
    - sys:
        id: 2ldDGlFH61yvNc2x9DF2ex
    author:
    - sys:
        id: 55lvpa6OODrYw65JImLBDO
        created_at: !ruby/object:DateTime 2020-03-24 00:44:57.806000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:55.247000000 Z
        content_type_id: contributor
        revision: 3
      name: Kat Lo
      staff: true
      role: Product Strategy Researcher<br>Content Moderation Lead
      location: Los Angeles
      bio: |-
        Kat Lo serves as Content Moderation Lead and a product researcher at Meedan. Drawing from more than ten years of experience running online communities, Kat works with researchers, industry professionals, and community leaders to develop resources for fostering healthy social environments in games and social media. She has spent the past 5 years working with Intel, Microsoft Research, Instagram, Southern Poverty Law Center, AnyKey, and other organizations to study issues around diversity, inclusion and trust & safety. She has won fellowships from the National Science Foundation, Google, and the UC Berkeley Center for Technology, Society & Policy for her research on diversity and inclusion in technology.

        Kat is currently an industry consultant and researcher at the University of California Irvine, where she studies moderation practices in online spaces. Drawing from methods across user experience research and data science, her work focuses on the development of tools and community management best practices to mitigate toxic online environments. She advises non-profit organizations in mental health and online harassment, and assists in crisis support for targets of online harassment.
    additional_attribution: Lead, Meedan Content Moderation Project
    cover_image:
      sys:
        id: 6VYAFLYRLJnOVMBZSy1KD2
        created_at: !ruby/object:DateTime 2020-11-09 16:18:18.284000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:49.395000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/6VYAFLYRLJnOVMBZSy1KD2/4e40e178f196d8b024f0964ac95051e9/content-moderation-toolkit_cover-image.png"
    publication_promo_graphic:
      sys:
        id: 1DzzJ94lvtgdJnCaYLRegg
        created_at: !ruby/object:DateTime 2020-11-09 16:22:38.136000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:45.742000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/1DzzJ94lvtgdJnCaYLRegg/c3690f778b91576bebf6d62b5a14091f/content-moderation-toolkit_publication-promo-graphic.png"
    social_card_image:
      sys:
        id: 2biBznblY8VQEDW3NaheSe
        created_at: !ruby/object:DateTime 2020-11-17 19:26:39.227000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:41.047000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/2biBznblY8VQEDW3NaheSe/abdd8d11f56fd989a34d6d623428bbf3/Content-moderation-toolkit_executive-summary_social-card.png"
    publication_promo: This report is part of the [Content Moderation Toolkit](https://meedan-dev.netlify.app/reports/content-moderation-toolkit/)
    color:
      sys:
        id: sw3ntNjhkQRzLXqLpNM2m
        created_at: !ruby/object:DateTime 2020-11-09 16:06:32.559000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:35.341000000 Z
        content_type_id: color
        revision: 1
      name: Magenta
      value: "#ff4d7e"
      thumbnail:
        sys:
          id: ewZMwgt0xilyUB6umRuQs
          created_at: !ruby/object:DateTime 2020-11-09 16:07:07.170000000 Z
          updated_at: !ruby/object:DateTime 2020-11-18 20:04:32.936000000 Z
        title: Magenta
        description: "#ff4d7e"
        url: "//images.ctfassets.net/g118h5yoccvd/ewZMwgt0xilyUB6umRuQs/68b8db4246a336ad25a4c7658c0fafd8/magenta-01.png"
      text: "#262626"
  color:
    sys:
      id: 1dTLqYx4mJGbkisd6tJHvu
      created_at: !ruby/object:DateTime 2020-11-09 22:02:06.734000000 Z
      updated_at: !ruby/object:DateTime 2020-11-09 22:02:20.212000000 Z
      content_type_id: color
      revision: 0
    name: Texas Rose
    value: "#ffb34d"
    thumbnail:
      sys:
        id: 4lcFjuaSsFT8K6Pq2qm3pb
        created_at: !ruby/object:DateTime 2020-11-09 22:02:17.550000000 Z
        updated_at: !ruby/object:DateTime 2020-11-09 22:03:03.129000000 Z
      title: Texas Rose
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/4lcFjuaSsFT8K6Pq2qm3pb/d40d402f26f23af61ed9c46de7745e5d/texas_rose-01.png"
  image: https://images.ctfassets.net/g118h5yoccvd/4EFqJXtvJSMQkzMcVINrNN/835d8659984c28b6097a54512536b8cf/content-moderation-toolkit_working-group-and-literature-review-report_social-card.png
- sys:
    id: 6B6HKhC8nkzrzqV2UKI34
    created_at: !ruby/object:DateTime 2020-11-10 10:57:47.836000000 Z
    updated_at: !ruby/object:DateTime 2020-11-17 20:14:35.427000000 Z
    content_type_id: report
    revision: 0
    space:
      sys:
        id: g118h5yoccvd
    type: Entry
    content_type:
      sys:
        id: report
    environment:
      sys:
        id: master
        type: Link
        linkType: Environment
    locale: en-US
  title: 'Hate Speech and Machine Learning: A Review of the Literature'
  slug: hate-speech-and-machine-learning-a-review-of-the-literature
  lead_image:
    sys:
      id: 36RVlpSAcCIiLehrfeBCwW
      created_at: !ruby/object:DateTime 2020-11-10 11:02:47.401000000 Z
      updated_at: !ruby/object:DateTime 2020-11-17 19:55:44.685000000 Z
    title: 'Hate Speech and Machine Learning: A Review of the Literature'
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/36RVlpSAcCIiLehrfeBCwW/062f1e76ccd9ce7ea3e71fdad5fd3840/content-moderation-toolkit_hate-speech_lead-image.png"
  social_card_image:
    sys:
      id: 1Ne7vr9DQQebfWXlj69zXL
      created_at: !ruby/object:DateTime 2020-11-10 11:03:25.870000000 Z
      updated_at: !ruby/object:DateTime 2020-11-17 19:56:14.429000000 Z
    title: 'Hate Speech and Machine Learning: A Review of the Literature'
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/1Ne7vr9DQQebfWXlj69zXL/f0704bf859f2a7ae40ba8f645c6759a8/content-moderation-toolkit_hate-speech_social-card.png"
  note: 'Warning: This report contains offensive language.'
  introduction: 'This document tackles three major topics: (1) definitions of hate
    speech, (2) technical issues in the automated identification of hate speech, and
    (3) the experience of hate speech moderation in practice. In each part, there
    are clear challenges but also opportunities for platforms, academics, NGOs, and
    users to deepen cooperation. '
  body: "Researchers, civil society, and platforms have all defined hate speech in
    subtly different ways, which often leads to confusion about what is or is not
    hate speech. In particular, the definitions differ in the protected groups or
    characteristics they list. Most definitions specifically call attention to gender
    and race, but some include other characteristics such as “veteran status” or no
    characteristics at all. Agreeing on a common concept is a major, but often overlooked,
    element of effectively detecting and countering hate speech online.\n\nOnce a
    definition is chosen, the challenges shift towards the technical issues in detecting
    hate speech content. Among others, there are issues in operationalizing concepts
    (Is hate speech binary or continuous?), issues in obtaining data (Where does the
    data come from? Who has annotated it?), and issues in algorithmic implementation
    (What features? What algorithms?).\n\nIn practice, moderators are concerned about
    overly broad or narrow systems as well as a lack of transparency. These issues
    have different manifestations depending on the context (live-streaming or not)
    and who is doing the moderation (community members or employees/contractors of
    the platforms). This document only briefly touches upon this final issue, which
    is treated in-depth in a document of its own.\n\n## What is Hate Speech?\n\nThe
    definition of hate speech is a fundamental aspect of moderation, but one on which
    there is little legal or cultural agreement. The United States with its first
    amendment protections on freedom of speech represents one end of the spectrum.
    In contrast, other governments (including liberal democracies) have criminalized
    speech that attacks individuals on the basis of various characteristics. What
    these characteristics are vary from country to country and often depend heavily
    on context. Social media platforms operate, however, in a global environment and
    often seek policies that can be applied across multiple jurisdictions. This section
    discusses the ways researchers, civil society organizations, and platforms deal
    with the definitional question of what constitutes hate speech.\n\n### Researcher
    Perspectives\n\nIn this subsection we address how researchers define and operationalize
    hate speech for automated detection. In summary, it is not clear what is considered
    hate speech by researchers, which points to a general problem of the lack of common
    definitions/understandings. We illustrate this by providing definitions from papers.
    \n\nEarly work in this space broadly put hate speech under the umbrella of “abusive”
    content, cyberbullying, or harassment. A recent review by Vidgen et al., however,
    highlighted this lack of clarity in the task as a major impediment to forward
    progress. \n\nFor research that isolates out hate speech explicitly, one popular
    style of definition from a highly cited paper (Davidson et al.) describes hate
    speech as targeting a group and expressing hatred:\n\n> \"language that is used
    to express hatred towards a targeted group or is intended to be derogatory, to
    humiliate, or to insult the members of the group\".\n\nThis kind of definition
    is adopted by many papers when the group target is not the object of study. Most
    papers take the instigator -> target model for hate speech. In other words, a
    general definitions like this would be applied in learning representations of
    general versions of hate. \n\nIn another paper, Schmidt and Wiegand (German researchers)
    references an American dictionary for hate speech definitions:\n\n> \"Hate speech
    is commonly defined as any communication that disparages a person or a group on
    the basis of some characteristic such as race, color, ethnicity, gender, sexual
    orientation, nationality, religion, or other characteristic (Nockleby, 2000).
    Examples are (1)-(3).\n(1) Go fucking kill yourself and die already useless ugly
    pile of shit scumbag.\n(2) The Jew Faggot Behind The Financial Collapse\n(3) Hope
    one of those bitches falls over and breaks her leg\"\n\nWe find this definition
    pops up often when the author is focusing on hatred towards a specific group.
    The definition is often operationalized by providing examples, evoking the idea
    of “I know it when I see it”—a used in 1964 by United States Supreme Court Justice
    Potter Stewart to describe what constituted obscene materials unprotected by the
    First Amendment.\n\nIn terms of referencing domain sources or experts for definitions
    of hate speech, we only saw one definition articulated through alternative disciplinary
    perspectives. Waseem and Hovy define hate speech along inclusivity and intersectionality
    frameworks. \n\nThe vast majority of research group hatred in general and directed
    hate at a person as the same category. El Sherief et al., ICWSM 2018 (“Hate Lingo:
    A Target-Based Linguistic Analysis of Hate Speech in Social Media”) disambiguate
    this problem in their work:\n\n> \"We adopt the definition of hate speech along
    the same lines of prior literature (Hine et al. 2017; Davidson et al. 2017) and
    inspired by social networking community standards and hateful conduct policy (Facebook
    2016; Twitter 2016) as “direct and serious attacks on any protected category of
    people based on their race, ethnicity, national origin, religion, sex, gender,
    sexual orientation, disability or disease.\"\n\nThe authors then go onto distinguish
    between directed hate and generalized hate against a group broadly:\n\n> “We adopt
    the same typology and define the following in the context of hate speech: \n\n>
    Directed hate: hate language towards a specific individual or entity. An example
    is: “@usr4 your a f•cking queer f•gg•t b•tch”.\n\n> Generalized hate: hate language
    towards a general group of individuals who share a common protected characteristic,
    e.g., ethnicity or sexual orientation. An example is: “— was born a racist and
    — will die a racist! — will not rest until every worthless n•gger is rounded up
    and hung, n•ggers are the scum ofthe earth!! wPww WHITE America.”\n\nSome researchers
    refer to hate speech as a “crime,” confounding hate speech with hate crimes (which
    is culturally and legally situated construct that we’ll talk about below):\n\n>
    “Hate speech is a crime that has been growing in recent years…not only in face-to-face
    inter- actions but also in online communication” (Fortuna and Nunes)\n\nPulling
    some trends of these definitions together, we noticed several common themes that
    can be described in short as—hate speech definitions are a terrible application
    of \"you know it when you see it\".\n\n- Research often acknowledges that hate
    speech is an attack, but then does not clarify what attack or hatred actually
    is. Defining hate speech as “hatred” is tautological, and does not explain how
    the authors quantify harms/attack/hatred.\n- Technocentric positioning of some
    of these definitions without engagement of hate speech as an issue in other areas.
    Very rarely is work cited outside of computer science (CS) or a platform that
    has inspired the hate speech conversation\n- We also noticed a lack of cultural
    engagement with the meaning and concerns about hate speech—the protected categories
    of interest feels very US centric through US policies on hate speech are not the
    only ones influencing this research area.  \n- Interestingly, computer scientists
    do not often work with domain experts in this space or reference research from
    outside CS when establishing their definitions of hate speech.\n- No definitional
    management of context: There is a lack of distinguishing between profanity detection
    and other examples (most of the examples used are profanity/slurs against racial
    groups, but hateful content comes in more subtle forms.) It is worth noting that
    profanity or even racial slurs do not always constitute hateful content. For example,
    in some African American communities the N. word is used between two black individuals
    without a hateful connotation. In general, context is very important but often
    overlooked (connecting with the earlier point on lack of research about who is
    directing hateful speech at whom).\n\n### Civil Society/Advocacy Group Definitions
    of Hate Speech\n\nWe highlight three explanations from civil society organizations
    as examples although many other approaches exist. These organizations discuss
    the slippery nature of the definition of hate speech and legal limitations such
    as how third-party communication about a person does not usually constitute hate
    speech legally and the challenges with demonstrating intent.\n\n#### Pen America
    - Online Harassment Field Manual\n\n“Hateful speech: Hateful speech is a form
    of expression attacking a specific aspect of a person’s identity, such as one’s
    race, ethnicity, gender identity, religion, sexual orientation, or disability.
    Hateful speech online often takes the form of ad hominem attacks, which invoke
    prejudicial feelings over intellectual arguments in order to avoid discussion
    of the topic at hand by attacking a person’s character or attributes.”\n[Defining
    “Online Abuse”: A Glossary of Terms](https://onlineharassmentfieldmanual.pen.org/defining-online-harassment-a-glossary-of-terms/
    \"Defining “Online Abuse”: A Glossary of Terms\")\n\n#### Women’s Media Center
    Speech Project\n\n“Hate speech has no uniform legal definition. Online, this means
    that every social media platform has its own unique definition . As a baseline,
    however, hate speech is language or imagery that denigrates, insults, threatens,
    or targets and individual or groups of people on the basis of their identity –
    gender, based on race, color, religion, national origin, sexual orientation, disability,
    or other traits. There is no hate speech exception to the First Amendment. Hate
    speech usually has specific, discriminatory harms rooted in history and usually
    employs words, action and the use of images meant to deliberately shame, annoy,
    scare, embarrass, humiliate, denigrate, or threaten another person. Most legal
    definitions of harassment take into consideration the intent of the harasser.
    This, however, fails to translate usefully in the case of cyberharassment, the
    use of the Internet, electronic and mobile applications for these purposes. In
    the case of technology enabled harassment and abuse, intent can be difficult to
    prove and diffuse. For example, most laws do not currently consider third party
    communications to be harassing. So, whereas the law understands sending someone
    a threatening message for the purposes of extortion, it does not understand the
    non-consensual sharing of sexual images to someone other than the subject of the
    photograph illegal or hateful.” \n[Women's Media Centre, Speech Project, Online
    Abuse 101](https://www.womensmediacenter.com/speech-project/online-abuse-101#hateSpeech
    \"Women's Media Centre, Speech Project, Online Abuse 101\")\n\n#### Dangerous
    Speech: A Practical Guide\n\n“Most definitions specify that to be considered hate
    speech, messages must be directed at particular types of groups, such as people
    of the same religion, race, or ethnicity. Some definitions also add disability,
    sexual orientation, gender, sex, age culture, belief, or life stance. For example
    section 135a of Norway’s penal code defines hate speech as “threatening or insulting
    anyone, or inciting hatred or persecution of or contempt for anyone because of
    his or her a) skin color or national or ethnic origin, b) religion or life stance,
    or c) homosexuality, lifestyle or orientation” (Norwegian Penal Code). South Africa’s
    hate speech law is one of the most detailed and comprehensive, specifying groups
    and attributes that are absent from other countries’ laws such as pregnancy, marital
    status, conscience, language, color, and “any other group where discrimination
    based on that other ground (i) causes or perpetuates systemic disadvantage; (ii)
    undermines human dignity; or (iii) adversely affects the equal enjoyment of a
    person’s rights and freedoms in a serious manner that is comparable to discrimination
    […]” (Promotion of Equality, 2000, pp. 3-5). Most countries’ laws don’t prohibit
    hate speech at all, instead criminalizing other related forms of speech such as
    incitement to hatred.\n\nBroad or vague definitions of hate speech and related
    crimes can jeopardize freedom of speech, since vagueness allows for subjective
    application. *Indeed, laws against hate speech or hateful speech are often misused
    to punish and silence journalists, dissenters, and minorities, recently in countries
    as varied as Hungary, India, Rwanda, Kazakhstan, and Bahrain.*\n\nWe focus instead
    on Dangerous Speech since it is a narrower, more specific category, defined not
    by a subjective emotion such as hatred, but by its capacity to inspire a harm
    that is all too easy to identify – mass violence – and that almost everyone can
    agree on wanting to prevent.”\n[Dangerous Speech: A Practical guide](https://dangerousspeech.org/guide/
    \"Dangerous Speech: A Practical guide\")\n\n### Platform Perspectives\n\nMany,
    though not all, major social platforms define hate speech or content that may
    be associated with violent language or attacks on people based on protected characteristics.
    These are typically found in the community guidelines.\n\nOne unifying characteristic
    across these definitions that we found was a focus on two pieces:\n\n1. Targeted
    intention to harm, intimidate, or attack (an individual or a community)\n2. The
    harm is directed at a person or community’s protected group/characteristics\n\nAlmost
    all platforms prohibit targeting people directly or the groups that they belong
    to. This can fall under anti-harassment policies or under more specific policies.
    They often specifically call out attacks, spreading hatred or being hateful, and
    other directed attacks at people. It was not clear to us if/where these rules
    were applied and enforced on platforms: Do these rules apply in private conversations,
    like private messages? If a person says hateful things in PMs, would s/he get
    banned? There’s an implied public performance of these characteristics.\n\nTowards
    the 2nd point, platform guidelines place a stronger emphasis on protected groups
    or categories. They often will define what groups or identities are covered. We
    believe this may allow for better operationalizing when certain slurs and comments
    would be about hate speech.\n\n*Possibilities of protected category (extracted
    from the platform guidelines): race, gender identity, age, caste, ethnicity, national
    origin, religious affiliation, sexual orientation, sex, disease/disability status,
    immigrant status, veteran status, victims of major violent events and their kin,
    culture, political affiliation, physical condition*\n\nNo one platform protects
    all these categories. In some cases, these protected categories are listed, whereas
    in others, platforms say “acts of discrimination” as a broader umbrella for hateful
    behavior. \n\nFrom this, a few questions emerge:\n  - Why the selection of certain
    protected categories? Some appear to be related to legal precedents but others
    do not. How do platforms determine what protected categories appear in their definitions
    and what does not?\n  - How do platforms distinguish between the reclamation of
    terms by communities in automated systems and hate speech? For example, \n  -
    How do they operationalize across different cultural contexts, such as hateful
    terms for political protestors in certain regions, and emerging hateful terms
    (such as [the “ok” hand sign now being associated with white supremacy?](https://www.npr.org/2019/09/26/764728163/the-ok-hand-gesture-is-now-listed-as-a-symbol-of-hate
    \"the “ok” hand sign now being associated with white supremacy?\")) \n\nGlobal
    contexts complicate definitions of hate speech for platforms to cleanly adopt
    a policy. As Facebook stated in a blog post about the issue: “Although a number
    of countries have laws against hate speech, their definitions of it vary significantly.
    In Germany, for example, laws forbid incitement to hatred; you could find yourself
    the subject of a police raid if you post such content online. In the US, on the
    other hand, even the most vile kinds of speech are legally protected under the
    US Constitution. People who live in the same country — or next door — often have
    different levels of tolerance for speech about protected characteristics.” \n[Hard
    Questions: Who Should Decide What Is Hate Speech in an Online Global Community?](https://about.fb.com/news/2017/06/hard-questions-hate-speech/
    \"Hard Questions: Who Should Decide What Is Hate Speech in an Online Global Community?\")\n\nIt
    is also unclear whether platforms should operate in isolation or consider the
    wider societal context or actions of people on other platforms. Twitch notably
    writes that the platform, “[will now consider verifiable harassing conduct that
    takes place off-Twitch when making moderation decisions for actions that occur
    on Twitch](https://blog.twitch.tv/en/2018/02/08/twitch-community-guidelines-updates-f2e82d87ae58/
    \"Twitch Community Guidelines Updates\")\".\nThis is a statement that we did not
    see explicitly by other platforms.\n\n### Formal Platform Definitions\n\nIn addition
    to gathering details about academic definitions, we also went through major platforms
    and found their content policies on hate speech.\n\nWhen not available, we captured
    their content on problematic speech.\n\n#### Facebook\n\n“We do not allow hate
    speech on Facebook because it creates an environment of intimidation and exclusion
    and in some cases may promote real-world violence.\n\nWe define hate speech as
    a direct attack on people based on what we call protected characteristics — race,
    ethnicity, national origin, religious affiliation, sexual orientation, caste,
    sex, gender, gender identity, and serious disease or disability. We also provide
    some protections for immigration status. We define attack as violent or dehumanizing
    speech, statements of inferiority, or calls for exclusion or segregation. We separate
    attacks into three tiers of severity, as described below.\n\nSometimes people
    share content containing someone else’s hate speech for the purpose of raising
    awareness or educating others. In some cases, words or terms that might otherwise
    violate our standards are used self-referentially or in an empowering way. People
    sometimes express contempt in the context of a romantic break-up. Other times,
    they use gender-exclusive language to control membership in a health or positive
    support group, such as a breastfeeding group for women only. In all of these cases,
    we allow the content but expect people to clearly indicate their intent, which
    helps us better understand why they shared it. Where the intention is unclear,
    we may remove the content.\n\nWe allow humor and social commentary related to
    these topics. In addition, we believe that people are more responsible when they
    share this kind of commentary using their authentic identity.”\n\n([Facebook Blog
    post](https://newsroom.fb.com/news/2017/06/hard-questions-hate-speech/) about
    this defintion.)\n\n([Source on 8/9/19](https://www.facebook.com/communitystandards/objectionable_content))\n\n####
    Twitter\n“__[Hateful conduct](https://help.twitter.com/en/rules-and-policies/twitter-rules#hateful-conduct
    \"Hateful conduct\")__: You may not promote violence against or directly attack
    or threaten other people on the basis of race, ethnicity, national origin, sexual
    orientation, gender, gender identity, religious affiliation, age, disability,
    or serious disease.” \n\n([Twitter: Hateful conduct policy](https://help.twitter.com/en/rules-and-policies/hateful-conduct-policy
    \"Hateful conduct policy\") on 8/9/19)\n\n#### Tumblr\n“**Hate Speech**. Don't
    encourage violence or hatred. Don't post content for the purpose of promoting
    or inciting the hatred of, or dehumanizing, individuals or groups based on race,
    ethnic or national origin, religion, gender, gender identity, age, veteran status,
    sexual orientation, disability or disease. If you encounter content that violates
    our hate speech policies, please report it.\n\nKeep in mind that a post might
    be mean, tasteless, or offensive without necessarily encouraging violence or hatred.
    In cases like that, you can always block the person who made the post—or, if you're
    up for it, you can express your concerns to them directly, or use Tumblr to speak
    up, challenge ideas, raise awareness or generate discussion and debate.”\n\n([Tumbler:
    Community Guidelines](https://www.tumblr.com/policy/en/community \"Tumbler Community\")
    as of 8/9/19)\n\n#### Twitch\n\n“Hateful conduct is any content or activity that
    promotes, encourages, or facilitates discrimination, denigration, objectification,
    harassment, or violence based on the following characteristics, and is strictly
    prohibited: \n  - Race, ethnicity, or national origin\n  - Religion\n  - Sex,
    Gender, or Gender Identity\n  - Sexual Orientation\n  - Age\n  - Disability or
    Medical Condition\n  - Physical Characteristics\n  - Veteran Status\n\nTwitch
    will consider a number of factors to determine the intent and context of any reported
    hateful conduct. Hateful conduct is a zero-tolerance violation. We will take action
    on all accounts associated with such conduct with a range of enforcement actions,
    including and up to indefinite suspension.”\n\n([Twitch: Hateful Conduct and Harassment](https://www.twitch.tv/p/legal/community-guidelines/harassment/
    \"Twitch: Hateful Conduct and Harassment\"))\n\n([Blog post](https://blog.twitch.tv/en/2018/02/08/twitch-community-guidelines-updates-f2e82d87ae58/
    \"Twitch community guidelines\") to the Twitch community.)\n\n#### Reddit\n\nThe
    platform has no explicit mention of hate speech specifically, which isn’t surprising
    given Reddit’s ethos of open expression. There were two related concepts, however:\n\n*Content
    cannot incite violence:* “[Do not post content that encourages, glorifies, incites,
    or calls for violence or physical harm against an individual or a group of people](https://www.reddithelp.com/hc/en-us/articles/360043513151
    \"Do not post violent content\")”\n\n*Content cannot harass other people:*\n“We
    do not tolerate the harassment of people on our site, nor do we tolerate communities
    dedicated to fostering harassing behavior.\n\nHarassment on Reddit is defined
    as systematic and/or continued actions to torment or demean someone in a way that
    would make a reasonable person conclude that Reddit is not a safe platform to
    express their ideas or participate in the conversation, or fear for their safety
    or the safety of those around them.\n\nBeing annoying, vote brigading, or participating
    in a heated argument is not harassment, but following an individual or group of
    users, online or off, to the point where they no longer feel that it's safe to
    post online or are in fear of their real life safety is.”\n\n([Reddit: Do not
    threaten, harass, or bully](https://www.reddithelp.com/hc/en-us/articles/360043071072
    \"Do not threaten, harass, or bully\") as of 8/9/19) \n\n#### YouTube\n\n “Hate
    speech is not allowed on YouTube. We remove content promoting violence or hatred
    against individuals or groups based on any of the following attributes:\n\n Age;
    Caste; Disability; Ethnicity; Gender Identity; Nationality; Race; Immigration
    Status; Religion; Sex/Gender; Sexual Orientation; Victims of a major violent event
    and their kin; Veteran Status”\n\n([YouTube: Hate Speech Policy](https://support.google.com/youtube/answer/2801939?hl=en
    \"Hate speech policy\") on 8/9/19)\n\n#### Wikimedia\n\n“Certain activities, whether
    legal or illegal, may be harmful to other users and violate our rules, and some
    activities may also subject you to liability. Therefore, for your own protection
    and for that of other users, you may not engage in such activities on our sites.
    These activities include:\n\n  - Harassing and Abusing Others \n  - Engaging in
    harassment, threats, stalking, spamming, or vandalism; and Transmitting chain
    mail, junk mail, or spam to other users.”\n  - \n([Wikimedia: Refraining from
    Certain Activities](https://foundation.wikimedia.org/wiki/Terms_of_Use/en#4._Refraining_from_Certain_Activities
    \"https://foundation.wikimedia.org/wiki/Terms_of_Use/en#4._Refraining_from_Certain_Activities\"))
    \n\n#### Instagram \n\n“We want to foster a positive, diverse community. We remove
    content that contains credible threats or hate speech, content that targets private
    individuals to degrade or shame them, personal information meant to blackmail
    or harass someone, and repeated unwanted messages. We do generally allow stronger
    conversation around people who are featured in the news or have a large public
    audience due to their profession or chosen activities.\n\nIt's never OK to encourage
    violence or attack anyone based on their race, ethnicity, national origin, sex,
    gender, gender identity, sexual orientation, religious affiliation, disabilities,
    or diseases. When hate speech is being shared to challenge it or to raise awareness,
    we may allow it. In those instances, we ask that you express your intent clearly.”\n\n([Instagram:
    Community Guidelines](https://help.instagram.com/477434105621119 \"Community Guidelines\")
    on 8/9/19) \n\n#### Pinterest \n\n“Content attacking vulnerable groups based on
    their race, ethnicity, national origin, religion, sex, gender, sexual orientation,
    age, disability, or medical condition, among others, is not allowed on Pinterest.\n\nUnkind
    language targeted at governments, public figures, or other groups does not violate
    our policies.\n\nIf you see hate speech on Pinterest, report it to us. If you
    see a Pin or comment indicating a real threat of violence, let us know. If necessary,
    contact your local authorities.”\n\n([Pinterest help](https://help.pinterest.com/en/article/report-hate-speech
    \"Pinterest help\"))\n\n#### TikTok\n\n“TikTok is an inclusive community. It is
    not ok to attack or incite violence against other users.\n\n  - DO NOT post, share,
    or send any content that incites hatred against a group of people based on their
    race, ethnicity, religion, nationality, culture, disability, sexual orientation,
    gender, gender identity, age, or any other discrimination.\n  - DO NOT post, share,
    or send contents that may trigger hostility, including trolling or provocative
    remarks.”\n\n([TikTok: Community Guidelines](https://www.tiktok.com/community-guidelines?lang=en
    \"Community Guidelines\"))\n\n#### LinkedIn\n\n“We do not allow hate speech acts
    like attacking people because of their race, ethnicity, national origin, gender,
    sexual orientation, political or religious affiliations, or medical or physical
    condition. Also, hate groups, terrorists, and those who engage in violent crimes
    are not welcome and not permitted on the Services.”\n\n([LinkedIn help](https://www.linkedin.com/help/linkedin/answer/89880
    \"LinkedIn Professional Community Policies\"))\n\n#### WhatsApp \n\nIn the ToS,
    not a community guideline...\n\n“You will not use (or assist others in using)
    our Services in ways that:...(b) are illegal, obscene, defamatory, threatening,
    intimidating, harassing, hateful, racially, or ethnically offensive, or instigate
    or encourage conduct that would be illegal, or otherwise inappropriate, including
    promoting violent crimes”\n\n([Whats App: Terms of service](https://www.whatsapp.com/legal/terms-of-service-eea
    \"Whats App: terms of service\"))\n\n#### WeChat\n\n“We are not responsible for
    and we do not endorse, support or guarantee the lawfulness, accuracy or reliability
    of any content submitted to, transmitted or displayed by or linked by WeChat,
    including content provided by users of WeChat or by our advertisers. You acknowledge
    and agree that by using WeChat, you may be exposed to content which is inaccurate,
    misleading, defamatory, offensive or unlawful. Any reliance on or use of any content
    on or accessible from WeChat by you is at your own risk. Your use of WeChat does
    not give you any rights in or to any content you may access or obtain in connection
    with your use of WeChat.”\n\n“We may review (but make no commitment to review)
    content (including any content posted by WeChat users) or third party programs
    or services made available through WeChat to determine whether or not they comply
    with our policies, applicable laws and regulations or are otherwise objectionable.
    We may remove or refuse to make available or link to certain content or third
    party programs or services if they infringe intellectual property rights, are
    obscene, defamatory or abusive, violate any rights or pose any risk to the security
    or performance of WeChat.”\n\n([WeChat: Service terms](https://www.wechat.com/en/service_terms.html
    \"WeChat: Service terms\"))\n\n#### Viber\n\n“Loving Vibes Prevail - We defend
    a person’s right to express unpopular points of view, but we do not allow speech
    which attacks or demeans a group based on ethnic or national origin, race, religion,
    disability, gender, gender identity, age or sexual orientation.”\n\n([Viber: Public
    content policy](https://www.viber.com/en/terms/viber-public-content-policy/ \"Viber:
    Public content policy\")) \n\n#### Snapchat\n\n“Hate Speech & False Information:
    Don't post any content that demeans, defames, or promotes discrimination or violence
    on the basis of race, ethnicity, national origin, religion, sexual orientation,
    gender identity, disability, or veteran status.”\n\n([Snapchat: Community guidelines](https://www.snap.com/en-US/community-guidelines/
    \"Snapchat: Community guidelines\"))\n\n## Technical Issues in Automated Identification
    of Hate Speech\n\nMost automated hate speech detection is being accomplished with
    supervised machine learning (ML) techniques. Simply put, ML builds a statistical
    model for a specific task or goal that does not use explicit instructions or guidelines.
    Rather, it learns the model from patterns and inferences in the data it sees.\n\nIt
    is important to understand the ML piece behind this, because this transformation
    of hate speech -> data -> model -> outcome dramatically impacts how hate speech
    will be understood by an algorithm. \n\nML essentially learns patterns from example/annotated
    data. so, the ML cannot detect things that are very different from all examples
    it has seen.\n\n### Annotation and Data Issues\n\n__Because ML learns its patterns
    from the data, ML cannot detect things that are very different from all examples
    it has seen.__\n\n__This means the quality and performance of these algorithms
    is directly impacted by who is annotating, their personal contexts, and their
    own higher-order conceptions of hate speech definitions.__ Data quality and the
    annotations that make up hate speech detection are crucial in defining the problem
    of hate speech—data scientists and machine learning researchers will sarcastically
    reflect, “garbage in, garbage out”. This section refers to the major part of “garbage/data
    in,” which is the quality and procedures for annotating data. \n\nIssues in hate
    speech detection around operationalizing these definitions relate to issues of
    *scientific validity*. Validity is a concept taken from the experimental and causal
    literature that describes how scientists translate concepts into pragmatic implementation
    and the evaluation of their reliability, accuracy, and generalizability (drawn
    from Shadish, Cook, and Cambell, 2002).\n\nThere are many kinds of validity that
    are important within the spectrum of understanding hate speech, but here, we will
    focus on __construct validity__, or how effectively and accurately have you translated
    a high-level meta concept to a pragmatic implementation (e.g. *measuring depression
    with a specific screening technology*). First, we’ll talk about coming to consensus
    on the meta-concept of hate speech, then we’ll talk about operationalizing that
    to specific platforms and contexts. \n\n#### Meta-level Validity Problems\n\nAs
    demonstrated by the definitional debates in the previous section, there is no
    grand and similar meta-level concept of hate speech amongst platforms, researchers,
    and we would argue for society at large. Individuals, groups, and communities
    don’t consistently agree on what hate speech is, even when guided with specific
    instructions or something like a platform policy to interpret and apply. \n\n*Example:
    The term “gypsy” and its status as a slur is complex, because in the US, it is
    not commonly seen as an ethic slur and may refer to wanderers or travelers who
    lack homes. The term is romanticized through popular culture and music (see Stevie
    Nick’s song “Gypsy”). However, in other parts of the world (especially Europe),
    the cultural context of the term is embedded with centuries of discrimination
    and rights violations against the Romani people, where “gypsy” is often used as
    a slur against this ethic group.*\n\nSo, it can be hard to operationalize this
    concept in part because the definitional aspects of the meta-concept are nebulous—in
    many ways, operationalizing this part of the process is a “wicked problem,” i.e.
    a social or cultural problem that is impossible to solve because of incomplete,
    contradictory, and changing requirements that are often difficult to recognize
    (Tonkinwise, 2015).\n\n#### Issues in Translating Meta-Concepts to Pragmatic Definitions\n\n__How
    do you teach someone to annotate for hate speech?__ It’s important to consider
    how the annotation dataset is assembled, by who, where the annotation come from,
    and when they were provided. Even if we had a well-defined meta concept of hate
    speech, how could we operationalize that into labels of content that may contain
    hate speech or not? \n\nThere are many potential sources of data, and datasets
    may be compiled of previously moderated content, hand-identified hate speech examples
    by moderators or project engineers, and/or data scientists. \n\nHere are some
    notes on specific sources of data:\n\n- Historically moderated examples: This
    is a really common source of data, since it is the closest thing to ground truth
    most platforms have. Much of this data comes from historically moderated content
    that was reported to the platform and then identified as hate speech. If actually
    preserved on the platform, there is often lots of this data. One important aspect
    to note of this data is that it is biased towards content that would be reported
    to platforms—content that is NOT reported would likely not be seen by mods (assuming
    you’re starting from scratch). This means that in communities that encourage hateful
    speech/endorse it, this behavior would be normative and therefore not reported.
    \n- Internal engineers and team members: Good to have because they have an acute
    awareness of the problems/technical limitations. Can bias algorithm to technical
    implementation details since they are often of algorithm “sticking points”\n-
    Hate speech experts (outside CS): good balance of knowledge and technical expertise.
    May lack visibility into platform methods of operationalizing this. \n- Crowdworkers/lay
    people annotation: Distributed and relatively inexpensive. \n- Data from another
    platform (using annotated hate speech on Facebook to predict hate speech on Twitter):
    Generally suffers from poor performance because it doesn’t match the content form/userbase/behaviors
    on the new platform\n\n#### Pragmatic Implications of Annotations\n\nEven if annotations
    were easy to accomplish and could be perfectly aligned with definitions and concepts
    around hate speech, there are pragmatic limitations of getting more data for teams
    to use to identify hate speech.\n\n*Costs:* One is cost. It is expensive to source
    different data, labeled by experts. This is both measured in dollar costs as well
    as time costs. Having data annotated by people is expensive, and teams with limited
    resources or tight deadlines may not practically be able to source this kind of
    information.\n\nOne of the authors has run into the problem personally in her
    work on mental health status identification on social media data - even if we
    had the money to hire trained clinical psychologists, they are very busy people
    and can only annotate data so quickly.\n\n*Cultural Context and Geography:* Cultural
    context and the political realities of individuals can change whether or not something
    is considered hate speech. This is influenced by legal restrictions as well as
    cultural perceptions of what is considered hateful, as well as temporal standards
    for appropriate conduct as language use evolves over time. Recall the example
    of the term “gypsy” from earlier, which is deeply rooted in cultural and geographic
    constraints.\n\n### Model Updates and Changing Perspectives/Topic Drift\n\nIt
    is a mistake to assume a static and unchanging relationship between independent
    and dependent variables for machine learning. Concept or topic drift is a known
    problem within ML models, where the data coming into the model changes over time,
    and therefore, the model performance will change and likely degrade over time.\n\nOn
    social platforms, concept drift can be caused by many different factors:\n\n  -
    Natural language drift\n  - Behavior drifts\n  - Exogenous events (like crises,
    platform instability, etc)\n  - Political events that change national conversations\n
    \ - Changes in cultural standards for slurs and offensive language\n  - Changes
    in platform affordances\n  - Changes in platform policy and enforcement mechanisms\n
    \ - Responses to algorithmic systems (changing language behaviors to avoid detection)\n\nFor
    hate speech detection, all of these factors can cause models to drift away from
    the conceptualized/meta-definition of hate speech. This means that a model to
    detect hate speech will need to be updated.\n\nBecause of concept drift, there
    are some important factors to consider:\n\n- How can concept drift be detected?\n\t-
    Performance measures against newer data\n\t- Dissatisfaction of moderation decisions
    with a model\n\t- Poor performance of the model in automatic triaging/more posts
    having to be adjusted or disputed with human moderators\n- How often do you update
    the model to reflect against this drift? Pragmatics come into play here, because
    updating a model requires new data, engineering cycles, and some kind of evaluation
    for testing.\n- Should models reflect the sentiments of current political trends
    or adhere to a larger concept of drift?\n- Should new models be deployed with
    or without “beta” testing, to be reactive to issues in automated identification?
    \n\n### Feature engineering\n\nFeature engineering/identifying relevant independent
    variables is an essential component of machine learning development and insights.
    Feature engineering bounds the problem detection space of what is an appropriate
    signal, and instructs the algorithm on the insights it may draw from the data
    to make its inferences. \n\nIn the research, we see researchers grappling with
    how to determine appropriate features based on the definitions of hate speech.
    Most of the literature uses linguistic features (such as LIWC (Linguistic Inquiry
    and Word Count) or sentiment, or detection of a dictionary of “hateful” words
    to identify hate speech). These can get as complex as neural network representations
    of language or other behavioral signals.\n\nHowever, as we’ve noted before in
    the annotation section, hate speech is inherently contextual, especially when
    considering targeted hate speech towards someone (their protected categories matter
    in these scenarios, well at least it’s not clear if they should matter based on
    the difficulty of the definitions above). \n\n*Example: take the example of “bitch”
    - describing a woman as a bitch is very different than describing a man as a bitch,
    and one may be more likely to be hate speech. However, without the context of
    the situation, it is hard to know if “bitch” is universally a hateful term or
    if it has situational aspect of emasculating men, describing aggressive women,
    or being used in other contexts.*\n\nThis gets at a fundamental question related
    to operationalizing hate speech for developing features—__should feature engineering
    to detect hate speech be based on just the offending post/comment itself or be
    made aware of the surrounding context of hate speech?__ Said another way, what
    is hate speech without being aware of who is speaking it, who the targets are
    (if there are any), and the cultural issues? \n\nMost research does not grapple
    with this problem of targets and generalized hate speech. An exception is El Sherief
    et al. (2018) who focus on two kinds of hate speech (directed and generalized)
    to get at some of this contextual awareness. In contrast, most approaches in the
    research don’t really get at more than that.\n\nRelated to this is an interesting
    discussion around using demographic information, specifically that of protected
    groups, to determine when hate speech is occuring. In other fields of applied
    ML, researchers have derived demographic information like age, race, and gender
    from text to use as features for predictive models. This derivation can either
    be from form-based information, like someone’s self-stated age or gender, or be
    computationally inferred. This could be inferred from the perpetrator of the hate
    speech or perhaps the target to provide additional algorithmic context; however,
    accuracy rates for inferred demographics will never be 100% and may erase non-standard
    identities (male and female gender detection erases non-binary folks).\n\n###
    Machine Learning Applications\n\n#### Classification, Regression, or Something
    Else?\n\nIn the research literature, most often, hate speech detection is represented
    as a binary classification problem, where the categories of prediction for the
    algorithm are positively identified hate speech, or no hate speech. The mandates
    that there is a supervised setup that uses labeled data to train a classification
    problem, most of the time stated as “Is hate speech present or not?”. \n\nWhile
    it is computationally possible to treat hate speech as a continuous variable (i.e.,
    in line with the intuition that some content is more hateful/offensive than other
    content) this rarely happens in practice. Doing so would require annotators to
    indicate how hateful a piece of content is and is often seen as an unnecessary
    complication or a judgement that cannot be standardized. Moving away from a binary
    approach also requires new training data as well as the use of different algorithms.
    State-of-the-art approaches often uses neural networks, but neural networks are
    often overly confident in their estimations and, as such, their confidence values
    cannot be used as a proxy for the degree of hate (Guo et al., 2017, On Calibration
    of Modern Neural Networks). Vidgen et al. (2019) also point towards approaches
    that distinguish different types of hate (e.g., identifying gender-based hate
    distinctly from race-based hate), but these approaches are not widely in use to
    our knowledge.\n\n#### Maximizing Performance/Metrics Selection\n\nWhat is the
    correct metric and evaluation technique for hate speech detection? What is the
    definition of a “good” hate speech algorithm, and can you benchmark an algorithm
    against another dataset? This depends on how a platform wants to implement it,
    and the criteria for performance that could be applied \n\n([Toxic comment classification
    challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge
    \"Toxic comment classification challenge\"))\n\nIn machine learning, performance
    tends to be evaluated primary metrics like accuracy, precision, recall, and F1.
    Other common metrics are also RMSE (root mean squared error), RAE (root average
    error), average precision, and occasionally false negative rates.These can also
    include specificity and sensitivity, R^2, log loss, and confusion matrices. \n\nThere
    are other measurements of effectiveness of an algorithm that have little to do
    with the accuracy of the identification, however, that impact pragmatic scenarios.
    These include speed of removal or identification (how long the algo takes to run),
    data quality (what happens with the quality of data degrades?), performance in
    nuanced situations, and the interpretability of algorithms for humans who must
    interact with algorithms.\n\n__Common Datasets for Evaluation:__ In hate speech
    detection, there are very few common datasets to test against. This is because
    the definition and data around hate speech are constantly shifting and changing.
    There is also likely hesitancy by corporations and other stakeholders to share
    data or models, because people will learn how to abuse the system and subvert
    the automated detection.\n\n__Precision/recall tradeoffs:__ Is it worse to detecting
    something that is hate speech that isn’t, or miss real hate speech? Precision/recall
    tradeoffs come up often in ML research, and hate speech is no exception. \n\nFalse
    positives, or having poor precision, means that content that is not hate speech
    will be removed for being considered hate speech. Over-removing content from platforms
    can erode trust in moderation practices and decrease user engagement, and can
    also dramatically increase moderator or reviewer burden for these platforms. \n\nOn
    the other hand, false negatives or having poor recall means that legitimate hate
    speech content stays on the platform. This can also have the effect of eroding
    trust in the platform, and the harmful effects of hate speech to continue.\n"
  additional_text: "## Hate Speech Moderation in Practice\n\nIn many cases, such as
    on Reddit, Facebook Pages and Groups, livestreaming applications such as Mixer
    and Twitch, and community conversation platforms such as Discord, users (typically
    volunteer user moderators) have a direct role in applying and managing algorithmically-based
    tools for moderation. The level of control these moderators have over how the
    tools function varies, and tools’ detection algorithms are variably transparent.
    For example, Reddit has a history of formally adopting user-developed tools as
    official platform features, and the popular AutoModerator tool is one of the most
    visible examples of this. The functions of AutoModerator are extensively documented
    and fully controllable by moderators (see [Reddit Full AutoModerator Documentation](https://www.reddit.com/wiki/automoderator/full-documentation
    \"Reddit Full AutoModerator Documentation\")), though use of the tool requires
    a baseline level of technical knowledge because rules syntax is written in the
    YAML language. AutoModerator’s algorithms for parsing text of comments and posts
    are regex-based, meaning that they only identify explicitly-defined strings of
    characters, though regex allows some flexibility that can catch small variations
    on commonly problematic strings. AutoModerator does not currently employ machine
    learning driven algorithms, though recent academic research has explored the possibility
    of deploying these as a third-party tool. \n\nTwitch’s “AutoMod” is a built-in
    moderation tool that is designed to be easily accessible to users. Livestreamers
    and Moderators simply select a general all-purpose setting from 0-4 that defines
    how “strict” they want the filter to be, where 4 is very strict and 0 turns filtering
    off (See [Twitch: How to use automod](https://help.twitch.tv/s/article/how-to-use-automod?language=en_US
    \"Twitch: How to use automod\")). The default setting is a “Identity language”
    (which includes hate speech), “Sexually explicit language”, “Aggressive language”,
    and “Profanity”. The specific algorithms used in detection are not visible to
    streamers, moderators, or regular users, but the documentation implies in a general
    sense that the algorithms are primarily text-matching based like Reddit’s AutoModerator
    rather than using machine learning driven. Recent work by Seering and Kaufman
    finds that moderators also believe the algorithms to be primarily driven by text-matching.
    Twitch’s AutoMod and Reddit’s AutoModerator thus contrast on two dimensions—Reddit’s
    AutoModerator is fully transparent in its inner workings but requires a baseline
    level of technical knowledge to operate, while Twitch’s AutoMod is less transparent
    but very user-friendly. <sup><a id=\"fnref1\" href=\"#fn1\">[1]</a></sup>\n\nOn
    the dimension of technical complexity, however, these two tools are both relatively
    simple. If we make the above assumptions about the (less-transparent) way that
    Twitch’s AutoMod functions, neither tool has grappled with the challenges in detection
    outlined above in this document. Technical simplicity limits the usefulness of
    these tools primarily because filtering solely for strings of text does not engage
    at all with context. In interviews with LGBTQ moderators on Twitch, Seering and
    Kaufman found that some found AutoMod to be more trouble than it was worth because
    it caught words that could be used as a slur in many cases (e.g., “queer”) but
    that were used as self-affirming or simply as part of a user’s identity description
    within this particular context. However, the simplicity offered by the transparency
    in tools like Reddit’s AutoModerator allows moderators to retain full control
    over the tool’s actions. Whereas machine learning driven algorithms will in many
    cases be opaque to users in their decision-making processes because of the complexity
    of the algorithms, moderators will know in every case why a comment was flagged
    or removed by AutoModerator.\n\nThis leads to a broader point that moves beyond
    just detection algorithms and into the space of moderation as a whole. One of
    the major decisions that platforms have to make is where control over moderation
    decisions resides (which may vary for different types of decisions). The following
    examples show models ranging from mainly platform-driven moderation to mainly
    user-driven moderation: \n\n- Twitter is one visible example of a platform where
    most moderation decisions are made by the platform; Twitter has made it clear
    that the moderation process typically begins with users reporting content, which
    Twitter then reviews through various internal processes (likely including some
    combination of algorithms and human review). Individual users’ moderation tools
    are limited primarily to muting and blocking other users, which hides them from
    their view individually but does not otherwise impact the offending user’s capabilities.
    Note however that Twitter has been testing a “Hide Replies” feature since July
    2019 that allows users to hide tweets replying to their tweets from public view,
    meaning that anyone else viewing the thread does not immediately see these replies
    (though all hidden tweets are still viewable if a button is clicked). See [Using
    Twitter, mentions and replies](https://help.twitter.com/en/using-twitter/mentions-and-replies
    \"Using Twitter, mentions and replies\"). Instagram follows a similar platform-centric
    moderation model, and has a similar style of feature granting users limited moderation
    powers on their own posts.\n- Facebook uses a mixed model, with the platform driving
    most moderation of individuals’ posts on timelines while granting users much more
    authority within Groups and Pages. Timeline moderation functions in a form that
    is, at least to first order, conceptually similar to the way in which Twitter
    and Instagram are moderated, with users’ abilities to moderate their feed being
    limited mainly to reporting, unfollowing, unfriending, and blocking. Within Groups
    and Pages, however, moderators, “admins”, and page owners have significantly more
    autonomy in removing comments, and blocking, kicking, or temporarily muting users
    from posting on their Groups or Pages. Groups can also be set to require moderator
    permission to join. Facebook as a platform still monitors posts in these spaces
    and removes them not infrequently without communicating with moderators, and in
    some cases removes entire Groups or Pages for rule violations. It is also worth
    noting that Facebook has generally been less open to allowing third-party tools
    to access content, meaning that user-created moderation tools are much less common
    than on, e.g., Twitch, Discord, or Reddit.\n- A third category of platforms places
    authority for the vast majority of basic moderation decisions in the hands of
    volunteer user moderators. For example, Reddit very rarely intervenes to remove
    or edit individual posts or comments within subreddits, and interviewed moderators
    and livestreamers on Twitch were not aware of Twitch ever intervening to remove
    comments on an individual stream or to ban users from a particular stream. Instead,
    these platforms act primarily at the community-level, temporarily or permanently
    removing or restricting entire communities (i.e., “quarantining” on Reddit) when
    content is visibly rule-breaking and/or when moderators do not display willingness
    or ability to moderate these communities. Note that both platforms still do reserve
    the right to ban single users from the platform, e.g., Reddit’s recent ban waves
    of Russian spam accounts.\n\nBroadly, the above models have two dimensions—where
    the platform situates itself in the moderation process, and where the platform
    situates users in the moderation process. The platform can focus primarily on
    moderating individual users and individual pieces of content, as on Twitter and
    Instagram, on moderating full communities like on Reddit and Twitch, or on a mixture
    of the two. Similarly, the platform can grant users authority to moderate what
    they see themselves and/or to moderate what their communities see via the ability
    to become community moderators. A third dimension that is often not explicitly
    visible or consciously decided is what moderation decisions are delegated to algorithms.
    This decision is made both by platforms in their internal processes and in what
    tools they design for users and by community moderators in how they use available
    algorithmically-driven tools. As noted by Seering et al. (2019), both communities
    and platforms evolve through conversations about what types of content to permit,
    and delegation of authority to algorithms can impact how these conversations unfold.
    On one extreme, the choice not to involve any algorithms can result in an impossibly
    high workload for human moderators in some cases but allows them to see and discuss
    every decision, which can naturally lead to evolution of moderation philosophies
    over time. On the other extreme, algorithms can detect problematic content at
    scale, but when humans are less involved in moderation decisions, the ability
    to evolve in response to changing norms is hindered. Many researchers in recent
    years have advocated for keeping humans actively in the loop in algorithmic processes,
    developing tools that “support, rather than supplant” users’ judgment (Seering
    et al., 2019).\n"
  footnotes:
  - sys:
      id: 1UVnaJ9b87EtY4vMJ5wtbP
      created_at: !ruby/object:DateTime 2020-11-10 13:14:05.071000000 Z
      updated_at: !ruby/object:DateTime 2020-11-10 13:14:12.966000000 Z
      content_type_id: footnote
      revision: 0
    body: 'We note, however, that there are extensive user-developed resources for
      helping less technically-savvy moderators set up Reddit’s AutoModerator, and
      in interviews Seering and Kaufman also found that moderators frequently help
      each other by sharing settings and answering questions. See also Jhaver et al.’s
      interview work on how large subreddits use AutoModerator.

      '
  references: "- *Note: this is not a comprehensive lit review of all hate speech
    research. For such a list, please see, Fortuna and Nunes.*\n\n- Badjatiya, Pinkesh,
    Shashank Gupta, Manish Gupta, and Vasudeva Varma. 2017. “[Deep Learning for Hate
    Speech Detection in Tweets](https://doi.org/10.18653/v1/e17-2068 \"Deep Learning
    for Hate Speech Detection in Tweets\").” In WWW, 427–31.\n\n- Burnap, Pete, and
    Matthew Williams. 2015. “[Hate Speech , Machine Classification and Statistical
    Modelling of Information Flows on Twitter : Interpretation and Communication for
    Policy Decision Making](https://doi.org/http://dx.doi.org/10.1002/poi3.85 \"Wiley
    Online Library\").” Internet, Policy & Politics 9999 (9999): 1–18. \n\n- Chancellor,
    Stevie, Yannis Kalantidis, Jessica A. Pater, Munmun De Choudhury, and David A.
    Shamma. 2017. “[Multimodal Classification of Moderated Online Pro-Eating Disorder
    Content.](https://doi.org/10.1145/3025453.3025985 \"Multimodal Classification
    of Moderated Online Pro-Eating Disorder Content\")” In CHI, 3213–26.\n\n- Chandrasekharan,
    Eshwar, Umashanthi Pavalanathan, Anirudh Srinivasan, Adam Glynn, Jacob Eisenstein,
    and Eric Gilbert. 2017. “[You Can’t Stay Here: The Efficacy of Reddit’s 2015 Ban
    Examined Through Hate Speech.](https://doi.org/10.1145/3134666 \"You Can't Stay
    Here: The Efficacy of Reddit's 2015 Ban Examined Through Hate Speech\")” CSCW
    - Proc. ACM Hum.-Comput. Interact 1 (2): 1–22. \n\n- Davidson, Thomas, Dana Warmsley,
    Michael Macy, and Ingmar Weber. 2017. “[Automated Hate Speech Detection and the
    Problem of Offensive Language.](http://arxiv.org/abs/1703.04009 \"Automated Hate
    Speech Detection and the Problem of Offensive Language.\")” In ICWSM, 512–15.
    \n\n- Djuric, Nemanja, Jing Zhou, Robin Morris, Mihajlo Grbovic, Vlandan Radosavljevic,
    and Narayan Bhamidipati. 2014. “Hate Speech Detection with Comment Embeddings.”
    In WWW Companion?, 29–30.\n\n- ElSherief, Mai, Vivek Kulkarni, Dana Nguyen, William
    Yang Wang, and Elizabeth Belding. 2018. “[Hate Lingo: A Target-Based Linguistic
    Analysis of Hate Speech in Social Media.](http://arxiv.org/abs/1804.04257 \"Hate
    Lingo: A Target-Based Linguistic Analysis of Hate Speech in Social Media.\")”
    In ICWSM, 42–51.\n\n- Fortuna, Paula, and Sérgio Nunes. 2018. “[A Survey on Automatic
    Detection of Hate Speech in Text.](https://doi.org/10.1145/3232676 \"A Survey
    on Automatic Detection of Hate Speech in Text\")” ACM Computing Surveys 51 (4):
    1–30.\n\n- Gitari, Njagi Dennis, Zhang Zuping, Hanyurwimfura Damien, and Jun Long.
    2015. “[A Lexicon-Based Approach for Hate Speech Detection.](https://doi.org/10.14257/ijmue.2015.10.4.21
    \"A Lexicon-Based Approach for Hate Speech Detection\")” International Journal
    of Multimedia and Ubiquitous Engineering 10 (4): 215–30.\n\n- Jurgens, David,
    Eshwar Chandrasekharan, and Libby Hemphill. 2019. “[A Just and Comprehensive Strategy
    for Using NLP to Address Online Abuse.](http://arxiv.org/abs/1906.01738 \"A Just
    and Comprehensive Strategy for Using NLP to Address Online Abuse\")” In ACL. \n\n-
    Mondal, Mainack, Leandro Araújo Silva, and Fabrício Benevenuto. 2017. “[A Measurement
    Study of Hate Speech in Social Media.](https://doi.org/10.1145/3078714.3078723
    \"A Measurement Study of Hate Speech in Social Media.\")” In HT, 85–94.\n\n- Nobata,
    Chikashi, Joel Tetreault, Achint Thomas, Yashar Mehdad, and Yi Chang. 2017. “[Abusive
    Language Detection in Online User Content.](https://doi.org/10.1145/2872427.2883062
    \"Abusive Language Detection in Online User Content\")” In WWW, 145–53.\n\n- Ross,
    Björn, Michael Rist, Guillermo Carbonell, Benjamin Cabrera, Nils Kurowsky, and
    Michael Wojatzki. 2017. “[Measuring the Reliability of Hate Speech Annotations:
    The Case of the European Refugee Crisis.](https://doi.org/10.17185/duepublico/42132
    \"Measuring the Reliability of Hate Speech Annotations: The Case of the European
    Refugee Crisis.\")” In .\n\n- Schmidt, Anna, and Michael Wiegand. 2017. “[A Survey
    on Hate Speech Detection Using Natural Language Processing.](https://doi.org/10.18653/v1/w17-1101
    \"A Survey on Hate Speech Detection Using Natural Language Processing\")” In Workshop
    on Natural Language Processing for Social Media, 1–10.\n\n- Silva, Leandro, Mainack
    Mondal, Denzil Correa, Fabricio Benevenuto, and Ingmar Weber. 2016. “[Analyzing
    the Targets of Hate in Online Social Media.](http://arxiv.org/abs/1603.07709 \"Analyzing
    the Targets of Hate in Online Social Media\")” In ICWSM, 687–90. \n\n- Tonkinwise,
    Cameron. 2015.  \"[Design for Transitions‒from and to what?.](https://doi.org/10.1080/14487136.2015.1085686
    \"Design for Transitions‒from and to what?\")\" Design Philosophy Papers 13, no.
    1: 85-92.\n\n- Vidgen, Bertie, Alex Harris, Dong Nguyen, Rebekah Tromble, Scott
    Hale, and Helen Margetts. 2019. “[Challenges and Frontiers in Abusive Content
    Detection,](https://doi.org/10.18653/v1/w19-3509 \"Challenges and Frontiers in
    Abusive Content Detection\")” no. Section 2: 80–93. \n\n- Warner, William, and
    Julia Hirschberg. 2012. \"[Detecting Hate Speech on the World Wide Web.](https://www.aclweb.org/anthology/W12-2103/
    \"Detecting Hate Speech on the World Wide Web\")” In Workshop on Language in Social
    Media.\n\n- Waseem, Zeerak. 2016. “[Are You a Racist or Am I Seeing Things? Annotator
    Influence on Hate Speech Detection on Twitter,](https://doi.org/10.18653/v1/w16-5618
    \"Are You a Racist or Am I Seeing Things? Annotator Influence on Hate Speech Detection
    on Twitter,\")” 138–42.\n\n- Waseem, Zeerak, and Dirk Hovy. 2016. “[Hateful Symbols
    or Hateful People? Predictive Features for Hate Speech Detection on Twitter.](https://doi.org/10.18653/v1/n16-2013
    \"Hateful Symbols or Hateful People? \")” In NAACL, 88–93.\n\n- Wulczyn, Ellery,
    Nithum Thain, and Lucas Dixon. 2016. “[Ex Machina: Personal Attacks Seen at Scale,](http://arxiv.org/abs/1610.08914
    \"Ex Machina: Personal Attacks Seen at Scale\")” 1391–99."
  alt_date: !ruby/object:DateTime 2020-11-17 00:00:00.000000000 Z
  author:
  - sys:
      id: 55lvpa6OODrYw65JImLBDO
      created_at: !ruby/object:DateTime 2020-03-24 00:44:57.806000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:04:55.247000000 Z
      content_type_id: contributor
      revision: 3
    name: Kat Lo
    staff: true
    role: Product Strategy Researcher<br>Content Moderation Lead
    location: Los Angeles
    bio: |-
      Kat Lo serves as Content Moderation Lead and a product researcher at Meedan. Drawing from more than ten years of experience running online communities, Kat works with researchers, industry professionals, and community leaders to develop resources for fostering healthy social environments in games and social media. She has spent the past 5 years working with Intel, Microsoft Research, Instagram, Southern Poverty Law Center, AnyKey, and other organizations to study issues around diversity, inclusion and trust & safety. She has won fellowships from the National Science Foundation, Google, and the UC Berkeley Center for Technology, Society & Policy for her research on diversity and inclusion in technology.

      Kat is currently an industry consultant and researcher at the University of California Irvine, where she studies moderation practices in online spaces. Drawing from methods across user experience research and data science, her work focuses on the development of tools and community management best practices to mitigate toxic online environments. She advises non-profit organizations in mental health and online harassment, and assists in crisis support for targets of online harassment.
  publication:
  - sys:
      id: 1EPFTrNepDeTMUR1FbT1LV
      created_at: !ruby/object:DateTime 2020-11-09 15:59:09.591000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:06:32.246000000 Z
      content_type_id: publication
      revision: 1
    title: Content Moderation Toolkit
    short_title: Content Moderation Toolkit
    description: Content moderation is a process, not a solution in and of itself.
      With the increasing complexity of content on the internet and norms around governance,
      new standards are needed to help bridge the gap between industry and those seeking
      to engage with industry. With support from the Swedish International Development
      Agency and the Facebook Journalism Project, Meedan presents this work toward
      a common vocabulary and methodology for content moderation as a toolkit for
      civil society and platforms alike grappling with these issues.
    slug: content-moderation-toolkit
    sections:
    - sys:
        id: 2ldDGlFH61yvNc2x9DF2ex
    author:
    - sys:
        id: 55lvpa6OODrYw65JImLBDO
        created_at: !ruby/object:DateTime 2020-03-24 00:44:57.806000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:55.247000000 Z
        content_type_id: contributor
        revision: 3
      name: Kat Lo
      staff: true
      role: Product Strategy Researcher<br>Content Moderation Lead
      location: Los Angeles
      bio: |-
        Kat Lo serves as Content Moderation Lead and a product researcher at Meedan. Drawing from more than ten years of experience running online communities, Kat works with researchers, industry professionals, and community leaders to develop resources for fostering healthy social environments in games and social media. She has spent the past 5 years working with Intel, Microsoft Research, Instagram, Southern Poverty Law Center, AnyKey, and other organizations to study issues around diversity, inclusion and trust & safety. She has won fellowships from the National Science Foundation, Google, and the UC Berkeley Center for Technology, Society & Policy for her research on diversity and inclusion in technology.

        Kat is currently an industry consultant and researcher at the University of California Irvine, where she studies moderation practices in online spaces. Drawing from methods across user experience research and data science, her work focuses on the development of tools and community management best practices to mitigate toxic online environments. She advises non-profit organizations in mental health and online harassment, and assists in crisis support for targets of online harassment.
    additional_attribution: Lead, Meedan Content Moderation Project
    cover_image:
      sys:
        id: 6VYAFLYRLJnOVMBZSy1KD2
        created_at: !ruby/object:DateTime 2020-11-09 16:18:18.284000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:49.395000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/6VYAFLYRLJnOVMBZSy1KD2/4e40e178f196d8b024f0964ac95051e9/content-moderation-toolkit_cover-image.png"
    publication_promo_graphic:
      sys:
        id: 1DzzJ94lvtgdJnCaYLRegg
        created_at: !ruby/object:DateTime 2020-11-09 16:22:38.136000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:45.742000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/1DzzJ94lvtgdJnCaYLRegg/c3690f778b91576bebf6d62b5a14091f/content-moderation-toolkit_publication-promo-graphic.png"
    social_card_image:
      sys:
        id: 2biBznblY8VQEDW3NaheSe
        created_at: !ruby/object:DateTime 2020-11-17 19:26:39.227000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:41.047000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/2biBznblY8VQEDW3NaheSe/abdd8d11f56fd989a34d6d623428bbf3/Content-moderation-toolkit_executive-summary_social-card.png"
    publication_promo: This report is part of the [Content Moderation Toolkit](https://meedan-dev.netlify.app/reports/content-moderation-toolkit/)
    color:
      sys:
        id: sw3ntNjhkQRzLXqLpNM2m
        created_at: !ruby/object:DateTime 2020-11-09 16:06:32.559000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:35.341000000 Z
        content_type_id: color
        revision: 1
      name: Magenta
      value: "#ff4d7e"
      thumbnail:
        sys:
          id: ewZMwgt0xilyUB6umRuQs
          created_at: !ruby/object:DateTime 2020-11-09 16:07:07.170000000 Z
          updated_at: !ruby/object:DateTime 2020-11-18 20:04:32.936000000 Z
        title: Magenta
        description: "#ff4d7e"
        url: "//images.ctfassets.net/g118h5yoccvd/ewZMwgt0xilyUB6umRuQs/68b8db4246a336ad25a4c7658c0fafd8/magenta-01.png"
      text: "#262626"
  color:
    sys:
      id: 2rZsEVEEflN7PBITyTo5g0
      created_at: !ruby/object:DateTime 2020-11-10 10:58:48.574000000 Z
      updated_at: !ruby/object:DateTime 2020-11-10 10:59:19.578000000 Z
      content_type_id: color
      revision: 0
    name: Red
    value: "#ff5f4d"
    thumbnail:
      sys:
        id: 3pSfK3R13BCVEeAavHEKXU
        created_at: !ruby/object:DateTime 2020-11-10 10:59:18.218000000 Z
        updated_at: !ruby/object:DateTime 2020-11-10 11:00:25.926000000 Z
      title: Red
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/3pSfK3R13BCVEeAavHEKXU/8c613347f65a00791b5f01b3c29d86ec/red-01.png"
  image: https://images.ctfassets.net/g118h5yoccvd/1Ne7vr9DQQebfWXlj69zXL/f0704bf859f2a7ae40ba8f645c6759a8/content-moderation-toolkit_hate-speech_social-card.png
- sys:
    id: 1Q7LLqUAyLZefSMra2NRKZ
    created_at: !ruby/object:DateTime 2020-11-10 14:12:55.148000000 Z
    updated_at: !ruby/object:DateTime 2020-11-17 20:14:54.109000000 Z
    content_type_id: report
    revision: 0
    space:
      sys:
        id: g118h5yoccvd
    type: Entry
    content_type:
      sys:
        id: report
    environment:
      sys:
        id: master
        type: Link
        linkType: Environment
    locale: en-US
  title: A Case Study from the Kenyan Election and Census
  slug: a-case-study-from-the-kenyan-election-and-census
  lead_image:
    sys:
      id: 1TJgZsRZ4CrVL0tkRl9zRB
      created_at: !ruby/object:DateTime 2020-11-10 14:59:45.221000000 Z
      updated_at: !ruby/object:DateTime 2020-11-17 19:57:36.016000000 Z
    title: A Case Study from the Kenyan Election and Census
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/1TJgZsRZ4CrVL0tkRl9zRB/be1ba5a9928daaf0cc68ad46ed0669e0/content-moderation-toolkit_kenyan-election-and-census_lead-image.png"
  social_card_image:
    sys:
      id: 3CuWZ27QualPFqW9aInWe4
      created_at: !ruby/object:DateTime 2020-11-10 15:00:17.132000000 Z
      updated_at: !ruby/object:DateTime 2020-11-17 19:58:38.060000000 Z
    title: A Case Study from the Kenyan Election and Census
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/3CuWZ27QualPFqW9aInWe4/9fbe19707584bfdbc1882f5127f5c4fe/content-moderation-toolkit_kenyan-election-and-census_social-card.png"
  introduction: Our primary concern here is around hate speech and disinformation
    as they relate to interference with the Kenyan elections and census. This document
    outlines the list of different indicators that will be assessed during the annotation
    process (Indicators for Kenya CSO Annotation), developed in concert with Article
    19 and local Kenyan CSOs after a review of relevant content online.
  body: "This is not a comprehensive representation of indicators for measuring hate
    speech and disinformation; these are indicators that we suggest can be used for
    evaluation where there is limited access to context around the articles or pieces
    of content that are being evaluated, and which are relevant to the Kenyan elections
    of 2017 and the 2019 Housing and Population Census, with potential for impact
    in the 2022 elections as well. The intention is not to identify content that should
    be taken down, but to examine how this speech is used to bolster election/census
    interference, misinformation, hate, and incitement to violence. \n\n## Indicator
    Taxonomy\n### Category 1: Hate Speech\n#### Project Definition\n\nAny form of
    expression, language or imagery, that incites violence and discrimination towards
    an individual or group of people based on race, ethnicity, religious affiliation,
    sexual orientation, caste, sex, gender, serious disease or disability, immigration
    status, political affiliation, social class, culture, tribal affiliation, or regional
    affiliation. \n\nWe operationalize the types of people targeted by hate speech
    as “categories.” Several categories are broken down to clarify more directly how
    they may apply in Kenya:\n\n- __Race:__ skin, eye or hair color, bone structure\n-
    __Religious Affiliation:__ Christianity, Buddhism, Islam, Hinduism, indigenous
    African religions, mix\n- __Ethnicity:__ national origin, ancestry, language\n-
    __Culture:__ In Kenya this is recognised as a basis for discrimination. Examples
    of cultural customs that attract vilification from within the group or outside
    include: male circumcision- valued as a right of passage to adulthood in most
    communities. Luo community does not practice circumcision and are often vilified
    by being referred to as “boys” or “kihii” in Kikuyu language. \n- Maasai and other
    pastoralist communities- women are classified together with children and property.
    It is therefore vilifying to be referred to as “woman” or behaving like a woman\n-
    Luo community used to practice wife inheritance where widows were taken up by
    their late husband’s relatives. Those who prefer to remain widowed are vilified
    and a reference to someone as a widow is quite demeaning.\n\n### Definitions of
    Hate Speech\n\nHate speech is defined differently across different contexts by
    cultural norms, governing bodies and human rights groups. Hate speech is also
    politically and culturally situated regionally, and thus we must take care in
    understanding hate speech in and across the Kenyan context. We present these definitions
    to engage directly with the assumptions annotators and organizations may be predisposed
    to in their own definitions of hate speech, as well as to identify how human right
    violations amplified or caused by hate speech may be identified and acted upon
    by local governing bodies, social media platform operators, and global organizations.\n\nThe
    Content Moderation Project has assembled a literature review that explores definitions
    of hate speech from social media platforms, civil society organizations, and law.\n\n####
    Context for Legal Definitions\n\nSocial media companies attempt to govern their
    platforms based on generalized global norms, but operate from primarily from US-based
    perspectives. They additionally face limitations adjusting to national and regional
    norms. To understand how our definition of hate speech relates to how hate speech
    is formally acted on in Kenya, we examine how it is deployed in national legislation
    around hate speech and protected groups (i.e., legally defined categories) in
    Kenya. To better understand state-defined notions of hate speech, it is important
    to first understand the distinctions between how hate speech and protected groups
    are legally defined and regulated in the US and Kenya, both formally and in practice.\n\nDefinitions
    of hate speech that adhere to preserving freedom of expression according to international
    human rights standards do not necessarily align with definitions held by the Kenyan
    government. Thus, we must enable CSOs operating within Kenya to inform how our
    indicators on hate speech can be operationalized and deployed beyond how it is
    defined by the Kenyan government.\n\nThe legally protected categories in the US,
    typically called __“protected classes”__ or __protected groups__ in the __United
    States__ is defined differently based on federal (from civil rights, equal pay,
    discrimination acts, presidential executive orders) and state anti-discrimination
    legislation. Federal anti-discrimination law currently covers race, religion,
    national origin, age, sex, pregnancy, familial status, disability status, veteran
    status, and genetic information. Hate speech is legal  in the US under the First
    Amendment and  hate speech alone is not a crime (hate speech is criminal, however,
    in the case of hate crimes, when hate speech is directly connected with incitement
    to violence or acts of violence). \n\nIn Kenya, current hate speech legislation
    is primarily defined and effectively enforced around ethnic hate. The National
    Cohesion and Integration Act (2008) revised the definition of hate speech to incitement
    of ethnic hatred and determined that it is not protected under the right to freedom
    of expression under the Kenyan Constitution. This legislation on speech around
    ethnic hatred has been ambiguously defined, and thus bears the risk of abuse by
    the government to suppress dissenters while simultaneously making it difficult
    to successfully prosecute hate speech at all. \n\n### Construct 1.1: Discriminatory
    hatred directed towards a category of people\n\nExplanation: The goal of this
    construct is to assess articles, posts, and comments that contribute to discrimination
    towards categories of people. \n\n#### Indicators for Construct 1.1 \n\nWe can
    operationalize this construct by measuring the following indicators:\n\n**I. An
    article or piece of content contains non-consensual slurs, epithets, stereotypes,
    and other statement meant to degrade or dehumanize a group or an individual based
    on their membership in a category of people.**\n\n**Examples:** A social media
    post calling Luos people “Jaluo” after a violent incident eg fights after a football
    match. \n\n**Targets:** an individual in that category, people within that category
    more broadly, a gathering or organization of people within that category.\n\n**II.
    An expression of desire for hardship, discrimination, removal of rights for a
    category or an individual in a category**. In this case, a slur, epithet, or stereotype
    does not necessarily need to be used. A statement of support or affirmation of
    association or membership with a hate group known for inciting violence and cultivating
    fear or grievances falls under this category.\n\n__Examples:__ \n1. Homophobic
    statements about an LBGTQ author who had recently died, that he deserved to die
    and that he is going to hell for his homosexuality.\n2. Advocating for homosexuality
    to remain criminalized and that there cannot legally be registered LGBTQI NGOs.\n3.
    Claiming to be “Uthamaki” and supporting Kikuyu supremacy.\n\n__Targets:__ An
    individual in that category, broadcasted generally to an audience, towards people
    vocally supporting that category of people, towards organizations of people in
    that category or advocating for people in that category.\n\n#### Secondary Indicators
    for Construct 1.1\n\n1. The article or piece of content is made as a __response
    to more clear or radical forms of hate speech__.\n2. There is __coded language__
    about people in a category, or a history around and between people in particular
    categories, that is known to the audience. An example of this is the phrase \"go
    to work,\" used as code for killing during the Rwandan genocide.\n3. __Strong
    in-group identification__, specifically identify “us” language for groups of people.
    Here, in identifying strong in-group identities and the “exaltation of the [in
    group] above other groups”, we use this indicator to identify language that promotes
    strong within-group dynamics and promotion. This language can also be embedded
    within requests that readers share ‘if they agree’ with a particular group.\n\n###
    Construct 1.2: Inciting, encouraging, or lauding physical violence towards a category
    of people\n\n- Explanation: The goal of this construct is to identify clear instances
    of hate speech that inspire or incite physical violence towards a category of
    people. The [Dangerous Speech Project](https://dangerousspeech.org/guide/ \"Dangerous
    Speech Project\") proposes the term “dangerous speech” as an alternative to hate
    speech, which engages with potential for physical violence, rather than the more
    typically ambiguous terms “hate” or “harms.” They formally define dangerous speech
    as expression that increases the risk that its audience will condone violence
    against members of another category, where the severity of dangerous speech is
    determined by the extent to which five variables are maximized: \n\n  - a powerful
    speaker with a high degree of influence over the audience.\n  - the audience has
    grievances and fear that the speaker can cultivate.\n  - a speech act that is
    clearly understood as a call to violence,\n  - a social or historical context
    that is propitious for violence, for any of a variety of reasons, including longstanding
    competition between groups for resources, lack of efforts to solve grievances,
    or previous episodes of violence.\n  - a means of dissemination that is influential
    in itself, for example because it is the sole or primary source of news for the
    relevant audience. *Benesch, Susan. \"Dangerous speech: A proposal to prevent
    group violence.\" Voices That Poison: Dangerous Speech Project (2012).*\n\nGiven
    that annotators and moderators may have a limited ability to investigate the direct
    context around an article or piece of content (e.g., surrounding social media
    posts, responses to or preceding social media posts or articles), we focus on
    identifying clear calls to violence, and encouragement of violence through cultivation
    of fear and grievances. We have formed indicator questions to enable CSOs to provide
    information on the social and historical context of articles and pieces of content
    that may foster an environment for violence.\n\n#### Indicators for Construct
    1.2\n\nWe can operationalize this construct by measuring the following indicators:\n\n**A
    call for violence or an expression of desire (wishing or hoping) for physical
    violence to be perpetrated against a group or a gathering of people from a protected
    group.** This includes laudatory references towards or depictions of historical,
    systematic, current, or ongoing violence towards protected groups.\n\n__Examples:__
    \n1. Religious social media posts saying that gay people are abominations against
    god and they must die or deserve to be killed.\n2. A post containing imagery depicting
    graphic targeted ethnic violence towards Kikuyu people during the 2007 Kenyan
    crisis presented in a positive light.\n\n__Targets:__ An individual in that category,
    people who are vocally supporting people in that category, a broader audience
    that can see the post.\n\n#### Secondary Indicators for Construct 1.2\n\n1. [Identifying
    potential of direct harm by author] The author has posted additional content or
    media that appears to be advocating for violence or making physical threats against
    others.\n2. [Identifying the authors’ degree of contribution to content that is
    potentially harmful] The author is contributing to a large volume of calls for
    violence and threats on social media.\n3. [Examining the authors’ network] The
    author’s online presence is connected to other accounts that advocate for violence
    against people in that category, or organizations that have historically incited
    violence.\n4. [Identifying influence] The author has a high number of followers,
    is known to be a public figure, making a claim to authority, or other signifiers
    of having high influence\n5. [Identifying influence] The content is widely circulated
    through a high number of retweets, likes, shares and other forms of engagement.\n\n###
    Construct 1.3: Threats of direct violence towards an identifiable target or multiple
    targets based on their membership in a protected group\n\nExplanation: The goal
    of this construct is to identify and assess content that directly threatens the
    physical safety of a person in a category. Defining, identifying, and acting on
    threats is difficult to formalize because of the distributed nature of threats
    and harassment, and the inability to predict when a threat will manifest as offline
    action. Given this ambiguity, online threats casts the safety of targets in doubt
    regardless of outcome and contributes to suppression of their speech. Distributed
    forms of harassment that disproportionately target female journalists, which typically
    include both direct and indirect physical threats, create a “[chilling effect
    on the freedom of expression](https://medium.com/berkman-klein-center/online-harassment-of-women-journalists-and-international-law-not-just-a-gender-issue-but-a-b8c6a5c7e128
    \"chilling effect on the freedom of expression\")” for notable, underserved actors
    in democratic discourse.\n\nSocial media platforms often require that a threat
    must be considered ‘credible’ to be removed. However, it is difficult to ascertain
    when a threat of violence is considered ‘credible,’ where it is unclear that the
    perpetrator has a clear relationship with the target or the ability to find and
    perpetrate violence towards the target offline. Common characteristics of urgent
    and clear forms of ‘credible’ threats include when a perpetrator has posted the
    physical address of a target or their friends and family, suggested that the perpetrator
    knows personal details about the target, or references an event that the target
    is publicly planning to attend. It is not required that the perpetrator explicitly
    state that they will harm the target in order to make a credible threat of violence.
    These forms of threats often take place in ways that are difficult to identify
    in individual posts, however, given the limited information that an annotator
    is able to investigate around an individual article or piece of content, we limit
    identification of direct threats of violence to what is apparent in the post itself.
    A threat does not have to be directed at or to the target(s), but can be a credible
    statement of intent to harm that is made to others, including information about
    the target(s) that threaten their safety.\n\nThreats of direct violence can target:
    (1) An individual and people related to the individual or (2) Members of an organization
    or a group of people (particularly at an event or location).\n\n#### Indicators
    for Construct 1.3 \n\nWe can operationalize this construct by measuring the following
    indicator:\n\n**Statement of intention to harm an individual or multiple people
    based on their membership in a category.**\n\n__Examples:__ \n1. A tweet that
    states the perpetrator knows where the target lives and that they will find them.\n2.
    A statement that a group of people going to or organization an event will come
    into serious harm if they attend.\n3. A Whatsapp message to a group claiming to
    know where certain lesbians live/sharing the address of LGBTI people in a discussion
    thread opposing decriminalisation of homosexuality /Repeal 165\n\n__Targets:__
    Journalists, activists, people in that category, vocal supporters of people in
    a category.\n\n#### Secondary Indicators for Construct 1.3\n\n1. The perpetrator
    has previously made, or continues to make, harassing comments and oblique threats
    to the target.\n2. The perpetrator has made inciting comments, dehumanizing comments,
    or references to violence or lauding a history of violence against members of
    the protected group the target is a part of, or towards the group generally.\n3.
    The perpetrator is part of a network, group, organization, or campaign of other
    accounts that have had similar conduct.\n\n#### Secondary indicator types to investigate
    for all hate speech constructs\n\nA challenge in identifying speech that suppresses
    freedom of expression and contributes to human rights violations is grappling
    with the presence of a historical, pervasive threat of violence or discrimination
    towards marginalized groups and the chilling effect that comes with it. These
    are proposed categories of secondary indicators that may yield useful or even
    necessary context for assessing the nature of content beyond what is made explicit
    in a single comment, post, or article:\n\n1. Surrounding comments, when statements
    that can only be evaluated within the context of other comments made by the same
    author or within the community of that author\n2. The use of hashtags and terms
    associated with hate campaigns\n3. Affiliation or networked with perpetrators
    of hate speech, misinformation, harassment.\n4. The positionality of the speaker--
    specifically ethnic identity, gender, etc\n\n## Category 2: Disinformation\n\n###
    Project Definitions\n\nWe have operationalized disinformation as campaigns or
    content that are ‘inauthentic,’ ‘geopolitically strategic,’ and ‘coordinated.’
    We define these terms in the following ways:\n\n1. __Inauthentic:__ content is
    inaccurate according to a clear consensus of experts in fields or makes a claim
    that is spurious; or that the author or publisher of content is counterfeit (misrepresented).\n2.
    __Geopolitically strategic:__ the attempt to achieve specific “real-world” objectives,
    by motivating the behaviors of large numbers of people to take individual actions
    with geopolitical outcomes.\n3. __Coordinated:__ online content and themes have
    been replicated and amplified across multiple sources across days, weeks, and
    months to give the appearance of greater interest and/or support in a topic than
    would be typical without assistance (manual or automated).\n\nWe have isolated
    two constructs for disinformation that annotators from local civil society groups
    will be best positioned to answer within the scope of a local workshop event.\n\n###
    Construct 2.1: Polarizing, Emotional, and Exaggerated Language\n\nExplanation:
    Polarizing, emotional, and exaggerated language can be used to bias or mislead
    audiences towards inauthentic content or away from authentic content, particularly
    in the absence of verified claims.  \n\n#### Indicators for Construct 2.1 \n\n__I.
    Polarizing Language.__ The Oxford living dictionary defines polarization as the
    “division into two sharply contrasting groups or sets of opinions or beliefs.”
    This indicator can refer to topics or ideas in addition to groups of people. In
    examining how articles and posts polarize issues, here we ask annotators to look
    for linguistic qualifiers that divide on specific issues. In general, a division
    of two contrasting either/or sets of opinions and beliefs involve labelling ideas
    or concepts as pro/anti or using terms that are defined as being for or against
    a given idea or concept. It is important to note that what constitutes polarizing
    language may be contested across different disciplines. For example, while the
    terms “pro-vaccine” and “anti-vaccine” may be used by public health information
    authorities, they are in some ways an inauthentic classification of what is, in
    actuality, a range of beliefs.\n\n__Example:__ A tweet creating a division between
    Luo and Kamba people: “Millie Odhiambo is a pure luo while Sonko is a pure kamba
    \n\nThere is no any day in our lifetime Mike Sonko will match Hon Millie Odhiambo,
    unless he starts it at class seven where Oscar Kipchumba Sudi left it!!!”\n\n__Target:__
    \nBroader audience of article or post, audience on the author’s side of the issue,
    undecided audiences vulnerable to polarizing language.\n\n__II. Emotional Language.__
    Readers can be misled by the emotional tone of articles and posts. Such language
    is common in opinion pieces and social media posts, which readers may parse as
    straight news or authentic representation of current events. Emotional tone can
    sometimes be indicated through the positivity or negativity of the language.\n\n__Example:__
    A social media post saying: “I feel for Monica Okoth. She was busy taking care
    of Ken while he was busy cheating behind her back. Men are thrash.”\n\n__Target:__
    Broader audience of article or post, audience on the author’s side of the issue,
    undecided audiences vulnerable to emotional language.\n\n__III. Exaggerations
    or minimizations related to the central claims of the article or post.__ In news
    articles and social media posts, the language used determines whether events or
    situations are represented in a proportional manner. In some cases, an article
    or post may over-emphasize something, making it seem much better or worse than
    it may be in reality. An article or post may also minimize something, making it
    seem much less important than it is in reality. \n\n__Example:__ An article states
    that there is “a tsunami of immigrations” if there has been an increase in immigration.\n\n__Target:__
    Broader audience of article or post, audience on the author’s side of the issue,
    undecided audiences vulnerable to exaggerated language.\n\n#### Secondary Indicators
    for Construct 2.1\n\n1. __Presence of key terms surrounding geopolitical topics/events.__
    Some key themes or terms, identified as ‘trigger words’, represent issues that
    are politically contentious, divisive and/or politically polarizing. It is important
    that this secondary indicator be understood as a library of key terms that is
    country specific and dynamic overtime, changing depending on geopolitically strategic
    topics for a given context.\n2. __Presence of key term hashtagged posts.__ This
    indicator seeks to examine the presence of a trending hashtag on social media
    on a particular key term surrounding geopolitical topics/events. This would require
    the same understanding that these key terms may evolve overtime.\n3. __Othering.__
    This indicator can be used to specifically identify “them” language for groups
    of people - blanket statements and generalities about specific groups, whether
    those statements are positive or negative. This can also include vocabulary specific
    to how a given group is different. Of course, the conditions that define a particular
    instance of othering can be renegotiated if societal conditions change, so context
    and timing specificity apply here as well. \n4. __Humor.__ The use of humor can
    be used both in order to be critical of hate speech or to disguise the intention
    of hate speech. Identifying if a piece of content is using humor is important
    for understanding the intended impact and message it is communicating to its audience,
    as distinct from how a post could be taken literally.\n5. __Influence.__ Is the
    speaker highly influential or speaking on behalf of an influential institution?
    The potential for harm from a piece of content is contingent on how widely it
    spreads, to whom, and how audiences perceive it. Potential signals of influence
    on social media could be having many followers or being verified on social media.
    Formal sources of influence could be political or religious leadership, which
    may be especially influential towards followers of their institutions.\n\n###
    Construct 2.2: (Mis)alignment with expert consensus\n\nExplanation: The goal of
    this construct is to assess an article or post’s misalignment with the most up-to-date
    expert knowledge. Within the article, findings, conclusions, and recommendations
    are based on the best available data. This data, which may surface within an article
    as in-text citations, hyperlinks, references to key sources or experts and quotes,
    should be cited properly, referencing experts with relevant and credible expertise
    that validate the conclusions drawn, and should be produced by non-stakeholders.\n\n####
    Indicators for Construct 2.2\n\n**I. Explicitly Unverified Claims.** \nAny references
    to explicitly unverified claims and language used to describe the fact that they
    haven’t yet been verified, validated, or proven to be true.\n\nFor evaluating
    explicitly unverified claims, we will be asking annotators to identify any references
    to explicitly unverified claims and the language used to describe the fact that
    they haven’t yet been verified, validated or proven to be true or false by a trusted
    source such as an IFCN-verified signatory. This includes language in an article
    explicitly referencing that a claim has not yet been verified to date but the
    claim is being mentioned in the article nonetheless, such as: “charges have not
    been proven true,” or “there’s no red ‘false’ text to clearly point to the rumors
    as works of fiction,” or “ it isn’t clear if the talk about [...] is true or the
    stuff of conspiracy theory fantasies.” \n\n__Examples:__ A politician tweets out
    to his followers that he estimates that a particular tribe is approximately a
    certain percentage of the population of a region, but doesn’t substantiate it
    or say where that number came from.\n\n__Targets:__ Broader audience of article
    or post, audience on the author’s side of the issue, undecided audiences, audiences
    with lower education level.\n\n**II. Context for Fact-checked Claims.** \n\nThere
    are organizations that seek to verify or debunk claims appearing in articles.
    A prominent example is Snopes.com. And in particular, International Fact Checking
    Network signatories are organizations that have publicly made [five commitments
    in their work](https://ifcncodeofprinciples.poynter.org/know-more/the-commitments-of-the-code-of-principles),
    including principles of nonpartisanship, transparency, and an open corrections
    policy. Other fact-checking or verification organizations may also follow these
    guidelines, without having become a signatory. For our purposes, the IFCN provides
    a useful baseline through which to make analyses and observations. \n\n__Examples:__
    \nA South African charity made the claim that “You are four times more likely
    to have your gun used against you than be able to use it successfully in self-defence.”
    There is an [Africa Check article](https://africacheck.org/reports/no-research-backs-widely-shared-statistic-about-gun-ownership-risk-in-south-africa/
    \"Africa Check \") on this claim, which concludes that it is Unproven: “There
    is no recent, reliable data on the topic.”\n\n__Targets:__ N/A\n"
  alt_date: !ruby/object:DateTime 2020-11-17 00:00:00.000000000 Z
  author:
  - sys:
      id: 55lvpa6OODrYw65JImLBDO
      created_at: !ruby/object:DateTime 2020-03-24 00:44:57.806000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:04:55.247000000 Z
      content_type_id: contributor
      revision: 3
    name: Kat Lo
    staff: true
    role: Product Strategy Researcher<br>Content Moderation Lead
    location: Los Angeles
    bio: |-
      Kat Lo serves as Content Moderation Lead and a product researcher at Meedan. Drawing from more than ten years of experience running online communities, Kat works with researchers, industry professionals, and community leaders to develop resources for fostering healthy social environments in games and social media. She has spent the past 5 years working with Intel, Microsoft Research, Instagram, Southern Poverty Law Center, AnyKey, and other organizations to study issues around diversity, inclusion and trust & safety. She has won fellowships from the National Science Foundation, Google, and the UC Berkeley Center for Technology, Society & Policy for her research on diversity and inclusion in technology.

      Kat is currently an industry consultant and researcher at the University of California Irvine, where she studies moderation practices in online spaces. Drawing from methods across user experience research and data science, her work focuses on the development of tools and community management best practices to mitigate toxic online environments. She advises non-profit organizations in mental health and online harassment, and assists in crisis support for targets of online harassment.
  publication:
  - sys:
      id: 1EPFTrNepDeTMUR1FbT1LV
      created_at: !ruby/object:DateTime 2020-11-09 15:59:09.591000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:06:32.246000000 Z
      content_type_id: publication
      revision: 1
    title: Content Moderation Toolkit
    short_title: Content Moderation Toolkit
    description: Content moderation is a process, not a solution in and of itself.
      With the increasing complexity of content on the internet and norms around governance,
      new standards are needed to help bridge the gap between industry and those seeking
      to engage with industry. With support from the Swedish International Development
      Agency and the Facebook Journalism Project, Meedan presents this work toward
      a common vocabulary and methodology for content moderation as a toolkit for
      civil society and platforms alike grappling with these issues.
    slug: content-moderation-toolkit
    sections:
    - sys:
        id: 2ldDGlFH61yvNc2x9DF2ex
    author:
    - sys:
        id: 55lvpa6OODrYw65JImLBDO
        created_at: !ruby/object:DateTime 2020-03-24 00:44:57.806000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:55.247000000 Z
        content_type_id: contributor
        revision: 3
      name: Kat Lo
      staff: true
      role: Product Strategy Researcher<br>Content Moderation Lead
      location: Los Angeles
      bio: |-
        Kat Lo serves as Content Moderation Lead and a product researcher at Meedan. Drawing from more than ten years of experience running online communities, Kat works with researchers, industry professionals, and community leaders to develop resources for fostering healthy social environments in games and social media. She has spent the past 5 years working with Intel, Microsoft Research, Instagram, Southern Poverty Law Center, AnyKey, and other organizations to study issues around diversity, inclusion and trust & safety. She has won fellowships from the National Science Foundation, Google, and the UC Berkeley Center for Technology, Society & Policy for her research on diversity and inclusion in technology.

        Kat is currently an industry consultant and researcher at the University of California Irvine, where she studies moderation practices in online spaces. Drawing from methods across user experience research and data science, her work focuses on the development of tools and community management best practices to mitigate toxic online environments. She advises non-profit organizations in mental health and online harassment, and assists in crisis support for targets of online harassment.
    additional_attribution: Lead, Meedan Content Moderation Project
    cover_image:
      sys:
        id: 6VYAFLYRLJnOVMBZSy1KD2
        created_at: !ruby/object:DateTime 2020-11-09 16:18:18.284000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:49.395000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/6VYAFLYRLJnOVMBZSy1KD2/4e40e178f196d8b024f0964ac95051e9/content-moderation-toolkit_cover-image.png"
    publication_promo_graphic:
      sys:
        id: 1DzzJ94lvtgdJnCaYLRegg
        created_at: !ruby/object:DateTime 2020-11-09 16:22:38.136000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:45.742000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/1DzzJ94lvtgdJnCaYLRegg/c3690f778b91576bebf6d62b5a14091f/content-moderation-toolkit_publication-promo-graphic.png"
    social_card_image:
      sys:
        id: 2biBznblY8VQEDW3NaheSe
        created_at: !ruby/object:DateTime 2020-11-17 19:26:39.227000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:41.047000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/2biBznblY8VQEDW3NaheSe/abdd8d11f56fd989a34d6d623428bbf3/Content-moderation-toolkit_executive-summary_social-card.png"
    publication_promo: This report is part of the [Content Moderation Toolkit](https://meedan-dev.netlify.app/reports/content-moderation-toolkit/)
    color:
      sys:
        id: sw3ntNjhkQRzLXqLpNM2m
        created_at: !ruby/object:DateTime 2020-11-09 16:06:32.559000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:35.341000000 Z
        content_type_id: color
        revision: 1
      name: Magenta
      value: "#ff4d7e"
      thumbnail:
        sys:
          id: ewZMwgt0xilyUB6umRuQs
          created_at: !ruby/object:DateTime 2020-11-09 16:07:07.170000000 Z
          updated_at: !ruby/object:DateTime 2020-11-18 20:04:32.936000000 Z
        title: Magenta
        description: "#ff4d7e"
        url: "//images.ctfassets.net/g118h5yoccvd/ewZMwgt0xilyUB6umRuQs/68b8db4246a336ad25a4c7658c0fafd8/magenta-01.png"
      text: "#262626"
  color:
    sys:
      id: 23y8AbmbWJctVzskwqwMA2
      created_at: !ruby/object:DateTime 2020-11-10 14:57:53.317000000 Z
      updated_at: !ruby/object:DateTime 2020-11-10 14:58:11.656000000 Z
      content_type_id: color
      revision: 0
    name: Green 30
    value: "#3dca86"
    thumbnail:
      sys:
        id: 50qfPG44YZUymq2QpfFI0r
        created_at: !ruby/object:DateTime 2020-11-10 14:58:05.974000000 Z
        updated_at: !ruby/object:DateTime 2020-11-10 14:58:30.346000000 Z
      title: Green 30
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/50qfPG44YZUymq2QpfFI0r/e93449394eac79f1271240bac2d91335/green_30-01.png"
  image: https://images.ctfassets.net/g118h5yoccvd/3CuWZ27QualPFqW9aInWe4/9fbe19707584bfdbc1882f5127f5c4fe/content-moderation-toolkit_kenyan-election-and-census_social-card.png
- sys:
    id: 5HPWXq1exdySB1EdnYQvTB
    created_at: !ruby/object:DateTime 2020-11-10 15:03:40.089000000 Z
    updated_at: !ruby/object:DateTime 2020-11-18 20:06:15.370000000 Z
    content_type_id: report
    revision: 1
    space:
      sys:
        id: g118h5yoccvd
    type: Entry
    content_type:
      sys:
        id: report
    environment:
      sys:
        id: master
        type: Link
        linkType: Environment
    locale: en-US
  title: Toolkit for Civil Society and Moderation Inventory
  slug: toolkit-for-civil-society-and-moderation-inventory
  lead_image:
    sys:
      id: 7tI71PoW9pidpHhiGlbrxG
      created_at: !ruby/object:DateTime 2020-11-10 15:08:40.642000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:04:18.935000000 Z
    title: Toolkit for Civil Society and Moderation Inventory
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/7tI71PoW9pidpHhiGlbrxG/be6852a22c1f817fde016e07d731ce9f/content-moderation-toolkit_toolkit-for-civil-society-and-moderation-inventory_lead-image.png"
  social_card_image:
    sys:
      id: 5a5wRF4afli4T8FNKDSNgZ
      created_at: !ruby/object:DateTime 2020-11-10 15:06:49.533000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:04:15.060000000 Z
    title: Toolkit for Civil Society and Moderation Inventory
    description: 
    url: "//images.ctfassets.net/g118h5yoccvd/5a5wRF4afli4T8FNKDSNgZ/5e95538384c6c6dd7dbee6fb4fe9c63d/content-moderation-toolkit_toolkit-for-civil-society-and-moderation-inventory_social-card.png"
  introduction: 'Content moderation is a process, not a solution in and of itself.
    With the increasing complexity of content on the internet and norms around governance,
    new standards are needed to help bridge the gap between industry and those seeking
    to engage with industry. Developed by a multistakeholder working group, the Content
    Moderation Toolkit for Civil Society articulates a common vocabulary and methodology
    for content moderation. '
  body: "The Content Moderation Project developed this inventory through systematic
    documentation of moderation tools that are common on popular social media platforms
    (e.g., Facebook, WhatsApp, Twitter, Reddit, YouTube) to the extent they are available
    to the public, informed by interviews with industry practitioners. We developed
    categories of tools through a content analysis of the tools documented and discussions
    with members of the working group.\n\nWe don’t see this as being completely comprehensive
    per se, but as the beginning of a model that can be used for an ongoing inventory
    in an effort to develop standards. Indeed, this toolkit serves as a proof of concept
    for a fuller model that would be developed collaboratively with stakeholders from
    different industries. In the long term, we believe that articulating this inventory
    will help civil society actors provide more informed opinions in their conversations
    with platforms, while serving platforms in cross-industry conversation about these
    core issues.\n\n### Why develop an inventory? \n\nDuring our needs finding research,
    we identified a critical gap between how industry stakeholders discussed moderation
    decisions and how those outside industry—especially those in civil society, academia,
    and policymaking—understood these decisions. In that light, the Content Moderation
    Toolkit is designed with the following core goals in mind:\n\n1. __Develop a common
    vocabulary__ for stakeholders in civil society, academia, and policy to have meaningful,
    practical conversations with industry.\n\t- This toolkit and inventory aims to
    inform stakeholders outside of industry how to advocate for product or policy
    changes in an actionable language that is informed by industry practices. This
    includes having a more intricate, nuanced understanding of the different types
    of moderation and the types of actions they perform, and how these actions can
    connect to the change stakeholders desire to see. At the same time we aim to allow
    stakeholders to better understand the infrastructure/tooling needed to support
    the implementation and enforcement of those policies (or their goals/needs).\n\n1.
    \ __Compile research insights__ in practical, searchable literature databases
    and summaries tailored for industry.\n\t- Literature reviews of key academic papers.\n\t-
    Construct an ongoing database developed by a working group of academic and civil
    society experts in the field.\n\n1. __Develop an evidence-based model__ for making
    policy recommendations by collecting and auditing moderation-related data.\n\n##
    Defining Content Moderation and its Properties\n\n### What is this inventory?\n\nThis
    inventory is a proposed method of classification for developing a taxonomy of
    content moderation enforcement tools. This inventory catalogues the main moderation
    product features used to enforce content moderation decisions on social media
    platforms. We classify these moderation features as __tools__.\n\nDifferent social
    media platforms use different terms to describe similar or identical moderation
    features, and conversely, use the same terms to describe moderation features that
    are implemented differently across platforms (e.g., unfollow on Twitter vs. unfollow
    on Facebook). Moreover, platforms continue to change these features or release
    new features to address current and ever-emerging challenges in content moderation.
    \n\nUsers and organizations external to social media companies thus have limited,
    confusing, and inconsistent insight into how content moderation features operate
    within the systems underlying platform governance, from both technical and platform
    policy perspectives.\n\nRather than documenting the current versions of how each
    tool is designed and implemented on each platform, it’s important to develop consistent
    terminology that describes the functions of these tools to make moderation enforcement
    comprehensible and develop a vocabulary to differentiate between approaches or
    impacts of the ways social media companies deploy these tools.\n\nIn order to
    enable discussions of moderation features across platforms, this inventory proposes
    a set of __classes of tools__ that correspond to content moderation actions as
    they are understood and named by the public, and articulates variations of how
    platforms implement tools in these classes. \n\nFor each tool, the inventory lists
    a range of common implementations of that class of tool as it has been deployed
    on popular social media platforms. Each platform’s implementation of a moderation
    enforcement tool emerges from a differential sets of resources, priorities, and
    trade-offs, audiences, and structures of interaction on the platform. Therefore,
    the goal is not to impose a standard of how these tools should be articulated
    or labeled on platforms; rather, this inventory is designed to help standardize
    a vocabulary across these systems, to help build a meaningful way for civil society
    actors and platforms to engage.\n\nWe see three properties of content moderation
    enforcement tools that influence how they are implemented and used:\n\n1. __Context:__
    How interaction between users is structured on a platform\n2. __Ownership:__ Who
    is doing the moderation enforcement through each tool \n3. __Orientation:__ What
    temporal dimension each tool is meant to support. \n\nVariance in execution of
    enforcement between platforms can occur for a variety of reasons including:\n\n
    \ - How tools are designed and operate \n  - What is possible in a technical or
    operational capacity\n  - Policy limitations\n  - Relationship with other stages
    of the moderation process\n  - Concerns expressed by users and public stakeholders\n\nAccording
    to [a recent report](https://lab.witness.org/ticks-or-it-didnt-happen/ \"Witness:
    ticks or it didn't happen\") developed by human rights and media organization
    WITNESS, society stands at a precipice of opportunities and challenges around
    online content:\n\n- The opportunity: In today’s world, digital tools have the
    potential to increase civic engagement and participation—particularly for marginalized
    and vulnerable groups—enabling civic witnesses, journalists, and ordinary people
    to document abuse, speak truth to power, and protect and defend their rights.\n-
    The challenge: Bad actors are utilizing the same tools to spread misinformation,
    identify and silence dissenting voices, disrupt civil society and democracy, perpetuate
    hate speech, and put individual rights defenders and journalists at risk. AI-generated
    media in particular has the potential to amplify, expand, and alter existing problems
    around trust in information, verification of media, and weaponization of online
    spaces.\n\n- The report highlights the following core dilemmas:\n  - Who might
    be included and excluded from participating?\n  - The tools being built could
    be used to surveil people\n  - Voices could be both chilled and enhanced\n  -
    Authenticity infrastructure will both help and hinder access to justice and trust
    in legal systems\n  - Technical restraints might stop these tools from working
    in places they are needed the most.\n  - News outlets face pressure to authenticate
    media\n  - Social media platforms will introduce their own authenticity measures\n
    \ - Data storage, access and ownership: Who controls what?\n  - The technology
    and science is complex, emerging and, at times, misleading\n  - How to decode,
    understand, and appeal the information being given\n  - Those using older or niche
    hardware might be left behind\n  - Jailbroken devices will not be able to capture
    verifiable audio visual material \n  - If people can no longer be trusted, can
    blockchain be?\n\nWhile these dilemmas reflect the challenges of authenticating
    media, many apply equally well to the competing rights and responsibilities for
    content moderation in the context of free expression and user safety. Ultimately,
    content moderation sits at the intersection of a balance between harms to individuals
    and harms to society overall, with no easy solutions. This report seeks to create
    a vocabulary to enable more robust discussions around this topic.\n\n__What is
    content moderation?__\nThere is no authoritative definition of “content moderation,”
    but the term broadly refers to the process of monitoring, judging, and acting
    on user-generated content to enforce policies, rules, or guidelines (often called
    community standards) determined by a governing body. Public engagement with content
    moderation most prominently centers the role of social media companies in moderating
    content on their platforms. Therefore, social media companies are most commonly
    the assumed governing bodies. In reality, there are many intersecting layers of
    governance.\n\n- A few key working assumptions should be noted:\n\n  - Content
    moderation is distinct from moderation more broadly. Moderation may include regulation
    of broader patterns of behavior, networks, or collaboration, in addition to the
    production of content. Content moderation acts on content that is produced or
    posted on technology platforms, including media, comments, messages, and others.\n
    \ - Content moderation is widely articulated as being solely under the purview
    of moderation conducted by platforms. However, the work and responsibility of
    monitoring, judging, and acting on user-generated content is often offloaded onto
    platform users and community moderators.\n\n### What is content moderation “enforcement”?\n\n![What
    is content moderation “enforcement”?](//images.ctfassets.net/g118h5yoccvd/20RPqGvjppn2HxFGwONv0X/9ed730130b0aba1988716761584d0a63/What_is_content_moderation____enforcement___.png)\n\nThis
    document specifically focuses on __enforcement tools__, while recognizing the
    multi-layered nature of content moderation. That said, it’s important to recognize
    that content moderation is a process that can be articulated with five distinct
    components:\n\n1. __Detection:__ Locating and identifying content that may violate
    platform policy\n2. __Adjudication:__ Determining if the content is in violation
    of platform policy\n3. __Enforcement:__ Acting on content based on the consequence
    determined by platform policy\n4. __Appeal:__ Returning to the adjudication stage
    if a user contests or appeals a platform judgment\n5. __Policy:__ The set of principles,
    rules, or guidelines that determine what content is acceptable on a platform.
    In practice these guidelines are reviewed and updated based on other components
    of the content moderation process.\n\n## Properties of a Content Moderation Enforcement
    Tool\n\nStandards need persistent properties for comparison, measurement, and
    generalizability. In order to build towards those standards, we operationalize
    some core properties of content moderation tools.\n\nThis document defines a __tool__
    for content moderation enforcement on a platform as a feature or option an actor
    can use to prevent or mitigate harms perpetrated by another actor. The term “tool”
    distinguishes the *intended use* of moderation tools and their technical affordances
    from *practices* of moderation that might not be explicitly designed for as well
    as from the unintended effects of the actions.\n\n### Context: Forum, Feed, Direct
    Message\n\nContent moderation can operate within or govern a multiplicity of __social
    contexts__ on social media platforms. Each social context has its own characteristics
    that emerge from the ways interaction is mediated through the platform. While
    specifics vary considerably across platforms, we define three useful, general
    contexts: forums, feeds, and direct messages. \n\n- __Forum:__ Content is organized
    thematically in forums and users contribute to threads, pages, groups, subreddits,
    etc. according to the topic of their content. Elements of Wikipedia, Facebook
    Groups, Reddit, and general Internet forums such as Mumsnet fit this type.\n-
    __Feed:__ Users follow or friend one another and author content from their individual
    accounts. The content from a user’s contacts on the platform is combined and,
    often, algorithmically ordered to produce a feed—a single list of content that
    combines content from a user’s friends and is personal to that user. Facebook
    and Twitter are two popular platforms displaying content this way.\n- __Direct
    Message:__ Two or more users exchange private messages. These messages have a
    specific audience and content is shared only with these specific users. Algorithmic
    re-ranking/ordering is usually not present. Facebook Messenger, Twitter DM, and
    WhatsApp all fit this context.\n\nIt is often the case that one platform employs
    multiple contexts as the above examples show.\n\n### Owner: User, Community, Platform\n\nThis
    property categorizes tools based on the level at which they operate and who performs
    the actions. That is, some tools will enable users to change only their personal
    experiences of the platform, others will enable a community member (defined below)
    to change the experience for a subset of users of the platform, and still others
    will enable the platform owner to change the experience for anyone (or everyone)
    on the platform as a whole.\n\nUnderstanding the implications of current moderation
    tools and opportunities for intervention necessitates engaging with who is wielding
    which tools. Clarifying the question of “Who can moderate the content on a social
    media platform?” enables stakeholders beyond social media companies to investigate
    who is responsible for moderation and how to propose who should have power over
    content moderation and to what extent.\n\n- Naming who is wielding the tools of
    moderation and which tools they’re wielding (or, what kind of moderation they’re
    empowered to do) enables intervening stakeholders to:\n\n  - See what options
    they have to grant different actors different levels of power to moderate (and
    the resulting trade-offs)\n  - Imagine more sophisticated and distributed models
    of detection and adjudication\n  - Recognize that all of these actors intersect
    in key ways and do not operate in silos\n\n- In order to accomplish this, key
    questions must be asked of these actors.\n\n  - What moderation tools are available
    to them?\n  - What, or who, are they allowed to moderate?\n  - At what scale must
    they moderate?\n  - What are the limits of their ability to moderate (or moderate
    effectively)?\n  - Who is the moderator accountable to?\n  - Who is influenced
    by their moderation?\n  - What set of trade-offs must the moderator balance?\n
    \ - What are the goals or obligations of the moderator?\n  - Who does the moderator
    want to protect?\n  - What measures is the moderator willing to take?\n\nOn a
    technical level, any particular attribute of a category of owner is not necessarily
    mutually exclusive with the other categories. Rather, these categories articulate
    profiles of moderators that have emerged on social media platforms. Each has distinct
    characteristics that afford different modes of interaction and moderation.\n\n__User-centered
    content moderation__ typically targets the experience of the individual who is
    deploying moderation measures, with limited impact on the experience of other
    users on the platform. \n\nUser-centered moderation tools are not typically thought
    of as “content moderation,” but they figure centrally into how decision-makers
    at social media companies balance their desire to enable free expression on their
    platforms with their responsibility to minimize harm towards their users. Social
    media companies reduce their roles as singular arbiters of content by distributing
    control of how content is experienced on platforms by offering tools that enable
    users to moderate what content they experience. Resources on abuse offered by
    social media companies often encourage platform users to [employ user-centered
    moderation tools before flagging content](https://help.twitter.com/en/safety-and-security/cyber-bullying-and-online-abuse
    \"Twitter: cyber-bullying-and-online-abuse\") for platform moderation—however,
    whether the user or the platform should be taking moderation measures is a point
    of contention. Distinguishing and building a vocabulary around the distinct tools
    users can use relative to those of platforms enables more nuanced discussions
    of the trade-offs and impacts of one approach over the other.\n\nUser voting to
    make content less visible may be an edge case of user-centered content moderation
    enforcement. The moderating impact of user voting is typically opaque to users:
    different platforms and community spaces interpret and respond to user voting
    metrics differently.\n\n__Community Content Moderation__ is moderation conducted
    by a subset of accounts within a forum. Community moderator accounts can perform
    actions not available to other accounts on posts, accounts, and the forum itself.
    They may have a formal or informal responsibility to regulate the content in the
    forum, typically towards the goal of enforcing policies in line with the values
    of the forum’s community or the organization hosting the forum.\n\n“Community
    moderation” is not necessarily a formal role, but a category that describes a
    set of moderation features, roles, or obligations. On different social media sites
    or forums individuals who occupy this role may be formally designated as “moderators”
    (often abbreviated as “mods”), “administrators” (often abbreviated as “admins”),
    “operators” (sometimes abbreviated as “ops”), “community manager,” or other titles.\n\nMessaging
    services (e.g., WhatsApp, Skype, Facebook Messenger) typically introduce Community
    Moderator features when a group chat is created, as opposed to a direct message
    sent between two users. Community moderator features are most often granted by
    default to the user who created this group chat and often allow community moderators
    to designate other users as additional community moderators in the group chat.
    Exceptions include Twitter Direct Message groups and Signal group chats, which
    have no community moderation features.\n\nThe circumstances under which a community
    moderator can take moderation actions towards another community moderator varies
    by platform.\n\nCommunity moderation work may be volunteer or paid, but is (normally)
    not employed by the platform. An admin for a Facebook Group, for example, can
    be a volunteer moderator or a paid social media manager for a company running
    that group. Paid community moderation work falls within the purview of many professional
    roles and therefore is subsumed under a variety of terms, including social media
    management, community management, and community operations.\n\nIt’s important
    to distinguish community moderation from platform moderation because community
    moderation offers alternative models of top down moderation.\n\n__Platform content
    moderation__ is the process of monitoring, judging, and acting on user-generated
    content to enforce a technology platform’s policies, rules, or guidelines (often
    called community standards) by actors within the company that owns that platform.\n\nPlatform
    content moderation may be conducted by a number of people with different roles
    within or on behalf of the company that owns a platform, but moderation often
    appears to users through standardized messaging and actions on their account or
    their content without a clear actor.\n\n### Orientation: Proactive, Reactive,
    Preventative\n\nContent moderation is widely perceived to be limited to the actions
    taken to enforce against a post after it has been posted on a platform. However,
    regulation of user-generated content so that it adheres to the policy, rules,
    or guidelines of an online space or platform may also occur through actions that
    intervene with content and users before policy-violating content is on the platform
    and visible to other users. Pre-emptive moderation measures can prevent harmful
    content from reaching users entirely and cultivate user-driven norms that disincentivize
    posting harmful content or defuse the impact of harmful content that does reach
    users. Such preventative measures, however, also carry a risk of disincentivizing
    legitimate speech (this parallels the idea of a ‘chilling effect’ in legal contexts).\n\n-
    **Reactive moderation** acts on content *after* content has been posted to the
    platform. Example: A moderator of a chatroom on Twitch.tv can [clear (remove)
    all of the messages in a chatroom](https://help.twitch.tv/s/article/how-to-manage-harassment-in-chat?language=en_US#CleartheChat
    \"Twitch: how to manage harassment in chat\") if there is an influx of messages
    harassing users in the chatroom.\n\n- __Proactive moderation__ acts on content
    after it is submitted by a user, but before it is posted to the platform. Example:
    The [Automod feature on Twitch.tv](https://blog.twitch.tv/en/2016/12/12/moderators-are-the-sword-now-automod-is-the-shield-df3d8aae32a9/
    \"Automod feature on Twitch.tv\") enables chat room moderators to review a chat
    message after it’s submitted by a user but before the message is posted to the
    chat room. \n\n- __Preventative moderation__ *prevents* content from being submitted
    by a user. Example: The owner of a chatroom on Twitch.tv can [require that users
    view and agree to their chatroom rules](https://help.twitch.tv/s/article/how-to-manage-harassment-in-chat?language=en_US#RequireUserstoAgreetoChannelRules
    \"Twitch: how-to-manage-harassment-in-chat\") before the user can send messages
    to the chatroom, which has been shown to [reduce the amount of reactive moderation
    measures](https://speakerdeck.com/out_of_toner/moderation-tools-and-user-safety-data-driven-approaches-at-twitch?slide=23
    \"Speakerdeck: reduce the amount of reactive moderation measures\") deployed by
    chatroom moderators.\n\nIt is important to note that platforms may combine these
    three orientations together. A platform may, in general, employ reactive moderation
    but hold certain content posts based on an automated analysis of their content
    or the user posting them for proactive moderation. \n\n## Inventory of Tools for
    Content Moderation Enforcement\n\n__Terminology:__\n\n1. __User__ describes the
    platform user employing the moderation tool, while account describes the intended
    target of a moderation tool, typically the social media account of an actor who
    is a possible source of harm to the user.\n2. __Content__ is the text or media
    contained within a __post__.\n3. The terms “explicit,” “mature,” “sensitive,”
    “graphic,” “inappropriate,” and others do not have consistent definitions across
    platforms, and may variably be used to describe content that displays or depicts
    nudity, sexual activity, sexual abuse, child exploitation, gore, and violent imagery.\n\n###
    Block\n\n- __Description:__ A block is broadly defined as a feature that enables
    a user to 1) restrict the access that an account has to that user’s content and
    profile and 2) make content made by the blocked account no longer visible to that
    user.\n\n- __Owner:__ User<br />\n__Context:__ Feed, Forum, Direct Message<br
    />\n__Orientation:__ Reactive, Proactive<br />\n\n- __Example: Tumblr:__ Once
    a user blocks an account, neither the user nor the blocked account see one another
    aside from threaded posts where someone can make a comment and another account
    reblogs and adds to it. Because of Tumblr’s design, these reblogs with added comments
    offer some level of visibility on what a user is saying or doing if another person
    reblogs that user’s content with a comment.\n- __Example: Twitter:__ A user can
    individually block an account. The user no longer sees the account’s tweets and
    the blocked account no longer can see the user’s. Neither party can send a direct
    message (DM) to the other. The user can still navigate to the profile of the account
    they have blocked to opt into seeing the tweets only when navigating directly
    that account’s profile.\n- __Example: Discord:__ A user can individually block
    an account. Once an account is blocked, the blocked account can see the user’s
    contributions to a conversation in a group chat they are both in, but the user
    cannot see the contributions of the blocked account in a group chat.\n- __Example:
    Facebook:__ A user can block individual accounts. Blocked accounts cannot see
    the user’s profile, post on their profile, tag them in posts, see links to their
    profile, send invites to groups, pages, or events, or send a friend request. This
    block does not include apps, games, or groups that both the user and the blocked
    account participate in. \n\n### Mute\n\n- __Description:__ Mute is a feature that
    enables a user to remove a specific piece of content, user, or keyword from their
    feed and/or notifications so it is no longer visible to them by default. This
    feature may still enable users to seek out muted posts, profiles, or keywords
    proactively, or will hide them entirely. Different types of mutes can happen on:\n
    \ - Notifications from comment threads\n  - Global content\n  - Direct or private
    messages\n\n- __Owner:__ User<br />\n__Context:__ Feed, Forum, Direct Message<br
    />\n__Orientation:__ Reactive, Proactive<br />\n\n- __Example: Facebook:__ A user
    can choose the “Snooze for 30 days” for an account so that posts made by that
    account do not show up in the user’s newsfeed for 30 days.\n\n### Hide Post\n\n-
    **Description:** An *individual* post or message is no longer visible to the user,
    but is still viewable by other users. In direct messages, a user may be able to
    hide a message by choosing a “delete for me” option but they will not be able
    to retrieve it once deleted for them.\n\n- __Owner:__ User<br />\n__Context:__
    Feed, Forum, Direct Message<br />\n__Orientation:__ Reactive, Proactive\n\n- __Example:
    Reddit:__ A user can choose to hide a post; it won’t show up in their feed or
    in the subreddit where it was posted. The user can find the hidden post by navigating
    to their account profile and selecting the “hidden” posts section.\n\n### Remove
    Post\n\n- **Description:** The user can remove content that has been posted by
    an account on a user’s post or profile so that it is no longer viewable by any
    account where it was posted. Post removals through platform moderation are also
    commonly called content takedowns. Removing the posted content may result in:\n
    \ - the content being deleted from the platform, \n  - remaining on the profile
    of the account that posted the content, or \n  - moved to a less visible location
    (See: Twitter “[Hide reply](https://blog.twitter.com/en_us/topics/product/2019/more-control-over-your-conversations-globally.html
    \"Twitter: More control over your conversations\")”) \n\n- __Owner:__ User, Community
    Moderator, Platform<br />\n__Context:__ Feed, Forum, Direct Message<br />\n__Orientation:__
    Reactive, Proactive<br />\n\n- __Example: Discord:__ A Discord moderator can select
    “Delete” from the overflow menu (which appears as “•••”) of a chat message, which
    will remove the message from the chat entirely.\n\n### Unfollow/Unfriend\n\n-
    __Description:__ The user removes an account from their social network, reducing
    the visibility of the account to the user or the ability of the account to act
    on the user, where the account may:\n  - no longer appear in the feed of the user\n
    \ - lose access to non-public content posted by the user\n  - lose the ability
    to send the user a direct message\n\n- __Owner:__ User<br />\n__Context:__ Feed,
    Forum, Direct Message<br />\n__Orientation:__ Reactive, Proactive<br />\n\n- __Example:
    LinkedIn:__ A user can select “Remove Connection” on another account, which will
    remove that account from their Connections. That account will no longer show up
    as a Connection and the account will only be able to see the user’s profile details
    and content posted by the user that are available to the public.\n\n### Personal
    Filter\n\n- **Description:** Features of this type modify what type of content
    or behavior is visible to the user, typically in their feeds, notifications, or
    direct messages. Direct messages that are filtered may be directed to a separate
    inbox or deleted. A personal filter is distinct from mute, which hides content
    explicitly based on user-chosen keywords or users. \n\n- __Owner:__ User<br />\n__Context:__
    Feed, Forum, Direct Message<br />\n__Orientation:__ Reactive, Proactive<br />\n\n-
    __Example: Twitter:__ Users can choose to turn on a [quality filter](https://help.twitter.com/en/managing-your-account/understanding-the-notifications-timeline
    \"Twitter: Understanding the notifications timeline\"), which prevents tweets
    with “low-quality” content from appearing in their notifications.\n\n### Restrict
    Personal Audience\n\n- __Description:__ Content posted by the user or users within
    a community is restricted to the audiences designated by the user or community
    moderator.\n\n- __Owner:__ User, Community Moderator<br />\n__Context:__ Feed,
    Forum<br />\n__Orientation:__ Preventative<br />\n\n- __Example: YouTube:__ A
    user can set a video they upload to their channel to “Private,” which removes
    the video from the “Videos” tab on their channel (i.e. the user’s profile), YouTube’s
    search results, and recommendations. Comments are automatically disabled for the
    video and the video cannot be shared by a link. The user can choose to send an
    invitation to view the video to other user emails, but the video is unviewable
    otherwise.\n\n### Restrict Participation on Content\n\n- __Description:__ A user,
    community moderator, or platform moderator can prevent multiple accounts from
    making posts or interacting with posts in a designated space (e.g., posts in a
    forum space, the comments section of a post, edits on a wiki article). This can
    include:\n  - Turning off comments\n  - Turning off voting\n  - Filtering posts
    by a characteristic (e.g., automatically removes posts containing slurs)\n  -
    Filtering posting by user characteristics (e.g., A Reddit moderator may make a
    filter that removes posts made by all accounts less than 10 days old)\n  - Tagging\n
    \ - Turning off or filtering content edits (e.g., editing wiki pages)\n\n- __Owner:__
    User, Community Moderator, Platform<br />\n__Context:__ Feed, Forum<br />\n__Orientation:__
    Reactive, Proactive, Preventative<br />\n\n- __Example: Wikipedia:__ Unregistered
    users can edit most but not all articles, and they cannot create new articles
    on the English edition without an account.\n\n### Content Screen\n\n- __Description:__
    A screen that blocks a type of content partially or entirely with warning about
    the nature of the content—typically by hiding or blurring the content. When encountering
    screened content, the user is given the ability to choose whether to view that
    content or not. Common content types for when the platform screens content by
    default are graphic sexual or violent content.\n\n- __Owner:__ User, Community
    Moderator, Platform<br />\n__Context:__ Feed, Forum, Direct Message<br />\n__Orientation:__
    Reactive, Proactive<br />\n\n- __Example: Reddit:__ Puts a screen in front of
    content that is labeled NSFW (not safe for work), which blurs the thumbnails and
    media preview for users who are using the “Safe browsing mode” setting. Users
    can choose to click on the content screen to view that content.\n\n### Limit Account\n-
    __Description:__ The range of actions otherwise available to a designated account
    is restricted. Actions that can be restricted for that account include:\n  - Posting
    on the platform or community space\n  - Commenting on other user posts\n  - Sending
    direct messages\n  - Interacting with a post (e.g., “Like,” “Favorite,” votes)\n
    \ - Sharing other user posts\n  - Reducing or removing the ability to monetize
    content that is produced by the account\n  - Reducing or removing the ability
    to post advertisements on the platform\n\n- __Owner:__ Community Moderator, Platform<br
    />\n__Context:__ Feed, Forum, Direct Message<br />\n__Orientation:__ Reactive<br
    />\n\n- __Example: YouTube:__ YouTube videos that do not follow guidelines for
    “ad-friendly” content may be demonetized, removing the video creator’s ability
    to host paid advertisements on their video and by extension their ability to generate
    revenue from that video through advertisements.\n\n### Account Suspension/Ban\n\n-
    __Description:__ Removes the account from the platform or forum.\n\n- __Forum:__
    An account banned from a forum will not be able to rejoin that forum, and possibly
    will be restricted from viewing the contents of that forum. \n\n- __Platform:__
    The account suspended by the platform can no longer be accessed, and the account
    profile will no longer be visible to other users. \n\nA suspension or ban may
    be lifted if the account owner performs requested actions or a set amount of time
    has passed. An account may be suspended permanently if it has received multiple
    conditional or temporary suspensions.\n\n- __Owner:__ Community Moderator, Platform<br
    />\n__Context:__ Feed, Forum, Direct Message<br />\n__Orientation:__ Reactive<br
    />\n\n- __Example: Facebook:__ A Facebook Group Admin or Moderator can remove
    a Facebook account from the Group and then choose to “Block” that account from
    the group. An account blocked from the group won't be able to find, see or join
    that group.\n\n### Sanction\n\n- __Description:__ The platform warns a user about
    content or actions that have violated platform policy. A warning may result in:\n\n-
    __An account strike:__ A strike on an account where the account faces further
    moderation actions (e.g., account suspension or ban) once a certain number of
    strikes have been acquired within an allotted time period.\n\n- __Requiring action
    from the user:__ The platform may require that the user remove a post, alter their
    content or profile, or take other measures within a certain period of time to
    avoid further moderation actions or reduce, e.g., to lift an account suspension.\n\nPlease
    note that some sanctions are reflective of cumulative flags against the user that
    may not be visible immediately but are logged.\n\n- __Owner:__ Community Moderator,
    Platform<br />\n__Context:__ Feed, Forum, Direct Message<br />\n__Orientation:__
    Reactive<br />\n\n- __Example: Instagram:__ An account may receive an account
    warning when they have made multiple posts that violate Instagram community guidelines,
    notifying them that their account will be deleted if they continue to violate
    policy. The warning includes a list of previous policy violations the account
    has made.\n\n### Downrank\n\n- __Description:__ Content from a particular account,
    accounts with particular characteristics, or content with particular characteristics
    will be ranked lower in user feeds, recommended content, home screens, or front
    pages of social media platforms, making it less visible to users.\n\n- __Owner:__
    Platform<br />\n__Context:__ Feed, Forum<br />\n__Orientation:__ Reactive, Proactive<br
    />\n\n- __Example: Twitter:__ [Twitter announced July 2018](https://blog.twitter.com/en_us/topics/company/2018/Setting-the-record-straight-on-shadow-banning.html
    \"Twitter: Setting the record straight on shadow banning\") that tweets associated
    with bad actors were ranked lower with bad actors determined based on signals
    like account age, others accounts in its network (i.e., followers), and the moderation
    actions that users took against that account.\n"
  alt_date: !ruby/object:DateTime 2020-11-18 00:00:00.000000000 Z
  author:
  - sys:
      id: 55lvpa6OODrYw65JImLBDO
      created_at: !ruby/object:DateTime 2020-03-24 00:44:57.806000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:04:55.247000000 Z
      content_type_id: contributor
      revision: 3
    name: Kat Lo
    staff: true
    role: Product Strategy Researcher<br>Content Moderation Lead
    location: Los Angeles
    bio: |-
      Kat Lo serves as Content Moderation Lead and a product researcher at Meedan. Drawing from more than ten years of experience running online communities, Kat works with researchers, industry professionals, and community leaders to develop resources for fostering healthy social environments in games and social media. She has spent the past 5 years working with Intel, Microsoft Research, Instagram, Southern Poverty Law Center, AnyKey, and other organizations to study issues around diversity, inclusion and trust & safety. She has won fellowships from the National Science Foundation, Google, and the UC Berkeley Center for Technology, Society & Policy for her research on diversity and inclusion in technology.

      Kat is currently an industry consultant and researcher at the University of California Irvine, where she studies moderation practices in online spaces. Drawing from methods across user experience research and data science, her work focuses on the development of tools and community management best practices to mitigate toxic online environments. She advises non-profit organizations in mental health and online harassment, and assists in crisis support for targets of online harassment.
  publication:
  - sys:
      id: 1EPFTrNepDeTMUR1FbT1LV
      created_at: !ruby/object:DateTime 2020-11-09 15:59:09.591000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:06:32.246000000 Z
      content_type_id: publication
      revision: 1
    title: Content Moderation Toolkit
    short_title: Content Moderation Toolkit
    description: Content moderation is a process, not a solution in and of itself.
      With the increasing complexity of content on the internet and norms around governance,
      new standards are needed to help bridge the gap between industry and those seeking
      to engage with industry. With support from the Swedish International Development
      Agency and the Facebook Journalism Project, Meedan presents this work toward
      a common vocabulary and methodology for content moderation as a toolkit for
      civil society and platforms alike grappling with these issues.
    slug: content-moderation-toolkit
    sections:
    - sys:
        id: 2ldDGlFH61yvNc2x9DF2ex
    author:
    - sys:
        id: 55lvpa6OODrYw65JImLBDO
        created_at: !ruby/object:DateTime 2020-03-24 00:44:57.806000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:55.247000000 Z
        content_type_id: contributor
        revision: 3
      name: Kat Lo
      staff: true
      role: Product Strategy Researcher<br>Content Moderation Lead
      location: Los Angeles
      bio: |-
        Kat Lo serves as Content Moderation Lead and a product researcher at Meedan. Drawing from more than ten years of experience running online communities, Kat works with researchers, industry professionals, and community leaders to develop resources for fostering healthy social environments in games and social media. She has spent the past 5 years working with Intel, Microsoft Research, Instagram, Southern Poverty Law Center, AnyKey, and other organizations to study issues around diversity, inclusion and trust & safety. She has won fellowships from the National Science Foundation, Google, and the UC Berkeley Center for Technology, Society & Policy for her research on diversity and inclusion in technology.

        Kat is currently an industry consultant and researcher at the University of California Irvine, where she studies moderation practices in online spaces. Drawing from methods across user experience research and data science, her work focuses on the development of tools and community management best practices to mitigate toxic online environments. She advises non-profit organizations in mental health and online harassment, and assists in crisis support for targets of online harassment.
    additional_attribution: Lead, Meedan Content Moderation Project
    cover_image:
      sys:
        id: 6VYAFLYRLJnOVMBZSy1KD2
        created_at: !ruby/object:DateTime 2020-11-09 16:18:18.284000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:49.395000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/6VYAFLYRLJnOVMBZSy1KD2/4e40e178f196d8b024f0964ac95051e9/content-moderation-toolkit_cover-image.png"
    publication_promo_graphic:
      sys:
        id: 1DzzJ94lvtgdJnCaYLRegg
        created_at: !ruby/object:DateTime 2020-11-09 16:22:38.136000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:45.742000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/1DzzJ94lvtgdJnCaYLRegg/c3690f778b91576bebf6d62b5a14091f/content-moderation-toolkit_publication-promo-graphic.png"
    social_card_image:
      sys:
        id: 2biBznblY8VQEDW3NaheSe
        created_at: !ruby/object:DateTime 2020-11-17 19:26:39.227000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:41.047000000 Z
      title: Content Moderation Toolkit
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/2biBznblY8VQEDW3NaheSe/abdd8d11f56fd989a34d6d623428bbf3/Content-moderation-toolkit_executive-summary_social-card.png"
    publication_promo: This report is part of the [Content Moderation Toolkit](https://meedan-dev.netlify.app/reports/content-moderation-toolkit/)
    color:
      sys:
        id: sw3ntNjhkQRzLXqLpNM2m
        created_at: !ruby/object:DateTime 2020-11-09 16:06:32.559000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:04:35.341000000 Z
        content_type_id: color
        revision: 1
      name: Magenta
      value: "#ff4d7e"
      thumbnail:
        sys:
          id: ewZMwgt0xilyUB6umRuQs
          created_at: !ruby/object:DateTime 2020-11-09 16:07:07.170000000 Z
          updated_at: !ruby/object:DateTime 2020-11-18 20:04:32.936000000 Z
        title: Magenta
        description: "#ff4d7e"
        url: "//images.ctfassets.net/g118h5yoccvd/ewZMwgt0xilyUB6umRuQs/68b8db4246a336ad25a4c7658c0fafd8/magenta-01.png"
      text: "#262626"
  color:
    sys:
      id: 4Esqafv6LMFgmVypNRiAhZ
      created_at: !ruby/object:DateTime 2020-11-10 15:05:33.283000000 Z
      updated_at: !ruby/object:DateTime 2020-11-18 20:05:41.107000000 Z
      content_type_id: color
      revision: 1
    name: Pale Blue
    value: "#5f9ee5"
    thumbnail:
      sys:
        id: 5WLCyMDUUBatdR50t5xMEF
        created_at: !ruby/object:DateTime 2020-11-10 15:05:45.916000000 Z
        updated_at: !ruby/object:DateTime 2020-11-18 20:05:36.787000000 Z
      title: Pale Blue
      description: 
      url: "//images.ctfassets.net/g118h5yoccvd/5WLCyMDUUBatdR50t5xMEF/b0f5e09a365f8a1c2a3663060c974f23/pale_blue-01.png"
  image: https://images.ctfassets.net/g118h5yoccvd/5a5wRF4afli4T8FNKDSNgZ/5e95538384c6c6dd7dbee6fb4fe9c63d/content-moderation-toolkit_toolkit-for-civil-society-and-moderation-inventory_social-card.png
